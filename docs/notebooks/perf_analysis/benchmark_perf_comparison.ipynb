{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Compare Model Zoo Benchmark performance between Intel optimized and stock Tensorflow\n",
    "\n",
    "This jupyter notebook will help you evaluate performance benefits from Intel-optimized Tensorflow via several pre-trained models from Intel Model Zoo. \n",
    "The notebook will show users a bar chart like below for performance comparison among Stock and Intel Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\perf_comparison.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Platform Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import PlatformUtils\n",
    "plat_utils = PlatformUtils()\n",
    "plat_utils.dump_platform_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Run the benchmark on the selected Jupyter Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check TensorFlow version and MKL enablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (\"We are using Tensorflow version\", tf.__version__)\n",
    "major_version = int(tf.__version__.split(\".\")[0])\n",
    "if major_version >= 2:\n",
    "   from tensorflow.python import _pywrap_util_port\n",
    "   print(\"MKL enabled:\", _pywrap_util_port.IsMklEnabled())\n",
    "else:\n",
    "   print(\"MKL enabled:\", tf.pywrap_tensorflow.IsMklEnabled())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure parameters for launch_benchmark.py according to the selected Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: List out the supported topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from profiling.profile_utils import ConfigFile\n",
    "config = ConfigFile()\n",
    "sections = config.read_section()\n",
    "print(\"Supported topologies: \")\n",
    "index =0 \n",
    "for section in sections:\n",
    "    print(\" %d: %s \" %(index, section))\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Pick a topology. \n",
    "#### ACTION : Please select one supported topology and change topo_index accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User picks a topology, Batch Size, and number of required threads\n",
    "## USER INPUT\n",
    "topo_index=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List out the selected topology name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if topo_index >= len(sections):\n",
    "    print(\"ERROR! please input a topo_index within range\")\n",
    "else:\n",
    "    topology_name=sections[topo_index]\n",
    "    print(topology_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: User can also manually set batch size and number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "cpu_count = psutil.cpu_count(logical=False)\n",
    "cpu_socket_count =  int(subprocess.check_output('cat /proc/cpuinfo | grep \"physical id\" | sort -u | wc -l', shell=True))\n",
    "print(\"CPU count per socket:\" ,  cpu_count ,\" \\nSocket count:\",cpu_socket_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTION: Users can change the value of thread_number and batch_size to see different performance\n",
    "1. thread_umber: the value will apply to num_cores parameters in launch_benchmark.py  \n",
    "2. utilized_socket_number:  the value will apply to the socket-id parameter in launch_benchmark.py \n",
    "3. num_inter_threads: the value will  apply to the num-inter-threads parameter in launch_benchmark.py \n",
    "4. num_intra_threads: the value will  apply to the num-intra-threads parameter in launch_benchmark.py \n",
    "5. batch_size: the value will apply to the batch_size parameter in launch_benchmark.py \n",
    "6. log_folder: the folder where the logs are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USER INPUT\n",
    "thread_number=cpu_count \n",
    "utilized_socket_number=1 #cpu_socket_count\n",
    "num_inter_threads = utilized_socket_number\n",
    "num_intra_threads = thread_number\n",
    "batch_size=32\n",
    "log_folder=os.getcwd() + os.sep + \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Check mandatory file \"launch_benchmark.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTION: Users should change the value of os.environ['ModelZooRoot'] according to their environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Users should change ModelZooRoot path according to their environment\n",
    "## USER INPUT\n",
    "current_path = os.getcwd()\n",
    "os.environ['ModelZooRoot'] = current_path + \"/../../../\"\n",
    "os.environ['ProfileUtilsRoot'] = os.environ['ModelZooRoot'] + \"docs/notebooks/perf_analysis/profiling/\"\n",
    "print(os.environ['ModelZooRoot'])\n",
    "print(os.environ['ProfileUtilsRoot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check those mandatory python scripts after users assign ModelRooRoot and ProfileUtilsRoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()\n",
    "benchmark_path = os.environ['ModelZooRoot'] + \"benchmarks/launch_benchmark.py\"\n",
    "if os.path.exists(benchmark_path) == True:\n",
    "    print(benchmark_path)\n",
    "else:\n",
    "    print(\"ERROR! Can't find benchmark script!\")\n",
    "    \n",
    "profile_utils_path = os.environ['ProfileUtilsRoot'] + \"profile_utils.py\"\n",
    "if os.path.exists(profile_utils_path) == True:\n",
    "    print(profile_utils_path)\n",
    "else:\n",
    "    print(\"ERROR! Can't find profile_utils script!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.6: Prepare pre-trained model and model parameters for running the benchmark\n",
    "1. Get related parameters according to selected topology\n",
    "2. Get pretrained model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the parameters\n",
    "configvals=config.read_config(topology_name)\n",
    "\n",
    "# Get the pre-trained model file\n",
    "if config.wget != '' and config.in_graph == '':\n",
    "    pretrain_model_path = config.download_pretrained_model(current_path=current_path)\n",
    "    config.in_graph = pretrain_model_path \n",
    "    configvals.append(\"--in-graph\")\n",
    "    configvals.append(pretrain_model_path)\n",
    "\n",
    "#Set output-dir folder\n",
    "if log_folder !='':\n",
    "    configvals.append(\"--output-dir\")\n",
    "    configvals.append(log_folder)\n",
    "\n",
    "params = config.get_parameters(topology_name, configvals,\n",
    "                   batch_size=batch_size, thread_number=thread_number, socket_number=utilized_socket_number,\n",
    "                   num_inter_threads=num_inter_threads, num_intra_threads=num_intra_threads)\n",
    "\n",
    "sys.argv=[benchmark_path]+params\n",
    "print(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.7: Create a CSV file to log the performance numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profiling.profile_utils import PerfPresenter\n",
    "job_type = 'inference'\n",
    "csv_fname=job_type+'_'+topology_name.replace(' ', '')+'.csv'\n",
    "print(csv_fname)\n",
    "perfp=PerfPresenter()\n",
    "perfp.create_csv_logfile(job_type, csv_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Run the benchmark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Below section will enable Tensorflow timeline for the model by patching it, and then unpatch it after the model completes its training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch related model script\n",
    "repo_path = os.environ['ModelZooRoot'] #current_path + os.sep + \"../../\"\n",
    "config.patch_model_to_enable_timeline(repopath=repo_path)\n",
    "\n",
    "# run the benchmark with the patch\n",
    "import sys\n",
    "benchmark_path = os.environ['ModelZooRoot']+os.sep+\"benchmarks/\"\n",
    "sys.path.append(benchmark_path)\n",
    "from launch_benchmark import LaunchBenchmark\n",
    "\n",
    "util = LaunchBenchmark()\n",
    "util.main()\n",
    "\n",
    "# unpatch related model script\n",
    "config.unpatch_model_to_enable_timeline(model_path=repo_path+'/models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parse output for performance number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the path of the latest log file\n",
    "configvals=config.read_config(topology_name)\n",
    "import os\n",
    "for file in os.listdir(log_folder):\n",
    "    if file.endswith(\".log\"):\n",
    "        logpath = os.path.join(log_folder, file)\n",
    "        used_logpath = logpath + \".old\"\n",
    "        os.rename(logpath, used_logpath)\n",
    "        print(used_logpath)\n",
    "        break\n",
    "\n",
    "val = config.throughput_keyword\n",
    "line = perfp.read_throughput(used_logpath, keyword=val)\n",
    "if line!=None:\n",
    "    throughput=line\n",
    "    print(throughput)\n",
    "    # log the perf number\n",
    "    perfp.log_infer_perfcsv(0,throughput, csv_fname)\n",
    "else:\n",
    "    print(\"ERROR! can't find correct performance number from log. please check log for runtime issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional : print out the log file for runtime issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = open(used_logpath)\n",
    "logout = logfile.read()\n",
    "print(logout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users should be able to see a new Timeline json file after running the benchmark\n",
    "If users don't see a new timeline json file, they need to make sure that they patch the model script correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l -h $ModelZooRoot/benchmarks/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Draw the performance comparison diagram\n",
    "### NOTE: Please go over Section 1 on different Jupyter kernel before comparison\n",
    "Users can find information in docs/notebooks/perf_analysis/README.md for switching among different Juypter kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from profiling.profile_utils import PerfPresenter\n",
    "\n",
    "perfp=PerfPresenter()\n",
    "        \n",
    "# inference  throughput\n",
    "perfp.draw_perf_diag_from_csv(csv_fname,'throughput','throughput (image/sec)', topology_name)\n",
    "perfp.draw_perf_ratio_diag_from_csv(csv_fname,'throughput','speedup', topology_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Gather all generated Tensorflow Timeline Json files\n",
    "Copy the timeline json file from benchmark folder to the Timeline folder with time information.\n",
    "Those Timeline files will be analyzed in another Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir Timeline; mv $ModelZooRoot/benchmarks/*.json Timeline;mv Timeline Timeline_$(date +%m%d%H%M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-tensorflow",
   "language": "python",
   "name": "stock-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
