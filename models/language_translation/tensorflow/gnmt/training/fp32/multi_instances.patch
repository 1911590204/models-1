diff --git a/nmt/inference.py b/nmt/inference.py
index 2cbef07..b9e08f3 100644
--- a/nmt/inference.py
+++ b/nmt/inference.py
@@ -98,7 +98,10 @@ def get_model_creator(hparams):
 def start_sess_and_load_model(infer_model, ckpt_path):
   """Start session and load model."""
   sess = tf.Session(
-      graph=infer_model.graph, config=utils.get_config_proto())
+      graph=infer_model.graph, config=utils.get_config_proto(
+        num_intra_threads=hparams.num_intra_threads,
+        num_inter_threads=hparams.num_inter_threads
+        ))
   with infer_model.graph.as_default():
     loaded_infer_model = model_helper.load_model(
         infer_model.model, ckpt_path, sess, "infer")
@@ -152,12 +155,25 @@ def single_worker_inference(sess,
 
   # Read data
   infer_data = load_data(inference_input_file, hparams)
+  infer_data_feed = infer_data
+
+  #sort the input file if no hparams.inference_indices is defined
+  index_pair = {}
+  new_input =[]
+  if hparams.inference_indices is None:
+    start_time = time.time()
+    input_length = [(len(line.split()), i) for i, line in enumerate(infer_data)]
+    sorted_input_bylens = sorted(input_length)
+    for ni, (_, oi) in enumerate(sorted_input_bylens):
+      new_input.append(infer_data[oi])
+      index_pair[oi] = ni
+    infer_data_feed = new_input
 
   with infer_model.graph.as_default():
     sess.run(
         infer_model.iterator.initializer,
         feed_dict={
-            infer_model.src_placeholder: infer_data,
+            infer_model.src_placeholder: infer_data_feed,
             infer_model.batch_size_placeholder: hparams.infer_batch_size
         })
     # Decode
@@ -172,7 +188,7 @@ def single_worker_inference(sess,
           tgt_eos=hparams.eos,
           subword_option=hparams.subword_option)
     else:
-      nmt_utils.decode_and_evaluate(
+      _, end_time, num_sentences = nmt_utils.decode_and_evaluate(
           "infer",
           loaded_infer_model,
           sess,
@@ -183,8 +199,13 @@ def single_worker_inference(sess,
           beam_width=hparams.beam_width,
           tgt_eos=hparams.eos,
           num_translations_per_input=hparams.num_translations_per_input,
-          infer_mode=hparams.infer_mode)
-
+          infer_mode=hparams.infer_mode,
+          index_pair=index_pair)
+      duration  = end_time - start_time
+      if hparams.infer_batch_size is 1:
+        print("  The latency of the model is %.4f ms/sentences" % (1000*duration /num_sentences ))
+      else:
+        print("  The throughput of the model is %.4f sentences/s" % (num_sentences / duration))
 
 def multi_worker_inference(sess,
                            infer_model,
diff --git a/nmt/model.py b/nmt/model.py
index e0c4f4e..ed1c15d 100644
--- a/nmt/model.py
+++ b/nmt/model.py
@@ -23,6 +23,7 @@ import collections
 import numpy as np
 
 import tensorflow as tf
+import horovod.tensorflow as hvd
 
 from . import model_helper
 from .utils import iterator_utils
@@ -206,11 +207,21 @@ class BaseModel(object):
       else:
         raise ValueError("Unknown optimizer type %s" % hparams.optimizer)
 
+      # Add Horovod Distributed Optimizer
+      opt = hvd.DistributedOptimizer(opt)
+
       # Gradients
-      gradients = tf.gradients(
+      #gradients = tf.gradients(
+      #    self.train_loss,
+      #    params,
+      #    colocate_gradients_with_ops=hparams.colocate_gradients_with_ops)
+
+      # Horovod compute_gradients
+      # Allreduce the gradients before returning them
+      gradients, variables = zip(*opt.compute_gradients(
           self.train_loss,
           params,
-          colocate_gradients_with_ops=hparams.colocate_gradients_with_ops)
+          colocate_gradients_with_ops=hparams.colocate_gradients_with_ops))
 
       clipped_grads, grad_norm_summary, grad_norm = model_helper.gradient_clip(
           gradients, max_gradient_norm=hparams.max_gradient_norm)
@@ -394,9 +405,13 @@ class BaseModel(object):
 
       ## Loss
       if self.mode != tf.contrib.learn.ModeKeys.INFER:
-        with tf.device(model_helper.get_device_str(self.num_encoder_layers - 1,
-                                                   self.num_gpus)):
-          loss = self._compute_loss(logits, decoder_cell_outputs)
+        #with tf.device(model_helper.get_device_str(self.num_encoder_layers - 1,
+        #                                           self.num_gpus)):
+        #  loss = self._compute_loss(logits, decoder_cell_outputs)
+
+        # Horovod
+        loss = self._compute_loss(logits, decoder_cell_outputs)
+
       else:
         loss = tf.constant(0.0)
 
@@ -649,7 +664,9 @@ class BaseModel(object):
     target_output = self.iterator.target_output
     if self.time_major:
       target_output = tf.transpose(target_output)
-    max_time = self.get_max_time(target_output)
+    #max_time = self.get_max_time(target_output)
+    max_time = tf.reduce_max(self.iterator.target_sequence_length)
+    target_output = tf.slice(target_output, [0, 0], [max_time, -1])
 
     crossent = self._softmax_cross_entropy_loss(
         logits, decoder_cell_outputs, target_output)
diff --git a/nmt/model_helper.py b/nmt/model_helper.py
index 65e1114..7e05591 100644
--- a/nmt/model_helper.py
+++ b/nmt/model_helper.py
@@ -115,16 +115,16 @@ def create_train_model(
     # Note: One can set model_device_fn to
     # `tf.train.replica_device_setter(ps_tasks)` for distributed training.
     model_device_fn = None
-    if extra_args: model_device_fn = extra_args.model_device_fn
-    with tf.device(model_device_fn):
-      model = model_creator(
-          hparams,
-          iterator=iterator,
-          mode=tf.contrib.learn.ModeKeys.TRAIN,
-          source_vocab_table=src_vocab_table,
-          target_vocab_table=tgt_vocab_table,
-          scope=scope,
-          extra_args=extra_args)
+    #if extra_args: model_device_fn = extra_args.model_device_fn
+    #with tf.device(model_device_fn):
+    model = model_creator(
+        hparams,
+        iterator=iterator,
+        mode=tf.contrib.learn.ModeKeys.TRAIN,
+        source_vocab_table=src_vocab_table,
+        target_vocab_table=tgt_vocab_table,
+        scope=scope,
+        extra_args=extra_args)
 
   return TrainModel(
       graph=graph,
@@ -236,10 +236,15 @@ def create_infer_model(model_creator, hparams, scope=None, extra_args=None):
 
 def _get_embed_device(vocab_size):
   """Decide on which device to place an embed matrix given its vocab size."""
+  '''
   if vocab_size > VOCAB_SIZE_THRESHOLD_CPU:
     return "/cpu:0"
   else:
     return "/gpu:0"
+  '''
+  # Horovod
+  # All device are CPU for horovod CPU training
+  return "/cpu:0"
 
 
 def _create_pretrained_emb_from_txt(
@@ -269,9 +274,10 @@ def _create_pretrained_emb_from_txt(
   emb_mat = tf.constant(emb_mat)
   emb_mat_const = tf.slice(emb_mat, [num_trainable_tokens, 0], [-1, -1])
   with tf.variable_scope(scope or "pretrain_embeddings", dtype=dtype) as scope:
-    with tf.device(_get_embed_device(num_trainable_tokens)):
-      emb_mat_var = tf.get_variable(
-          "emb_mat_var", [num_trainable_tokens, emb_size])
+    #with tf.device(_get_embed_device(num_trainable_tokens)):
+    # Horovod
+    emb_mat_var = tf.get_variable(
+        "emb_mat_var", [num_trainable_tokens, emb_size])
   return tf.concat([emb_mat_var, emb_mat_const], 0)
 
 
@@ -281,9 +287,10 @@ def _create_or_load_embed(embed_name, vocab_file, embed_file,
   if vocab_file and embed_file:
     embedding = _create_pretrained_emb_from_txt(vocab_file, embed_file)
   else:
-    with tf.device(_get_embed_device(vocab_size)):
-      embedding = tf.get_variable(
-          embed_name, [vocab_size, embed_size], dtype)
+    #with tf.device(_get_embed_device(vocab_size)):
+    #Horovod
+    embedding = tf.get_variable(
+        embed_name, [vocab_size, embed_size], dtype)
   return embedding
 
 
@@ -545,6 +552,23 @@ def load_model(model, ckpt_path, session, name):
   return model
 
 
+# Horovod
+def horovod_load_model(model, ckpt_path, session, name):
+  """Load model from a checkpoint."""
+  start_time = time.time()
+  try:
+    model.saver.restore(session, ckpt_path)
+  except tf.errors.NotFoundError as e:
+    utils.print_out("Can't load checkpoint")
+    print_variables_in_ckpt(ckpt_path)
+    utils.print_out("%s" % str(e))
+
+  utils.print_out(
+      "  loaded %s model parameters from %s, time %.2fs" %
+      (name, ckpt_path, time.time() - start_time))
+  return model
+
+
 def avg_checkpoints(model_dir, num_last_checkpoints, global_step,
                     global_step_name):
   """Average the last N checkpoints in the model_dir."""
@@ -634,6 +658,17 @@ def create_or_load_model(model, model_dir, session, name):
   return model, global_step
 
 
+# Horovod
+def horovod_create_or_load_model(model, model_dir, session, name):
+  """Create translation model and initialize or load parameters in session."""
+  latest_ckpt = tf.train.latest_checkpoint(model_dir)
+  if latest_ckpt:
+    model = horovod_load_model(model, latest_ckpt, session, name)
+
+  global_step = model.global_step.eval(session=session)
+  return model, global_step
+
+
 def compute_perplexity(model, sess, name):
   """Compute perplexity of the output of the model.
 
diff --git a/nmt/nmt.py b/nmt/nmt.py
index f5823d8..57fdfd3 100644
--- a/nmt/nmt.py
+++ b/nmt/nmt.py
@@ -227,7 +227,10 @@ def add_arguments(parser):
                       """)
 
   # Misc
-  parser.add_argument("--num_gpus", type=int, default=1,
+  #parser.add_argument("--num_gpus", type=int, default=1,
+
+  # Horovod
+  parser.add_argument("--num_gpus", type=int, default=0,
                       help="Number of gpus in each worker.")
   parser.add_argument("--log_device_placement", type="bool", nargs="?",
                       const=True, default=False, help="Debug GPU allocation.")
@@ -653,6 +656,14 @@ def run_main(flags, default_hparams, train_fn, inference_fn, target_session=""):
         out_dir, default_hparams, flags.hparams_path,
         save_hparams=(jobid == 0))
 
+  # GPU device
+  #config_proto = utils.get_config_proto(
+  #    allow_soft_placement=True,
+  #    num_intra_threads=hparams.num_intra_threads,
+  #    num_inter_threads=hparams.num_inter_threads)
+  #utils.print_out(
+  #    "# Devices visible to TensorFlow: %s" % repr(tf.Session(config=config_proto).list_devices()))
+
   ## Train / Decode
   if flags.inference_input_file:
     # Inference output directory
@@ -686,7 +697,9 @@ def run_main(flags, default_hparams, train_fn, inference_fn, target_session=""):
         utils.print_out("  %s: %.1f" % (metric, score))
   else:
     # Train
-    train_fn(hparams, target_session=target_session)
+    # train_fn(hparams, target_session=target_session)
+    # Horovod
+    train_fn(hparams, flags, target_session=target_session)
 
 
 def main(unused_argv):
diff --git a/nmt/standard_hparams/wmt16_gnmt_4_layer_multi_instances.json b/nmt/standard_hparams/wmt16_gnmt_4_layer_multi_instances.json
new file mode 100644
index 0000000..f03ed11
--- /dev/null
+++ b/nmt/standard_hparams/wmt16_gnmt_4_layer_multi_instances.json
@@ -0,0 +1,34 @@
+{
+  "attention": "normed_bahdanau",
+  "attention_architecture": "gnmt_v2",
+  "batch_size": 512,
+  "colocate_gradients_with_ops": true,
+  "dropout": 0.2,
+  "encoder_type": "gnmt",
+  "eos": "</s>",
+  "forget_bias": 1.0,
+  "init_weight": 0.1,
+  "learning_rate": 0.5,
+  "max_gradient_norm": 5.0,
+  "metrics": ["bleu"],
+  "num_buckets": 5,
+  "num_encoder_layers": 4,
+  "num_decoder_layers": 4,
+  "decay_scheme": "luong234",
+  "num_units": 1024,
+  "optimizer": "sgd",
+  "residual": true,
+  "share_vocab": false,
+  "subword_option": "bpe",
+  "sos": "<s>",
+  "src_max_len": 50,
+  "src_max_len_infer": null,
+  "steps_per_external_eval": null,
+  "steps_per_stats": 100,
+  "tgt_max_len": 50,
+  "tgt_max_len_infer": null,
+  "time_major": true,
+  "unit_type": "lstm",
+  "beam_width": 10,
+  "length_penalty_weight": 1.0
+}
diff --git a/nmt/train.py b/nmt/train.py
index 1f06148..5777938 100644
--- a/nmt/train.py
+++ b/nmt/train.py
@@ -21,6 +21,8 @@ import random
 import time
 
 import tensorflow as tf
+# Horovod
+import horovod.tensorflow as hvd
 
 from . import attention_model
 from . import gnmt_model
@@ -356,13 +358,16 @@ def update_stats(stats, start_time, step_result):
 
   # Update statistics
   batch_size = output_tuple.batch_size
-  stats["step_time"] += time.time() - start_time
+  step_time = time.time() - start_time
+  stats["step_time"] += step_time
   stats["train_loss"] += output_tuple.train_loss * batch_size
   stats["grad_norm"] += output_tuple.grad_norm
   stats["predict_count"] += output_tuple.predict_count
   stats["word_count"] += output_tuple.word_count
   stats["sequence_count"] += batch_size
 
+  if os.environ.get('GNMT_DEBUG_LOAD_BALANCE', None): print("Rank %2d: Train Step #%-6d batch_size: %4d, word_count: %6d, predict_count: %6d, loss = %10g, grad_norm: %10g, time: %7.3f sec wps: %7.3fK\n" % (hvd.rank(), output_tuple.global_step, batch_size, output_tuple.word_count, output_tuple.predict_count, output_tuple.train_loss, output_tuple.grad_norm, step_time, output_tuple.word_count/(step_time*1000.0)), end='')
+
   return (output_tuple.global_step, output_tuple.learning_rate,
           output_tuple.train_summary)
 
@@ -370,11 +375,11 @@ def update_stats(stats, start_time, step_result):
 def print_step_info(prefix, global_step, info, result_summary, log_f):
   """Print all info at the current global step."""
   utils.print_out(
-      "%sstep %d lr %g step-time %.2fs wps %.2fK ppl %.2f gN %.2f %s, %s" %
+      "%sstep %d lr %g step-time %.2fs wps %.2fK ppl %.2f gN %.2f %s, %s rank %s\n" %
       (prefix, global_step, info["learning_rate"], info["avg_step_time"],
        info["speed"], info["train_ppl"], info["avg_grad_norm"], result_summary,
-       time.ctime()),
-      log_f)
+       time.ctime(), hvd.rank()),
+      log_f, new_line=False)
 
 
 def add_info_summaries(summary_writer, global_step, info):
@@ -447,7 +452,11 @@ def get_model_creator(hparams):
   return model_creator
 
 
-def train(hparams, scope=None, target_session=""):
+#def train(hparams, scope=None, target_session=""):
+def train(hparams, flags, scope=None, target_session=""):
+  # Horovod
+  hvd.init()
+
   """Train a translation model."""
   log_device_placement = hparams.log_device_placement
   out_dir = hparams.out_dir
@@ -462,7 +471,7 @@ def train(hparams, scope=None, target_session=""):
 
   # Create model
   model_creator = get_model_creator(hparams)
-  train_model = model_helper.create_train_model(model_creator, hparams, scope)
+  train_model = model_helper.create_train_model(model_creator, hparams, scope, num_workers=hvd.size(), jobid=hvd.rank())
   eval_model = model_helper.create_eval_model(model_creator, hparams, scope)
   infer_model = model_helper.create_infer_model(model_creator, hparams, scope)
 
@@ -485,155 +494,168 @@ def train(hparams, scope=None, target_session=""):
       log_device_placement=log_device_placement,
       num_intra_threads=hparams.num_intra_threads,
       num_inter_threads=hparams.num_inter_threads)
-  train_sess = tf.Session(
-      target=target_session, config=config_proto, graph=train_model.graph)
-  eval_sess = tf.Session(
-      target=target_session, config=config_proto, graph=eval_model.graph)
-  infer_sess = tf.Session(
-      target=target_session, config=config_proto, graph=infer_model.graph)
 
   with train_model.graph.as_default():
-    loaded_train_model, global_step = model_helper.create_or_load_model(
-        train_model.model, model_dir, train_sess, "train")
-
-  # Summary writer
-  summary_writer = tf.summary.FileWriter(
-      os.path.join(out_dir, summary_name), train_model.graph)
-
-  # First evaluation
-  run_full_eval(
-      model_dir, infer_model, infer_sess,
-      eval_model, eval_sess, hparams,
-      summary_writer, sample_src_data,
-      sample_tgt_data, avg_ckpts)
-
-  last_stats_step = global_step
-  last_eval_step = global_step
-  last_external_eval_step = global_step
-
-  # This is the training loop.
-  stats, info, start_train_time = before_train(
-      loaded_train_model, train_model, train_sess, global_step, hparams, log_f)
-  while global_step < num_train_steps:
-    ### Run a step ###
-    start_time = time.time()
-    try:
-      step_result = loaded_train_model.train(train_sess)
-      hparams.epoch_step += 1
-    except tf.errors.OutOfRangeError:
-      # Finished going through the training dataset.  Go to next epoch.
-      hparams.epoch_step = 0
-      utils.print_out(
-          "# Finished an epoch, step %d. Perform external evaluation" %
-          global_step)
-      run_sample_decode(infer_model, infer_sess, model_dir, hparams,
-                        summary_writer, sample_src_data, sample_tgt_data)
-      run_external_eval(infer_model, infer_sess, model_dir, hparams,
-                        summary_writer)
+    hooks=[hvd.BroadcastGlobalVariablesHook(0)]
+    with tf.train.MonitoredTrainingSession(
+          #checkpoint_dir=flags.out_dir,
+          hooks=hooks, config=config_proto) as train_sess:
 
-      if avg_ckpts:
-        run_avg_external_eval(infer_model, infer_sess, model_dir, hparams,
-                              summary_writer, global_step)
+      eval_sess = tf.Session(
+          target=target_session, config=config_proto, graph=eval_model.graph)
+      infer_sess = tf.Session(
+          target=target_session, config=config_proto, graph=infer_model.graph)
 
-      train_sess.run(
-          train_model.iterator.initializer,
-          feed_dict={train_model.skip_count_placeholder: 0})
-      continue
+      loaded_train_model, global_step = model_helper.horovod_create_or_load_model(
+          train_model.model, model_dir, train_sess, "train")
 
-    # Process step_result, accumulate stats, and write summary
-    global_step, info["learning_rate"], step_summary = update_stats(
-        stats, start_time, step_result)
-    summary_writer.add_summary(step_summary, global_step)
+      # Summary writer
+      summary_writer = tf.summary.FileWriter(
+          os.path.join(out_dir, summary_name), train_model.graph)
+
+      '''
+      # First evaluation
+      # Rank 0 do evaluation
+      if hvd.rank() == 0:
+        run_full_eval(
+            model_dir, infer_model, infer_sess,
+            eval_model, eval_sess, hparams,
+            summary_writer, sample_src_data,
+            sample_tgt_data, avg_ckpts)
+      '''
 
-    # Once in a while, we print statistics.
-    if global_step - last_stats_step >= steps_per_stats:
       last_stats_step = global_step
-      is_overflow = process_stats(
-          stats, info, global_step, steps_per_stats, log_f)
-      print_step_info("  ", global_step, info, get_best_results(hparams),
-                      log_f)
-      if is_overflow:
-        break
-
-      # Reset statistics
-      stats = init_stats()
-
-    if global_step - last_eval_step >= steps_per_eval:
       last_eval_step = global_step
-      utils.print_out("# Save eval, global step %d" % global_step)
-      add_info_summaries(summary_writer, global_step, info)
-
-      # Save checkpoint
-      loaded_train_model.saver.save(
-          train_sess,
-          os.path.join(out_dir, "translate.ckpt"),
-          global_step=global_step)
-
-      # Evaluate on dev/test
-      run_sample_decode(infer_model, infer_sess,
-                        model_dir, hparams, summary_writer, sample_src_data,
-                        sample_tgt_data)
-      run_internal_eval(
-          eval_model, eval_sess, model_dir, hparams, summary_writer)
-
-    if global_step - last_external_eval_step >= steps_per_external_eval:
       last_external_eval_step = global_step
 
-      # Save checkpoint
-      loaded_train_model.saver.save(
-          train_sess,
-          os.path.join(out_dir, "translate.ckpt"),
-          global_step=global_step)
-      run_sample_decode(infer_model, infer_sess,
-                        model_dir, hparams, summary_writer, sample_src_data,
-                        sample_tgt_data)
-      run_external_eval(
-          infer_model, infer_sess, model_dir,
-          hparams, summary_writer)
-
-      if avg_ckpts:
-        run_avg_external_eval(infer_model, infer_sess, model_dir, hparams,
-                              summary_writer, global_step)
-
-  # Done training
-  loaded_train_model.saver.save(
-      train_sess,
-      os.path.join(out_dir, "translate.ckpt"),
-      global_step=global_step)
-
-  (result_summary, _, final_eval_metrics) = (
-      run_full_eval(
-          model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams,
-          summary_writer, sample_src_data, sample_tgt_data, avg_ckpts))
-  print_step_info("# Final, ", global_step, info, result_summary, log_f)
-  utils.print_time("# Done training!", start_train_time)
-
-  summary_writer.close()
-
-  utils.print_out("# Start evaluating saved best models.")
-  for metric in hparams.metrics:
-    best_model_dir = getattr(hparams, "best_" + metric + "_dir")
-    summary_writer = tf.summary.FileWriter(
-        os.path.join(best_model_dir, summary_name), infer_model.graph)
-    result_summary, best_global_step, _ = run_full_eval(
-        best_model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams,
-        summary_writer, sample_src_data, sample_tgt_data)
-    print_step_info("# Best %s, " % metric, best_global_step, info,
-                    result_summary, log_f)
-    summary_writer.close()
-
-    if avg_ckpts:
-      best_model_dir = getattr(hparams, "avg_best_" + metric + "_dir")
-      summary_writer = tf.summary.FileWriter(
-          os.path.join(best_model_dir, summary_name), infer_model.graph)
-      result_summary, best_global_step, _ = run_full_eval(
-          best_model_dir, infer_model, infer_sess, eval_model, eval_sess,
-          hparams, summary_writer, sample_src_data, sample_tgt_data)
-      print_step_info("# Averaged Best %s, " % metric, best_global_step, info,
-                      result_summary, log_f)
-      summary_writer.close()
-
-  return final_eval_metrics, global_step
+      # This is the training loop.
+      stats, info, start_train_time = before_train(
+          loaded_train_model, train_model, train_sess, global_step, hparams, log_f)
+      while global_step < num_train_steps:
+        ### Run a step ###
+        start_time = time.time()
+        try:
+          step_result = loaded_train_model.train(train_sess)
+          hparams.epoch_step += 1
+        except tf.errors.OutOfRangeError:
+          # Finished going through the training dataset.  Go to next epoch.
+          hparams.epoch_step = 0
+          utils.print_out(
+              "# Finished an epoch, step %d. Perform external evaluation" %
+              global_step)
+          run_sample_decode(infer_model, infer_sess, model_dir, hparams,
+                            summary_writer, sample_src_data, sample_tgt_data)
+          run_external_eval(infer_model, infer_sess, model_dir, hparams,
+                            summary_writer)
+
+          if avg_ckpts:
+            run_avg_external_eval(infer_model, infer_sess, model_dir, hparams,
+                                  summary_writer, global_step)
+
+          train_sess.run(
+              train_model.iterator.initializer,
+              feed_dict={train_model.skip_count_placeholder: 0})
+          continue
+
+        # Process step_result, accumulate stats, and write summary
+        global_step, info["learning_rate"], step_summary = update_stats(
+            stats, start_time, step_result)
+        summary_writer.add_summary(step_summary, global_step)
+
+        # Once in a while, we print statistics.
+        if global_step - last_stats_step >= steps_per_stats:
+          last_stats_step = global_step
+          is_overflow = process_stats(
+              stats, info, global_step, steps_per_stats, log_f)
+          print_step_info("  ", global_step, info, get_best_results(hparams),
+                          log_f)
+          if is_overflow:
+            break
+
+          # Reset statistics
+          stats = init_stats()
+
+        # Horovod
+        # Only master will do it
+        #if hvd.rank() == 0:
+        if global_step - last_eval_step >= steps_per_eval:
+          last_eval_step = global_step
+          utils.print_out("# Save eval, global step %d" % global_step)
+          add_info_summaries(summary_writer, global_step, info)
+
+          # Save checkpoint
+          loaded_train_model.saver.save(
+              train_sess._sess._sess._sess._sess,
+              os.path.join(out_dir, "translate.ckpt"),
+              global_step=global_step)
+
+          # Evaluate on dev/test
+          run_sample_decode(infer_model, infer_sess,
+                            model_dir, hparams, summary_writer, sample_src_data,
+                            sample_tgt_data)
+          run_internal_eval(
+              eval_model, eval_sess, model_dir, hparams, summary_writer)
+
+        if global_step - last_external_eval_step >= steps_per_external_eval:
+          last_external_eval_step = global_step
+
+          # Save checkpoint
+          loaded_train_model.saver.save(
+              train_sess._sess._sess._sess._sess,
+              os.path.join(out_dir, "translate.ckpt"),
+              global_step=global_step)
+          run_sample_decode(infer_model, infer_sess,
+                            model_dir, hparams, summary_writer, sample_src_data,
+                            sample_tgt_data)
+          run_external_eval(
+              infer_model, infer_sess, model_dir,
+              hparams, summary_writer)
+
+          if avg_ckpts:
+            run_avg_external_eval(infer_model, infer_sess, model_dir, hparams,
+                                  summary_writer, global_step)
+
+      # Done training
+      # Now out of the training loop. Doing the rest of full and best evaluation
+      # by only rank 0
+      if hvd.rank() == 0:
+        loaded_train_model.saver.save(
+            train_sess._sess._sess._sess._sess,
+            os.path.join(out_dir, "translate.ckpt"),
+            global_step=global_step)
+
+        (result_summary, _, final_eval_metrics) = (
+            run_full_eval(
+                model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams,
+                summary_writer, sample_src_data, sample_tgt_data, avg_ckpts))
+        print_step_info("# Final, ", global_step, info, result_summary, log_f)
+        utils.print_time("# Done training!", start_train_time)
+
+        summary_writer.close()
+
+        utils.print_out("# Start evaluating saved best models.")
+        for metric in hparams.metrics:
+          best_model_dir = getattr(hparams, "best_" + metric + "_dir")
+          summary_writer = tf.summary.FileWriter(
+              os.path.join(best_model_dir, summary_name), infer_model.graph)
+          result_summary, best_global_step, _ = run_full_eval(
+              best_model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams,
+              summary_writer, sample_src_data, sample_tgt_data)
+          print_step_info("# Best %s, " % metric, best_global_step, info,
+                          result_summary, log_f)
+          summary_writer.close()
+
+          if avg_ckpts:
+            best_model_dir = getattr(hparams, "avg_best_" + metric + "_dir")
+            summary_writer = tf.summary.FileWriter(
+                os.path.join(best_model_dir, summary_name), infer_model.graph)
+            result_summary, best_global_step, _ = run_full_eval(
+                best_model_dir, infer_model, infer_sess, eval_model, eval_sess,
+                hparams, summary_writer, sample_src_data, sample_tgt_data)
+            print_step_info("# Averaged Best %s, " % metric, best_global_step, info,
+                            result_summary, log_f)
+            summary_writer.close()
+            return final_eval_metrics, global_step
 
 
 def _format_results(name, ppl, scores, metrics):
@@ -716,7 +738,7 @@ def _external_eval(model, global_step, sess, hparams, iterator,
   sess.run(iterator.initializer, feed_dict=iterator_feed_dict)
 
   output = os.path.join(out_dir, "output_%s" % label)
-  scores = nmt_utils.decode_and_evaluate(
+  scores,_,_ = nmt_utils.decode_and_evaluate(
       label,
       model,
       sess,
diff --git a/nmt/utils/iterator_utils.py b/nmt/utils/iterator_utils.py
index 31efb11..c08ff22 100644
--- a/nmt/utils/iterator_utils.py
+++ b/nmt/utils/iterator_utils.py
@@ -16,8 +16,11 @@
 from __future__ import print_function
 
 import collections
+import os
 
 import tensorflow as tf
+# Horovod
+import horovod.tensorflow as hvd
 
 from ..utils import vocab_utils
 
@@ -30,7 +33,7 @@ class BatchedInput(
     collections.namedtuple("BatchedInput",
                            ("initializer", "source", "target_input",
                             "target_output", "source_sequence_length",
-                            "target_sequence_length"))):
+                            "target_sequence_length", "global_batch_size"))):
   pass
 
 
@@ -86,13 +89,16 @@ def get_infer_iterator(src_dataset,
   batched_dataset = batching_func(src_dataset)
   batched_iter = batched_dataset.make_initializable_iterator()
   (src_ids, src_seq_len) = batched_iter.get_next()
+  global_batch_size = tf.shape(src_seq_len)[0]
+  if os.environ.get('GNMT_DEBUG_LOAD_BALANCE', None): src_seq_len = tf.Print(src_seq_len, [tf.reduce_max(src_seq_len), global_batch_size], "Infer [Max_Src_len][Batch_Size] = ")
   return BatchedInput(
       initializer=batched_iter.initializer,
       source=src_ids,
       target_input=None,
       target_output=None,
       source_sequence_length=src_seq_len,
-      target_sequence_length=None)
+      target_sequence_length=None,
+      global_batch_size=global_batch_size)
 
 
 def get_iterator(src_dataset,
@@ -126,10 +132,15 @@ def get_iterator(src_dataset,
 
   src_tgt_dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))
 
-  src_tgt_dataset = src_tgt_dataset.shard(num_shards, shard_index)
   if skip_count is not None:
     src_tgt_dataset = src_tgt_dataset.skip(skip_count)
 
+  # Horovod requires same random_seed across all ranks
+  if num_shards > 1:
+    if not random_seed: random_seed = 1
+
+  print("# num_shards = {} and shard_index = {}, using random_seed = {}".format(num_shards, shard_index, random_seed))
+
   src_tgt_dataset = src_tgt_dataset.shuffle(
       output_buffer_size, random_seed, reshuffle_each_iteration)
 
@@ -187,10 +198,12 @@ def get_iterator(src_dataset,
 
   src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size)
 
+  global_batch_size = batch_size * num_shards
+
   # Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)
   def batching_func(x):
     return x.padded_batch(
-        batch_size,
+        batch_size * num_shards,
         # The first three entries are the source and target line rows;
         # these have unknown-length vectors.  The last two entries are
         # the source and target row sizes; these are scalars.
@@ -232,17 +245,40 @@ def get_iterator(src_dataset,
 
     batched_dataset = src_tgt_dataset.apply(
         tf.contrib.data.group_by_window(
-            key_func=key_func, reduce_func=reduce_func, window_size=batch_size))
+            key_func=key_func, reduce_func=reduce_func, window_size=batch_size*num_shards))
 
   else:
     batched_dataset = batching_func(src_tgt_dataset)
+
+  # Add global batch_size
+  batched_dataset = batched_dataset.map(
+      lambda src, tgt_in, tgt_out, src_len, tgt_len: (src, tgt_in, tgt_out, src_len, tgt_len, tf.shape(src)[0]))
+  if num_shards > 1:
+    def slice_func(src, tgt_in, tgt_out, src_len, tgt_len, global_batch):
+
+      start = (shard_index * global_batch) // num_shards
+      end = ((shard_index + 1) * global_batch) // num_shards
+      size = end - start
+      return (tf.slice(src, [start, 0], [size, -1]),
+           tf.slice(tgt_in, [start, 0], [size, -1]),
+           tf.slice(tgt_out, [start, 0], [size, -1]),
+           tf.slice(src_len, [start], [size]),
+           tf.slice(tgt_len, [start], [size]),
+           global_batch)
+
+    batched_dataset = batched_dataset.filter(
+        lambda src, tgt_in, tgt_out, src_len, tgt_len, global_batch: tf.math.greater_equal(global_batch, num_shards))
+    batched_dataset = batched_dataset.map(slice_func, num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size)
+
   batched_iter = batched_dataset.make_initializable_iterator()
   (src_ids, tgt_input_ids, tgt_output_ids, src_seq_len,
-   tgt_seq_len) = (batched_iter.get_next())
+   tgt_seq_len, global_batch_size) = (batched_iter.get_next())
+  if os.environ.get('GNMT_DEBUG_LOAD_BALANCE', None): src_seq_len = tf.Print(src_seq_len, [tf.reduce_max(src_seq_len), tf.reduce_max(tgt_seq_len)], "Rank %2d: Max_seq_len = " % hvd.rank())
   return BatchedInput(
       initializer=batched_iter.initializer,
       source=src_ids,
       target_input=tgt_input_ids,
       target_output=tgt_output_ids,
       source_sequence_length=src_seq_len,
-      target_sequence_length=tgt_seq_len)
+      target_sequence_length=tgt_seq_len,
+      global_batch_size=global_batch_size)
diff --git a/nmt/utils/misc_utils.py b/nmt/utils/misc_utils.py
index 63dc5a6..09f4727 100644
--- a/nmt/utils/misc_utils.py
+++ b/nmt/utils/misc_utils.py
@@ -26,6 +26,7 @@ import time
 from distutils import version
 
 import numpy as np
+from distutils.version import LooseVersion, StrictVersion
 import tensorflow as tf
 
 
diff --git a/nmt/utils/nmt_utils.py b/nmt/utils/nmt_utils.py
index 2115de9..11164d2 100644
--- a/nmt/utils/nmt_utils.py
+++ b/nmt/utils/nmt_utils.py
@@ -38,9 +38,12 @@ def decode_and_evaluate(name,
                         tgt_eos,
                         num_translations_per_input=1,
                         decode=True,
-                        infer_mode="greedy"):
+                        infer_mode="greedy",
+                        index_pair=[]):
   """Decode a test set and compute a score according to the evaluation task."""
   # Decode
+  end_time = None
+  num_sentences = None
   if decode:
     utils.print_out("  decoding to output %s" % trans_file)
 
@@ -54,7 +57,7 @@ def decode_and_evaluate(name,
         num_translations_per_input = 1
       elif infer_mode == "beam_search":
         num_translations_per_input = min(num_translations_per_input, beam_width)
-
+      translation = []
       while True:
         try:
           nmt_outputs, _ = model.decode(sess)
@@ -66,17 +69,23 @@ def decode_and_evaluate(name,
 
           for sent_id in range(batch_size):
             for beam_id in range(num_translations_per_input):
-              translation = get_translation(
+              translation.append(get_translation(
                   nmt_outputs[beam_id],
                   sent_id,
                   tgt_eos=tgt_eos,
-                  subword_option=subword_option)
-              trans_f.write((translation + b"\n").decode("utf-8"))
+                  subword_option=subword_option))
         except tf.errors.OutOfRangeError:
+          end_time = time.time()
           utils.print_time(
               "  done, num sentences %d, num translations per input %d" %
               (num_sentences, num_translations_per_input), start_time)
           break
+      if len(index_pair) is 0:
+        for sentence in translation:
+          trans_f.write((sentence + b"\n").decode("utf-8"))
+      else:
+        for i in index_pair:
+          trans_f.write((translation[index_pair[i]] + b"\n").decode("utf-8"))
 
   # Evaluation
   evaluation_scores = {}
@@ -90,7 +99,7 @@ def decode_and_evaluate(name,
       evaluation_scores[metric] = score
       utils.print_out("  %s %s: %.1f" % (metric, name, score))
 
-  return evaluation_scores
+  return evaluation_scores, end_time, num_sentences
 
 
 def get_translation(nmt_outputs, sent_id, tgt_eos, subword_option):
