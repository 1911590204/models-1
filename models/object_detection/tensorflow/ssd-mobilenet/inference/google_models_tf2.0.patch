diff --git a/research/object_detection/anchor_generators/grid_anchor_generator.py b/research/object_detection/anchor_generators/grid_anchor_generator.py
index ba43f013..e340f9c0 100644
--- a/research/object_detection/anchor_generators/grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/grid_anchor_generator.py
@@ -56,13 +56,13 @@ class GridAnchorGenerator(anchor_generator.AnchorGenerator):
     # Handle argument defaults
     if base_anchor_size is None:
       base_anchor_size = [256, 256]
-    base_anchor_size = tf.to_float(tf.convert_to_tensor(base_anchor_size))
+    base_anchor_size = tf.cast(tf.convert_to_tensor(value=base_anchor_size), dtype=tf.float32)
     if anchor_stride is None:
       anchor_stride = [16, 16]
-    anchor_stride = tf.to_float(tf.convert_to_tensor(anchor_stride))
+    anchor_stride = tf.cast(tf.convert_to_tensor(value=anchor_stride), dtype=tf.float32)
     if anchor_offset is None:
       anchor_offset = [0, 0]
-    anchor_offset = tf.to_float(tf.convert_to_tensor(anchor_offset))
+    anchor_offset = tf.cast(tf.convert_to_tensor(value=anchor_offset), dtype=tf.float32)
 
     self._scales = scales
     self._aspect_ratios = aspect_ratios
@@ -175,9 +175,9 @@ def tile_anchors(grid_height,
   widths = scales * ratio_sqrts * base_anchor_size[1]
 
   # Get a grid of box centers
-  y_centers = tf.to_float(tf.range(grid_height))
+  y_centers = tf.cast(tf.range(grid_height), dtype=tf.float32)
   y_centers = y_centers * anchor_stride[0] + anchor_offset[0]
-  x_centers = tf.to_float(tf.range(grid_width))
+  x_centers = tf.cast(tf.range(grid_width), dtype=tf.float32)
   x_centers = x_centers * anchor_stride[1] + anchor_offset[1]
   x_centers, y_centers = ops.meshgrid(x_centers, y_centers)
 
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
index b870adce..a3e152ab 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
@@ -180,22 +180,22 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
                 for list_item in feature_map_shape_list]):
       raise ValueError('feature_map_shape_list must be a list of pairs.')
 
-    im_height = tf.to_float(im_height)
-    im_width = tf.to_float(im_width)
+    im_height = tf.cast(im_height, dtype=tf.float32)
+    im_width = tf.cast(im_width, dtype=tf.float32)
 
     if not self._anchor_strides:
-      anchor_strides = [(1.0 / tf.to_float(pair[0]), 1.0 / tf.to_float(pair[1]))
+      anchor_strides = [(1.0 / tf.cast(pair[0], dtype=tf.float32), 1.0 / tf.cast(pair[1], dtype=tf.float32))
                         for pair in feature_map_shape_list]
     else:
-      anchor_strides = [(tf.to_float(stride[0]) / im_height,
-                         tf.to_float(stride[1]) / im_width)
+      anchor_strides = [(tf.cast(stride[0], dtype=tf.float32) / im_height,
+                         tf.cast(stride[1], dtype=tf.float32) / im_width)
                         for stride in self._anchor_strides]
     if not self._anchor_offsets:
       anchor_offsets = [(0.5 * stride[0], 0.5 * stride[1])
                         for stride in anchor_strides]
     else:
-      anchor_offsets = [(tf.to_float(offset[0]) / im_height,
-                         tf.to_float(offset[1]) / im_width)
+      anchor_offsets = [(tf.cast(offset[0], dtype=tf.float32) / im_height,
+                         tf.cast(offset[1], dtype=tf.float32) / im_width)
                         for offset in self._anchor_offsets]
 
     for arg, arg_name in zip([anchor_strides, anchor_offsets],
diff --git a/research/object_detection/box_coders/faster_rcnn_box_coder.py b/research/object_detection/box_coders/faster_rcnn_box_coder.py
index af25e21a..3dc110f5 100644
--- a/research/object_detection/box_coders/faster_rcnn_box_coder.py
+++ b/research/object_detection/box_coders/faster_rcnn_box_coder.py
@@ -79,15 +79,15 @@ class FasterRcnnBoxCoder(box_coder.BoxCoder):
 
     tx = (xcenter - xcenter_a) / wa
     ty = (ycenter - ycenter_a) / ha
-    tw = tf.log(w / wa)
-    th = tf.log(h / ha)
+    tw = tf.math.log(w / wa)
+    th = tf.math.log(h / ha)
     # Scales location targets as used in paper for joint training.
     if self._scale_factors:
       ty *= self._scale_factors[0]
       tx *= self._scale_factors[1]
       th *= self._scale_factors[2]
       tw *= self._scale_factors[3]
-    return tf.transpose(tf.stack([ty, tx, th, tw]))
+    return tf.transpose(a=tf.stack([ty, tx, th, tw]))
 
   def _decode(self, rel_codes, anchors):
     """Decode relative codes to boxes.
@@ -101,7 +101,7 @@ class FasterRcnnBoxCoder(box_coder.BoxCoder):
     """
     ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()
 
-    ty, tx, th, tw = tf.unstack(tf.transpose(rel_codes))
+    ty, tx, th, tw = tf.unstack(tf.transpose(a=rel_codes))
     if self._scale_factors:
       ty /= self._scale_factors[0]
       tx /= self._scale_factors[1]
@@ -115,4 +115,4 @@ class FasterRcnnBoxCoder(box_coder.BoxCoder):
     xmin = xcenter - w / 2.
     ymax = ycenter + h / 2.
     xmax = xcenter + w / 2.
-    return box_list.BoxList(tf.transpose(tf.stack([ymin, xmin, ymax, xmax])))
+    return box_list.BoxList(tf.transpose(a=tf.stack([ymin, xmin, ymax, xmax])))
diff --git a/research/object_detection/box_coders/keypoint_box_coder.py b/research/object_detection/box_coders/keypoint_box_coder.py
index 67df3b82..dc2d71ee 100644
--- a/research/object_detection/box_coders/keypoint_box_coder.py
+++ b/research/object_detection/box_coders/keypoint_box_coder.py
@@ -67,7 +67,7 @@ class KeypointBoxCoder(box_coder.BoxCoder):
     self._keypoint_scale_factors = None
     if scale_factors is not None:
       self._keypoint_scale_factors = tf.expand_dims(tf.tile(
-          [tf.to_float(scale_factors[0]), tf.to_float(scale_factors[1])],
+          [tf.cast(scale_factors[0], dtype=tf.float32), tf.cast(scale_factors[1], dtype=tf.float32)],
           [num_keypoints]), 1)
 
   @property
@@ -93,7 +93,7 @@ class KeypointBoxCoder(box_coder.BoxCoder):
     ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()
     ycenter, xcenter, h, w = boxes.get_center_coordinates_and_sizes()
     keypoints = boxes.get_field(fields.BoxListFields.keypoints)
-    keypoints = tf.transpose(tf.reshape(keypoints,
+    keypoints = tf.transpose(a=tf.reshape(keypoints,
                                         [-1, self._num_keypoints * 2]))
     num_boxes = boxes.num_boxes()
 
@@ -105,8 +105,8 @@ class KeypointBoxCoder(box_coder.BoxCoder):
 
     tx = (xcenter - xcenter_a) / wa
     ty = (ycenter - ycenter_a) / ha
-    tw = tf.log(w / wa)
-    th = tf.log(h / ha)
+    tw = tf.math.log(w / wa)
+    th = tf.math.log(h / ha)
 
     tiled_anchor_centers = tf.tile(
         tf.stack([ycenter_a, xcenter_a]), [self._num_keypoints, 1])
@@ -123,7 +123,7 @@ class KeypointBoxCoder(box_coder.BoxCoder):
       tkeypoints *= tf.tile(self._keypoint_scale_factors, [1, num_boxes])
 
     tboxes = tf.stack([ty, tx, th, tw])
-    return tf.transpose(tf.concat([tboxes, tkeypoints], 0))
+    return tf.transpose(a=tf.concat([tboxes, tkeypoints], 0))
 
   def _decode(self, rel_codes, anchors):
     """Decode relative codes to boxes and keypoints.
@@ -138,8 +138,8 @@ class KeypointBoxCoder(box_coder.BoxCoder):
     """
     ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()
 
-    num_codes = tf.shape(rel_codes)[0]
-    result = tf.unstack(tf.transpose(rel_codes))
+    num_codes = tf.shape(input=rel_codes)[0]
+    result = tf.unstack(tf.transpose(a=rel_codes))
     ty, tx, th, tw = result[:4]
     tkeypoints = result[4:]
     if self._scale_factors:
@@ -158,14 +158,14 @@ class KeypointBoxCoder(box_coder.BoxCoder):
     ymax = ycenter + h / 2.
     xmax = xcenter + w / 2.
     decoded_boxes_keypoints = box_list.BoxList(
-        tf.transpose(tf.stack([ymin, xmin, ymax, xmax])))
+        tf.transpose(a=tf.stack([ymin, xmin, ymax, xmax])))
 
     tiled_anchor_centers = tf.tile(
         tf.stack([ycenter_a, xcenter_a]), [self._num_keypoints, 1])
     tiled_anchor_sizes = tf.tile(
         tf.stack([ha, wa]), [self._num_keypoints, 1])
     keypoints = tkeypoints * tiled_anchor_sizes + tiled_anchor_centers
-    keypoints = tf.reshape(tf.transpose(keypoints),
+    keypoints = tf.reshape(tf.transpose(a=keypoints),
                            [-1, self._num_keypoints, 2])
     decoded_boxes_keypoints.add_field(fields.BoxListFields.keypoints, keypoints)
     return decoded_boxes_keypoints
diff --git a/research/object_detection/box_coders/square_box_coder.py b/research/object_detection/box_coders/square_box_coder.py
index ee46b689..f503257a 100644
--- a/research/object_detection/box_coders/square_box_coder.py
+++ b/research/object_detection/box_coders/square_box_coder.py
@@ -90,13 +90,13 @@ class SquareBoxCoder(box_coder.BoxCoder):
 
     tx = (xcenter - xcenter_a) / la
     ty = (ycenter - ycenter_a) / la
-    tl = tf.log(l / la)
+    tl = tf.math.log(l / la)
     # Scales location targets for joint training.
     if self._scale_factors:
       ty *= self._scale_factors[0]
       tx *= self._scale_factors[1]
       tl *= self._scale_factors[2]
-    return tf.transpose(tf.stack([ty, tx, tl]))
+    return tf.transpose(a=tf.stack([ty, tx, tl]))
 
   def _decode(self, rel_codes, anchors):
     """Decodes relative codes to boxes.
@@ -111,7 +111,7 @@ class SquareBoxCoder(box_coder.BoxCoder):
     ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()
     la = tf.sqrt(ha * wa)
 
-    ty, tx, tl = tf.unstack(tf.transpose(rel_codes))
+    ty, tx, tl = tf.unstack(tf.transpose(a=rel_codes))
     if self._scale_factors:
       ty /= self._scale_factors[0]
       tx /= self._scale_factors[1]
@@ -123,4 +123,4 @@ class SquareBoxCoder(box_coder.BoxCoder):
     xmin = xcenter - l / 2.
     ymax = ycenter + l / 2.
     xmax = xcenter + l / 2.
-    return box_list.BoxList(tf.transpose(tf.stack([ymin, xmin, ymax, xmax])))
+    return box_list.BoxList(tf.transpose(a=tf.stack([ymin, xmin, ymax, xmax])))
diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
index 8af9d9cc..cc3e75c8 100644
--- a/research/object_detection/builders/dataset_builder.py
+++ b/research/object_detection/builders/dataset_builder.py
@@ -40,8 +40,8 @@ def make_initializable_iterator(dataset):
   Returns:
     A `tf.data.Iterator`.
   """
-  iterator = dataset.make_initializable_iterator()
-  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
+  iterator = tf.compat.v1.data.make_initializable_iterator(dataset)
+  tf.compat.v1.add_to_collection(tf.compat.v1.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
   return iterator
 
 
@@ -58,22 +58,22 @@ def read_dataset(file_read_func, input_files, config):
     A tf.data.Dataset of (undecoded) tf-records based on config.
   """
   # Shard, shuffle, and read files.
-  filenames = tf.gfile.Glob(input_files)
+  filenames = tf.io.gfile.glob(input_files)
   num_readers = config.num_readers
   if num_readers > len(filenames):
     num_readers = len(filenames)
-    tf.logging.warning('num_readers has been reduced to %d to match input file '
+    tf.compat.v1.logging.warning('num_readers has been reduced to %d to match input file '
                        'shards.' % num_readers)
   filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)
   if config.shuffle:
     filename_dataset = filename_dataset.shuffle(
         config.filenames_shuffle_buffer_size)
   elif num_readers > 1:
-    tf.logging.warning('`shuffle` is false, but the input data stream is '
+    tf.compat.v1.logging.warning('`shuffle` is false, but the input data stream is '
                        'still slightly shuffled since `num_readers` > 1.')
   filename_dataset = filename_dataset.repeat(config.num_epochs or None)
   records_dataset = filename_dataset.apply(
-      tf.contrib.data.parallel_interleave(
+      tf.data.experimental.parallel_interleave(
           file_read_func,
           cycle_length=num_readers,
           block_length=config.read_block_length,
diff --git a/research/object_detection/builders/dataset_builder_test.py b/research/object_detection/builders/dataset_builder_test.py
index cbcdb69c..23cdc409 100644
--- a/research/object_detection/builders/dataset_builder_test.py
+++ b/research/object_detection/builders/dataset_builder_test.py
@@ -31,7 +31,7 @@ class DatasetBuilderTest(tf.test.TestCase):
 
   def create_tf_record(self, has_additional_channels=False):
     path = os.path.join(self.get_temp_dir(), 'tfrecord')
-    writer = tf.python_io.TFRecordWriter(path)
+    writer = tf.io.TFRecordWriter(path)
 
     image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
     additional_channels_tensor = np.random.randint(
@@ -90,10 +90,10 @@ class DatasetBuilderTest(tf.test.TestCase):
     """.format(tf_record_path)
     input_reader_proto = input_reader_pb2.InputReader()
     text_format.Merge(input_reader_text_proto, input_reader_proto)
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
+    tensor_dict = tf.compat.v1.data.make_initializable_iterator(
+        dataset_builder, dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
 
-    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    sv = tf.compat.v1.train.Supervisor(logdir=self.get_temp_dir())
     with sv.prepare_or_wait_for_session() as sess:
       sv.start_queue_runners(sess)
       output_dict = sess.run(tensor_dict)
@@ -123,10 +123,10 @@ class DatasetBuilderTest(tf.test.TestCase):
     """.format(tf_record_path)
     input_reader_proto = input_reader_pb2.InputReader()
     text_format.Merge(input_reader_text_proto, input_reader_proto)
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
+    tensor_dict = tf.compat.v1.data.make_initializable_iterator(
+        dataset_builder, dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
 
-    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    sv = tf.compat.v1.train.Supervisor(logdir=self.get_temp_dir())
     with sv.prepare_or_wait_for_session() as sess:
       sv.start_queue_runners(sess)
       output_dict = sess.run(tensor_dict)
@@ -152,13 +152,13 @@ class DatasetBuilderTest(tf.test.TestCase):
           tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)
       return tensor_dict
 
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(
+    tensor_dict = tf.compat.v1.data.make_initializable_iterator(
+        dataset_builder, dataset_builder.build(
             input_reader_proto,
             transform_input_data_fn=one_hot_class_encoding_fn,
             batch_size=2)).get_next()
 
-    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    sv = tf.compat.v1.train.Supervisor(logdir=self.get_temp_dir())
     with sv.prepare_or_wait_for_session() as sess:
       sv.start_queue_runners(sess)
       output_dict = sess.run(tensor_dict)
@@ -195,13 +195,13 @@ class DatasetBuilderTest(tf.test.TestCase):
           tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)
       return tensor_dict
 
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(
+    tensor_dict = tf.compat.v1.data.make_initializable_iterator(
+        dataset_builder, dataset_builder.build(
             input_reader_proto,
             transform_input_data_fn=one_hot_class_encoding_fn,
             batch_size=2)).get_next()
 
-    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    sv = tf.compat.v1.train.Supervisor(logdir=self.get_temp_dir())
     with sv.prepare_or_wait_for_session() as sess:
       sv.start_queue_runners(sess)
       output_dict = sess.run(tensor_dict)
@@ -229,25 +229,25 @@ class ReadDatasetTest(tf.test.TestCase):
 
     for i in range(5):
       path = self._path_template % i
-      with tf.gfile.Open(path, 'wb') as f:
+      with tf.io.gfile.GFile(path, 'wb') as f:
         f.write('\n'.join([str(i + 1), str((i + 1) * 10)]))
 
     self._shuffle_path_template = os.path.join(self.get_temp_dir(),
                                                'shuffle_%s.txt')
     for i in range(2):
       path = self._shuffle_path_template % i
-      with tf.gfile.Open(path, 'wb') as f:
+      with tf.io.gfile.GFile(path, 'wb') as f:
         f.write('\n'.join([str(i)] * 5))
 
   def _get_dataset_next(self, files, config, batch_size):
     def decode_func(value):
-      return [tf.string_to_number(value, out_type=tf.int32)]
+      return [tf.strings.to_number(value, out_type=tf.int32)]
 
     dataset = dataset_builder.read_dataset(
         tf.data.TextLineDataset, files, config)
     dataset = dataset.map(decode_func)
     dataset = dataset.batch(batch_size)
-    return dataset.make_one_shot_iterator().get_next()
+    return tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()
 
   def test_make_initializable_iterator_with_hashTable(self):
     keys = [1, 0, -1]
@@ -258,8 +258,8 @@ class ReadDatasetTest(tf.test.TestCase):
             values=list(reversed(keys))),
         default_value=100)
     dataset = dataset.map(table.lookup)
-    data = dataset_builder.make_initializable_iterator(dataset).get_next()
-    init = tf.tables_initializer()
+    data = tf.compat.v1.data.make_initializable_iterator(dataset_builder, dataset).get_next()
+    init = tf.compat.v1.tables_initializer()
 
     with self.test_session() as sess:
       sess.run(init)
@@ -294,7 +294,7 @@ class ReadDatasetTest(tf.test.TestCase):
     config.num_readers = 1
     config.shuffle = True
 
-    tf.set_random_seed(1)  # Set graph level seed.
+    tf.compat.v1.set_random_seed(1)  # Set graph level seed.
     data = self._get_dataset_next(
         [self._shuffle_path_template % '*'], config, batch_size=10)
     expected_non_shuffle_output = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
diff --git a/research/object_detection/builders/graph_rewriter_builder.py b/research/object_detection/builders/graph_rewriter_builder.py
index 77e60479..4e800845 100644
--- a/research/object_detection/builders/graph_rewriter_builder.py
+++ b/research/object_detection/builders/graph_rewriter_builder.py
@@ -33,10 +33,10 @@ def build(graph_rewriter_config, is_training):
     # Quantize the graph by inserting quantize ops for weights and activations
     if is_training:
       tf.contrib.quantize.create_training_graph(
-          input_graph=tf.get_default_graph(),
+          input_graph=tf.compat.v1.get_default_graph(),
           quant_delay=graph_rewriter_config.quantization.delay)
     else:
-      tf.contrib.quantize.create_eval_graph(input_graph=tf.get_default_graph())
+      tf.contrib.quantize.create_eval_graph(input_graph=tf.compat.v1.get_default_graph())
 
     tf.contrib.layers.summarize_collection('quant_vars')
   return graph_rewrite_fn
diff --git a/research/object_detection/builders/graph_rewriter_builder_test.py b/research/object_detection/builders/graph_rewriter_builder_test.py
index 5f38d5a2..334428c1 100644
--- a/research/object_detection/builders/graph_rewriter_builder_test.py
+++ b/research/object_detection/builders/graph_rewriter_builder_test.py
@@ -34,7 +34,7 @@ class QuantizationBuilderTest(tf.test.TestCase):
             graph_rewriter_proto, is_training=True)
         graph_rewrite_fn()
         _, kwargs = mock_quant_fn.call_args
-        self.assertEqual(kwargs['input_graph'], tf.get_default_graph())
+        self.assertEqual(kwargs['input_graph'], tf.compat.v1.get_default_graph())
         self.assertEqual(kwargs['quant_delay'], 10)
         mock_summarize_col.assert_called_with('quant_vars')
 
@@ -49,7 +49,7 @@ class QuantizationBuilderTest(tf.test.TestCase):
             graph_rewriter_proto, is_training=False)
         graph_rewrite_fn()
         _, kwargs = mock_quant_fn.call_args
-        self.assertEqual(kwargs['input_graph'], tf.get_default_graph())
+        self.assertEqual(kwargs['input_graph'], tf.compat.v1.get_default_graph())
         mock_summarize_col.assert_called_with('quant_vars')
 
 
diff --git a/research/object_detection/builders/hyperparams_builder.py b/research/object_detection/builders/hyperparams_builder.py
index bb1a94a4..d2f3c1a4 100644
--- a/research/object_detection/builders/hyperparams_builder.py
+++ b/research/object_detection/builders/hyperparams_builder.py
@@ -240,9 +240,9 @@ def _build_slim_regularizer(regularizer):
   """
   regularizer_oneof = regularizer.WhichOneof('regularizer_oneof')
   if  regularizer_oneof == 'l1_regularizer':
-    return slim.l1_regularizer(scale=float(regularizer.l1_regularizer.weight))
+    return tf.keras.regularizers.l1(l=float(regularizer.l1_regularizer.weight))
   if regularizer_oneof == 'l2_regularizer':
-    return slim.l2_regularizer(scale=float(regularizer.l2_regularizer.weight))
+    return tf.keras.regularizers.l2(l=0.5 * (float(regularizer.l2_regularizer.weight)))
   raise ValueError('Unknown regularizer function: {}'.format(regularizer_oneof))
 
 
@@ -285,11 +285,11 @@ def _build_initializer(initializer, build_for_keras=False):
   """
   initializer_oneof = initializer.WhichOneof('initializer_oneof')
   if initializer_oneof == 'truncated_normal_initializer':
-    return tf.truncated_normal_initializer(
+    return tf.compat.v1.truncated_normal_initializer(
         mean=initializer.truncated_normal_initializer.mean,
         stddev=initializer.truncated_normal_initializer.stddev)
   if initializer_oneof == 'random_normal_initializer':
-    return tf.random_normal_initializer(
+    return tf.compat.v1.random_normal_initializer(
         mean=initializer.random_normal_initializer.mean,
         stddev=initializer.random_normal_initializer.stddev)
   if initializer_oneof == 'variance_scaling_initializer':
@@ -300,7 +300,7 @@ def _build_initializer(initializer, build_for_keras=False):
                                             mode].name
     if build_for_keras:
       if initializer.variance_scaling_initializer.uniform:
-        return tf.variance_scaling_initializer(
+        return tf.compat.v1.variance_scaling_initializer(
             scale=initializer.variance_scaling_initializer.factor,
             mode=mode.lower(),
             distribution='uniform')
@@ -315,7 +315,7 @@ def _build_initializer(initializer, build_for_keras=False):
         # creates a truncated distribution, whereas it created untruncated
         # distributions in older versions.
         try:
-          return tf.variance_scaling_initializer(
+          return tf.compat.v1.variance_scaling_initializer(
               scale=initializer.variance_scaling_initializer.factor,
               mode=mode.lower(),
               distribution='truncated_normal')
@@ -324,16 +324,16 @@ def _build_initializer(initializer, build_for_keras=False):
           truncated_scale = initializer.variance_scaling_initializer.factor / (
               truncate_constant * truncate_constant
           )
-          return tf.variance_scaling_initializer(
+          return tf.compat.v1.variance_scaling_initializer(
               scale=truncated_scale,
               mode=mode.lower(),
               distribution='normal')
 
     else:
-      return slim.variance_scaling_initializer(
-          factor=initializer.variance_scaling_initializer.factor,
-          mode=mode,
-          uniform=initializer.variance_scaling_initializer.uniform)
+      return tf.compat.v1.keras.initializers.VarianceScaling(
+          scale=initializer.variance_scaling_initializer.factor,
+          mode=(mode).lower(),
+          distribution=("uniform" if initializer.variance_scaling_initializer.uniform else "truncated_normal"))
   raise ValueError('Unknown initializer function: {}'.format(
       initializer_oneof))
 
diff --git a/research/object_detection/builders/hyperparams_builder_test.py b/research/object_detection/builders/hyperparams_builder_test.py
index 2375dc41..aa1728ad 100644
--- a/research/object_detection/builders/hyperparams_builder_test.py
+++ b/research/object_detection/builders/hyperparams_builder_test.py
@@ -560,12 +560,12 @@ class HyperparamsBuilderTest(tf.test.TestCase):
                                 tol=1e-2):
     with tf.Graph().as_default() as g:
       with self.test_session(graph=g) as sess:
-        var = tf.get_variable(
+        var = tf.compat.v1.get_variable(
             name='test',
             shape=shape,
             dtype=tf.float32,
             initializer=initializer)
-        sess.run(tf.global_variables_initializer())
+        sess.run(tf.compat.v1.global_variables_initializer())
         values = sess.run(var)
         self.assertAllClose(np.var(values), variance, tol, tol)
 
diff --git a/research/object_detection/builders/image_resizer_builder_test.py b/research/object_detection/builders/image_resizer_builder_test.py
index f7da1912..f0608285 100644
--- a/research/object_detection/builders/image_resizer_builder_test.py
+++ b/research/object_detection/builders/image_resizer_builder_test.py
@@ -27,8 +27,8 @@ class ImageResizerBuilderTest(tf.test.TestCase):
     image_resizer_config = image_resizer_pb2.ImageResizer()
     text_format.Merge(text_proto, image_resizer_config)
     image_resizer_fn = image_resizer_builder.build(image_resizer_config)
-    images = tf.to_float(
-        tf.random_uniform(input_shape, minval=0, maxval=255, dtype=tf.int32))
+    images = tf.cast(
+        tf.random.uniform(input_shape, minval=0, maxval=255, dtype=tf.int32), dtype=tf.float32)
     resized_images, _ = image_resizer_fn(images)
     with self.test_session() as sess:
       return sess.run(resized_images).shape
@@ -113,7 +113,7 @@ class ImageResizerBuilderTest(tf.test.TestCase):
     image_resizer_config = image_resizer_pb2.ImageResizer()
     text_format.Merge(text_proto, image_resizer_config)
     image_resizer_fn = image_resizer_builder.build(image_resizer_config)
-    image_placeholder = tf.placeholder(tf.uint8, [1, None, None, 3])
+    image_placeholder = tf.compat.v1.placeholder(tf.uint8, [1, None, None, 3])
     resized_image, _ = image_resizer_fn(image_placeholder)
     with self.test_session() as sess:
       return sess.run(resized_image, feed_dict={image_placeholder: image})
diff --git a/research/object_detection/builders/input_reader_builder.py b/research/object_detection/builders/input_reader_builder.py
index 8cb5e2f0..b2a6f607 100644
--- a/research/object_detection/builders/input_reader_builder.py
+++ b/research/object_detection/builders/input_reader_builder.py
@@ -55,7 +55,7 @@ def build(input_reader_config):
                        '`input_reader_config`.')
     _, string_tensor = parallel_reader.parallel_read(
         config.input_path[:],  # Convert `RepeatedScalarContainer` to list.
-        reader_class=tf.TFRecordReader,
+        reader_class=tf.compat.v1.TFRecordReader,
         num_epochs=(input_reader_config.num_epochs
                     if input_reader_config.num_epochs else None),
         num_readers=input_reader_config.num_readers,
diff --git a/research/object_detection/builders/input_reader_builder_test.py b/research/object_detection/builders/input_reader_builder_test.py
index f09f60e5..2bcea2e6 100644
--- a/research/object_detection/builders/input_reader_builder_test.py
+++ b/research/object_detection/builders/input_reader_builder_test.py
@@ -32,7 +32,7 @@ class InputReaderBuilderTest(tf.test.TestCase):
 
   def create_tf_record(self):
     path = os.path.join(self.get_temp_dir(), 'tfrecord')
-    writer = tf.python_io.TFRecordWriter(path)
+    writer = tf.io.TFRecordWriter(path)
 
     image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
     flat_mask = (4 * 5) * [1.0]
@@ -79,7 +79,7 @@ class InputReaderBuilderTest(tf.test.TestCase):
     text_format.Merge(input_reader_text_proto, input_reader_proto)
     tensor_dict = input_reader_builder.build(input_reader_proto)
 
-    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    sv = tf.compat.v1.train.Supervisor(logdir=self.get_temp_dir())
     with sv.prepare_or_wait_for_session() as sess:
       sv.start_queue_runners(sess)
       output_dict = sess.run(tensor_dict)
@@ -111,7 +111,7 @@ class InputReaderBuilderTest(tf.test.TestCase):
     text_format.Merge(input_reader_text_proto, input_reader_proto)
     tensor_dict = input_reader_builder.build(input_reader_proto)
 
-    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    sv = tf.compat.v1.train.Supervisor(logdir=self.get_temp_dir())
     with sv.prepare_or_wait_for_session() as sess:
       sv.start_queue_runners(sess)
       output_dict = sess.run(tensor_dict)
diff --git a/research/object_detection/builders/optimizer_builder.py b/research/object_detection/builders/optimizer_builder.py
index ce64bfe6..bffc32f9 100644
--- a/research/object_detection/builders/optimizer_builder.py
+++ b/research/object_detection/builders/optimizer_builder.py
@@ -39,7 +39,7 @@ def build(optimizer_config):
     config = optimizer_config.rms_prop_optimizer
     learning_rate = _create_learning_rate(config.learning_rate)
     summary_vars.append(learning_rate)
-    optimizer = tf.train.RMSPropOptimizer(
+    optimizer = tf.compat.v1.train.RMSPropOptimizer(
         learning_rate,
         decay=config.decay,
         momentum=config.momentum_optimizer_value,
@@ -49,7 +49,7 @@ def build(optimizer_config):
     config = optimizer_config.momentum_optimizer
     learning_rate = _create_learning_rate(config.learning_rate)
     summary_vars.append(learning_rate)
-    optimizer = tf.train.MomentumOptimizer(
+    optimizer = tf.compat.v1.train.MomentumOptimizer(
         learning_rate,
         momentum=config.momentum_optimizer_value)
 
@@ -57,7 +57,7 @@ def build(optimizer_config):
     config = optimizer_config.adam_optimizer
     learning_rate = _create_learning_rate(config.learning_rate)
     summary_vars.append(learning_rate)
-    optimizer = tf.train.AdamOptimizer(learning_rate)
+    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)
 
   if optimizer is None:
     raise ValueError('Optimizer %s not supported.' % optimizer_type)
@@ -91,7 +91,7 @@ def _create_learning_rate(learning_rate_config):
   if learning_rate_type == 'exponential_decay_learning_rate':
     config = learning_rate_config.exponential_decay_learning_rate
     learning_rate = learning_schedules.exponential_decay_with_burnin(
-        tf.train.get_or_create_global_step(),
+        tf.compat.v1.train.get_or_create_global_step(),
         config.initial_learning_rate,
         config.decay_steps,
         config.decay_factor,
@@ -108,13 +108,13 @@ def _create_learning_rate(learning_rate_config):
     learning_rate_sequence = [config.initial_learning_rate]
     learning_rate_sequence += [x.learning_rate for x in config.schedule]
     learning_rate = learning_schedules.manual_stepping(
-        tf.train.get_or_create_global_step(), learning_rate_step_boundaries,
+        tf.compat.v1.train.get_or_create_global_step(), learning_rate_step_boundaries,
         learning_rate_sequence, config.warmup)
 
   if learning_rate_type == 'cosine_decay_learning_rate':
     config = learning_rate_config.cosine_decay_learning_rate
     learning_rate = learning_schedules.cosine_decay_with_warmup(
-        tf.train.get_or_create_global_step(),
+        tf.compat.v1.train.get_or_create_global_step(),
         config.learning_rate_base,
         config.total_steps,
         config.warmup_learning_rate,
diff --git a/research/object_detection/builders/optimizer_builder_test.py b/research/object_detection/builders/optimizer_builder_test.py
index 343a858f..f58d0212 100644
--- a/research/object_detection/builders/optimizer_builder_test.py
+++ b/research/object_detection/builders/optimizer_builder_test.py
@@ -123,7 +123,7 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
     optimizer, _ = optimizer_builder.build(optimizer_proto)
-    self.assertTrue(isinstance(optimizer, tf.train.RMSPropOptimizer))
+    self.assertTrue(isinstance(optimizer, tf.compat.v1.train.RMSPropOptimizer))
 
   def testBuildMomentumOptimizer(self):
     optimizer_text_proto = """
@@ -140,7 +140,7 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
     optimizer, _ = optimizer_builder.build(optimizer_proto)
-    self.assertTrue(isinstance(optimizer, tf.train.MomentumOptimizer))
+    self.assertTrue(isinstance(optimizer, tf.compat.v1.train.MomentumOptimizer))
 
   def testBuildAdamOptimizer(self):
     optimizer_text_proto = """
@@ -156,7 +156,7 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
     optimizer, _ = optimizer_builder.build(optimizer_proto)
-    self.assertTrue(isinstance(optimizer, tf.train.AdamOptimizer))
+    self.assertTrue(isinstance(optimizer, tf.compat.v1.train.AdamOptimizer))
 
   def testBuildMovingAverageOptimizer(self):
     optimizer_text_proto = """
diff --git a/research/object_detection/core/anchor_generator.py b/research/object_detection/core/anchor_generator.py
index f2797ef7..e7427291 100644
--- a/research/object_detection/core/anchor_generator.py
+++ b/research/object_detection/core/anchor_generator.py
@@ -99,7 +99,7 @@ class AnchorGenerator(object):
         len(feature_map_shape_list) != len(self.num_anchors_per_location())):
       raise ValueError('Number of feature maps is expected to equal the length '
                        'of `num_anchors_per_location`.')
-    with tf.name_scope(self.name_scope()):
+    with tf.compat.v1.name_scope(self.name_scope()):
       anchors_list = self._generate(feature_map_shape_list, **params)
       if self.check_num_anchors:
         with tf.control_dependencies([
@@ -146,5 +146,5 @@ class AnchorGenerator(object):
                                * feature_map_shape[0]
                                * feature_map_shape[1])
       actual_num_anchors += anchors.num_boxes()
-    return tf.assert_equal(expected_num_anchors, actual_num_anchors)
+    return tf.compat.v1.assert_equal(expected_num_anchors, actual_num_anchors)
 
diff --git a/research/object_detection/core/balanced_positive_negative_sampler.py b/research/object_detection/core/balanced_positive_negative_sampler.py
index 90b121c0..c838d0c2 100644
--- a/research/object_detection/core/balanced_positive_negative_sampler.py
+++ b/research/object_detection/core/balanced_positive_negative_sampler.py
@@ -67,10 +67,10 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
       A tuple containing the number of positive and negative labels in the
       subsample.
     """
-    input_length = tf.shape(sorted_indices_tensor)[0]
+    input_length = tf.shape(input=sorted_indices_tensor)[0]
     valid_positive_index = tf.greater(sorted_indices_tensor,
                                       tf.zeros(input_length, tf.int32))
-    num_sampled_pos = tf.reduce_sum(tf.cast(valid_positive_index, tf.int32))
+    num_sampled_pos = tf.reduce_sum(input_tensor=tf.cast(valid_positive_index, tf.int32))
     max_num_positive_samples = tf.constant(
         int(sample_size * self._positive_fraction), tf.int32)
     num_positive_samples = tf.minimum(max_num_positive_samples, num_sampled_pos)
@@ -97,7 +97,7 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
       from input_tensor.
 
     """
-    input_length = tf.shape(input_tensor)[0]
+    input_length = tf.shape(input=input_tensor)[0]
     start_positions = tf.less(tf.range(input_length), num_start_samples)
     end_positions = tf.greater_equal(
         tf.range(input_length), input_length - num_end_samples)
@@ -138,23 +138,23 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
       raise ValueError('batch_size has to be an integer when is_static is'
                        'True.')
 
-    input_length = tf.shape(indicator)[0]
+    input_length = tf.shape(input=indicator)[0]
 
     # Shuffle indicator and label. Need to store the permutation to restore the
     # order post sampling.
-    permutation = tf.random_shuffle(tf.range(input_length))
+    permutation = tf.random.shuffle(tf.range(input_length))
     indicator = ops.matmul_gather_on_zeroth_axis(
         tf.cast(indicator, tf.float32), permutation)
     labels = ops.matmul_gather_on_zeroth_axis(
         tf.cast(labels, tf.float32), permutation)
 
     # index (starting from 1) when cls_weight is True, 0 when False
-    indicator_idx = tf.where(
+    indicator_idx = tf.compat.v1.where(
         tf.cast(indicator, tf.bool), tf.range(1, input_length + 1),
         tf.zeros(input_length, tf.int32))
 
     # Replace -1 for negative, +1 for positive labels
-    signed_label = tf.where(
+    signed_label = tf.compat.v1.where(
         tf.cast(labels, tf.bool), tf.ones(input_length, tf.int32),
         tf.scalar_mul(-1, tf.ones(input_length, tf.int32)))
     # negative of index for negative label, positive index for positive label,
@@ -179,7 +179,7 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
         sampled_idx)
 
     sampled_idx_indicator = tf.cast(tf.reduce_sum(
-        tf.one_hot(sampled_idx, depth=input_length),
+        input_tensor=tf.one_hot(sampled_idx, depth=input_length),
         axis=0), tf.bool)
 
     # project back the order based on stored permutations
@@ -219,7 +219,7 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
     if indicator.dtype != tf.bool:
       raise ValueError('indicator should be of type bool. Received: %s' %
                        indicator.dtype)
-    with tf.name_scope(scope, 'BalancedPositiveNegativeSampler'):
+    with tf.compat.v1.name_scope(scope, 'BalancedPositiveNegativeSampler'):
       if self._is_static:
         return self._static_subsample(indicator, batch_size, labels)
 
@@ -231,16 +231,16 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
 
         # Sample positive and negative samples separately
         if batch_size is None:
-          max_num_pos = tf.reduce_sum(tf.to_int32(positive_idx))
+          max_num_pos = tf.reduce_sum(input_tensor=tf.cast(positive_idx, dtype=tf.int32))
         else:
           max_num_pos = int(self._positive_fraction * batch_size)
         sampled_pos_idx = self.subsample_indicator(positive_idx, max_num_pos)
-        num_sampled_pos = tf.reduce_sum(tf.cast(sampled_pos_idx, tf.int32))
+        num_sampled_pos = tf.reduce_sum(input_tensor=tf.cast(sampled_pos_idx, tf.int32))
         if batch_size is None:
           negative_positive_ratio = (
               1 - self._positive_fraction) / self._positive_fraction
-          max_num_neg = tf.to_int32(
-              negative_positive_ratio * tf.to_float(num_sampled_pos))
+          max_num_neg = tf.cast(
+              negative_positive_ratio * tf.cast(num_sampled_pos, dtype=tf.float32), dtype=tf.int32)
         else:
           max_num_neg = batch_size - num_sampled_pos
         sampled_neg_idx = self.subsample_indicator(negative_idx, max_num_neg)
diff --git a/research/object_detection/core/batcher.py b/research/object_detection/core/batcher.py
index c5dfb712..bb542d3d 100644
--- a/research/object_detection/core/batcher.py
+++ b/research/object_detection/core/batcher.py
@@ -83,12 +83,12 @@ class BatchQueue(object):
         {key: tensor.get_shape() for key, tensor in tensor_dict.items()})
     # Remember runtime shapes to unpad tensors after batching.
     runtime_shapes = collections.OrderedDict(
-        {(key + rt_shape_str): tf.shape(tensor)
+        {(key + rt_shape_str): tf.shape(input=tensor)
          for key, tensor in tensor_dict.items()})
 
     all_tensors = tensor_dict
     all_tensors.update(runtime_shapes)
-    batched_tensors = tf.train.batch(
+    batched_tensors = tf.compat.v1.train.batch(
         all_tensors,
         capacity=batch_queue_capacity,
         batch_size=batch_size,
diff --git a/research/object_detection/core/batcher_test.py b/research/object_detection/core/batcher_test.py
index 61b4390b..ed7ab981 100644
--- a/research/object_detection/core/batcher_test.py
+++ b/research/object_detection/core/batcher_test.py
@@ -45,7 +45,7 @@ class BatcherTest(tf.test.TestCase):
         for tensor in tensor_dict.values():
           self.assertAllEqual([None, 4], tensor.get_shape().as_list())
 
-      tf.initialize_all_variables().run()
+      tf.compat.v1.initialize_all_variables().run()
       with slim.queues.QueueRunners(sess):
         i = 2
         for _ in range(num_batches):
@@ -78,7 +78,7 @@ class BatcherTest(tf.test.TestCase):
         for tensor in tensor_dict.values():
           self.assertAllEqual([None, None], tensor.get_shape().as_list())
 
-      tf.initialize_all_variables().run()
+      tf.compat.v1.initialize_all_variables().run()
       with slim.queues.QueueRunners(sess):
         i = 2
         for _ in range(num_batches):
@@ -109,7 +109,7 @@ class BatcherTest(tf.test.TestCase):
         for tensor in tensor_dict.values():
           self.assertAllEqual([4, 3], tensor.get_shape().as_list())
 
-      tf.initialize_all_variables().run()
+      tf.compat.v1.initialize_all_variables().run()
       with slim.queues.QueueRunners(sess):
         i = 1
         for _ in range(num_batches):
@@ -141,7 +141,7 @@ class BatcherTest(tf.test.TestCase):
         for tensor in tensor_dict.values():
           self.assertAllEqual([None, None], tensor.get_shape().as_list())
 
-      tf.initialize_all_variables().run()
+      tf.compat.v1.initialize_all_variables().run()
       with slim.queues.QueueRunners(sess):
         i = 2
         for _ in range(num_batches):
diff --git a/research/object_detection/core/box_coder.py b/research/object_detection/core/box_coder.py
index f20ac956..95f189b5 100644
--- a/research/object_detection/core/box_coder.py
+++ b/research/object_detection/core/box_coder.py
@@ -68,7 +68,7 @@ class BoxCoder(object):
     Returns:
       a tensor representing N relative-encoded boxes
     """
-    with tf.name_scope('Encode'):
+    with tf.compat.v1.name_scope('Encode'):
       return self._encode(boxes, anchors)
 
   def decode(self, rel_codes, anchors):
@@ -82,7 +82,7 @@ class BoxCoder(object):
       boxlist: BoxList holding N boxes encoded in the ordinary way (i.e.,
         with corners y_min, x_min, y_max, x_max)
     """
-    with tf.name_scope('Decode'):
+    with tf.compat.v1.name_scope('Decode'):
       return self._decode(rel_codes, anchors)
 
   @abstractmethod
diff --git a/research/object_detection/core/box_list.py b/research/object_detection/core/box_list.py
index c0196f05..5b7e983f 100644
--- a/research/object_detection/core/box_list.py
+++ b/research/object_detection/core/box_list.py
@@ -62,7 +62,7 @@ class BoxList(object):
     Returns:
       a tensor representing the number of boxes held in the collection.
     """
-    return tf.shape(self.data['boxes'])[0]
+    return tf.shape(input=self.data['boxes'])[0]
 
   def num_boxes_static(self):
     """Returns number of boxes held in collection.
@@ -164,9 +164,9 @@ class BoxList(object):
     Returns:
       a list of 4 1-D tensors [ycenter, xcenter, height, width].
     """
-    with tf.name_scope(scope, 'get_center_coordinates_and_sizes'):
+    with tf.compat.v1.name_scope(scope, 'get_center_coordinates_and_sizes'):
       box_corners = self.get()
-      ymin, xmin, ymax, xmax = tf.unstack(tf.transpose(box_corners))
+      ymin, xmin, ymax, xmax = tf.unstack(tf.transpose(a=box_corners))
       width = xmax - xmin
       height = ymax - ymin
       ycenter = ymin + height / 2.
@@ -179,7 +179,7 @@ class BoxList(object):
     Args:
       scope: name scope of the function.
     """
-    with tf.name_scope(scope, 'transpose_coordinates'):
+    with tf.compat.v1.name_scope(scope, 'transpose_coordinates'):
       y_min, x_min, y_max, x_max = tf.split(
           value=self.get(), num_or_size_splits=4, axis=1)
       self.set(tf.concat([x_min, y_min, x_max, y_max], 1))
diff --git a/research/object_detection/core/box_list_ops.py b/research/object_detection/core/box_list_ops.py
index a755ef68..2c2cb493 100644
--- a/research/object_detection/core/box_list_ops.py
+++ b/research/object_detection/core/box_list_ops.py
@@ -50,7 +50,7 @@ def area(boxlist, scope=None):
   Returns:
     a tensor with shape [N] representing box areas.
   """
-  with tf.name_scope(scope, 'Area'):
+  with tf.compat.v1.name_scope(scope, 'Area'):
     y_min, x_min, y_max, x_max = tf.split(
         value=boxlist.get(), num_or_size_splits=4, axis=1)
     return tf.squeeze((y_max - y_min) * (x_max - x_min), [1])
@@ -67,7 +67,7 @@ def height_width(boxlist, scope=None):
     Height: A tensor with shape [N] representing box heights.
     Width: A tensor with shape [N] representing box widths.
   """
-  with tf.name_scope(scope, 'HeightWidth'):
+  with tf.compat.v1.name_scope(scope, 'HeightWidth'):
     y_min, x_min, y_max, x_max = tf.split(
         value=boxlist.get(), num_or_size_splits=4, axis=1)
     return tf.squeeze(y_max - y_min, [1]), tf.squeeze(x_max - x_min, [1])
@@ -85,7 +85,7 @@ def scale(boxlist, y_scale, x_scale, scope=None):
   Returns:
     boxlist: BoxList holding N boxes
   """
-  with tf.name_scope(scope, 'Scale'):
+  with tf.compat.v1.name_scope(scope, 'Scale'):
     y_scale = tf.cast(y_scale, tf.float32)
     x_scale = tf.cast(x_scale, tf.float32)
     y_min, x_min, y_max, x_max = tf.split(
@@ -117,7 +117,7 @@ def clip_to_window(boxlist, window, filter_nonoverlapping=True, scope=None):
   Returns:
     a BoxList holding M_out boxes where M_out <= M_in
   """
-  with tf.name_scope(scope, 'ClipToWindow'):
+  with tf.compat.v1.name_scope(scope, 'ClipToWindow'):
     y_min, x_min, y_max, x_max = tf.split(
         value=boxlist.get(), num_or_size_splits=4, axis=1)
     win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)
@@ -132,7 +132,7 @@ def clip_to_window(boxlist, window, filter_nonoverlapping=True, scope=None):
     if filter_nonoverlapping:
       areas = area(clipped)
       nonzero_area_indices = tf.cast(
-          tf.reshape(tf.where(tf.greater(areas, 0.0)), [-1]), tf.int32)
+          tf.reshape(tf.compat.v1.where(tf.greater(areas, 0.0)), [-1]), tf.int32)
       clipped = gather(clipped, nonzero_area_indices)
     return clipped
 
@@ -156,7 +156,7 @@ def prune_outside_window(boxlist, window, scope=None):
     valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes
      in the input tensor.
   """
-  with tf.name_scope(scope, 'PruneOutsideWindow'):
+  with tf.compat.v1.name_scope(scope, 'PruneOutsideWindow'):
     y_min, x_min, y_max, x_max = tf.split(
         value=boxlist.get(), num_or_size_splits=4, axis=1)
     win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)
@@ -165,7 +165,7 @@ def prune_outside_window(boxlist, window, scope=None):
         tf.greater(y_max, win_y_max), tf.greater(x_max, win_x_max)
     ], 1)
     valid_indices = tf.reshape(
-        tf.where(tf.logical_not(tf.reduce_any(coordinate_violations, 1))), [-1])
+        tf.compat.v1.where(tf.logical_not(tf.reduce_any(input_tensor=coordinate_violations, axis=1))), [-1])
     return gather(boxlist, valid_indices), valid_indices
 
 
@@ -188,7 +188,7 @@ def prune_completely_outside_window(boxlist, window, scope=None):
     valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes
      in the input tensor.
   """
-  with tf.name_scope(scope, 'PruneCompleteleyOutsideWindow'):
+  with tf.compat.v1.name_scope(scope, 'PruneCompleteleyOutsideWindow'):
     y_min, x_min, y_max, x_max = tf.split(
         value=boxlist.get(), num_or_size_splits=4, axis=1)
     win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)
@@ -197,7 +197,7 @@ def prune_completely_outside_window(boxlist, window, scope=None):
         tf.less_equal(y_max, win_y_min), tf.less_equal(x_max, win_x_min)
     ], 1)
     valid_indices = tf.reshape(
-        tf.where(tf.logical_not(tf.reduce_any(coordinate_violations, 1))), [-1])
+        tf.compat.v1.where(tf.logical_not(tf.reduce_any(input_tensor=coordinate_violations, axis=1))), [-1])
     return gather(boxlist, valid_indices), valid_indices
 
 
@@ -212,16 +212,16 @@ def intersection(boxlist1, boxlist2, scope=None):
   Returns:
     a tensor with shape [N, M] representing pairwise intersections
   """
-  with tf.name_scope(scope, 'Intersection'):
+  with tf.compat.v1.name_scope(scope, 'Intersection'):
     y_min1, x_min1, y_max1, x_max1 = tf.split(
         value=boxlist1.get(), num_or_size_splits=4, axis=1)
     y_min2, x_min2, y_max2, x_max2 = tf.split(
         value=boxlist2.get(), num_or_size_splits=4, axis=1)
-    all_pairs_min_ymax = tf.minimum(y_max1, tf.transpose(y_max2))
-    all_pairs_max_ymin = tf.maximum(y_min1, tf.transpose(y_min2))
+    all_pairs_min_ymax = tf.minimum(y_max1, tf.transpose(a=y_max2))
+    all_pairs_max_ymin = tf.maximum(y_min1, tf.transpose(a=y_min2))
     intersect_heights = tf.maximum(0.0, all_pairs_min_ymax - all_pairs_max_ymin)
-    all_pairs_min_xmax = tf.minimum(x_max1, tf.transpose(x_max2))
-    all_pairs_max_xmin = tf.maximum(x_min1, tf.transpose(x_min2))
+    all_pairs_min_xmax = tf.minimum(x_max1, tf.transpose(a=x_max2))
+    all_pairs_max_xmin = tf.maximum(x_min1, tf.transpose(a=x_min2))
     intersect_widths = tf.maximum(0.0, all_pairs_min_xmax - all_pairs_max_xmin)
     return intersect_heights * intersect_widths
 
@@ -237,7 +237,7 @@ def matched_intersection(boxlist1, boxlist2, scope=None):
   Returns:
     a tensor with shape [N] representing pairwise intersections
   """
-  with tf.name_scope(scope, 'MatchedIntersection'):
+  with tf.compat.v1.name_scope(scope, 'MatchedIntersection'):
     y_min1, x_min1, y_max1, x_max1 = tf.split(
         value=boxlist1.get(), num_or_size_splits=4, axis=1)
     y_min2, x_min2, y_max2, x_max2 = tf.split(
@@ -262,13 +262,13 @@ def iou(boxlist1, boxlist2, scope=None):
   Returns:
     a tensor with shape [N, M] representing pairwise iou scores.
   """
-  with tf.name_scope(scope, 'IOU'):
+  with tf.compat.v1.name_scope(scope, 'IOU'):
     intersections = intersection(boxlist1, boxlist2)
     areas1 = area(boxlist1)
     areas2 = area(boxlist2)
     unions = (
         tf.expand_dims(areas1, 1) + tf.expand_dims(areas2, 0) - intersections)
-    return tf.where(
+    return tf.compat.v1.where(
         tf.equal(intersections, 0.0),
         tf.zeros_like(intersections), tf.truediv(intersections, unions))
 
@@ -284,12 +284,12 @@ def matched_iou(boxlist1, boxlist2, scope=None):
   Returns:
     a tensor with shape [N] representing pairwise iou scores.
   """
-  with tf.name_scope(scope, 'MatchedIOU'):
+  with tf.compat.v1.name_scope(scope, 'MatchedIOU'):
     intersections = matched_intersection(boxlist1, boxlist2)
     areas1 = area(boxlist1)
     areas2 = area(boxlist2)
     unions = areas1 + areas2 - intersections
-    return tf.where(
+    return tf.compat.v1.where(
         tf.equal(intersections, 0.0),
         tf.zeros_like(intersections), tf.truediv(intersections, unions))
 
@@ -309,7 +309,7 @@ def ioa(boxlist1, boxlist2, scope=None):
   Returns:
     a tensor with shape [N, M] representing pairwise ioa scores.
   """
-  with tf.name_scope(scope, 'IOA'):
+  with tf.compat.v1.name_scope(scope, 'IOA'):
     intersections = intersection(boxlist1, boxlist2)
     areas = tf.expand_dims(area(boxlist2), 0)
     return tf.truediv(intersections, areas)
@@ -334,11 +334,11 @@ def prune_non_overlapping_boxes(
     keep_inds: A tensor with shape [N'] indexing kept bounding boxes in the
       first input BoxList `boxlist1`.
   """
-  with tf.name_scope(scope, 'PruneNonOverlappingBoxes'):
+  with tf.compat.v1.name_scope(scope, 'PruneNonOverlappingBoxes'):
     ioa_ = ioa(boxlist2, boxlist1)  # [M, N] tensor
-    ioa_ = tf.reduce_max(ioa_, reduction_indices=[0])  # [N] tensor
+    ioa_ = tf.reduce_max(input_tensor=ioa_, axis=[0])  # [N] tensor
     keep_bool = tf.greater_equal(ioa_, tf.constant(min_overlap))
-    keep_inds = tf.squeeze(tf.where(keep_bool), squeeze_dims=[1])
+    keep_inds = tf.squeeze(tf.compat.v1.where(keep_bool), axis=[1])
     new_boxlist1 = gather(boxlist1, keep_inds)
     return new_boxlist1, keep_inds
 
@@ -354,11 +354,11 @@ def prune_small_boxes(boxlist, min_side, scope=None):
   Returns:
     A pruned boxlist.
   """
-  with tf.name_scope(scope, 'PruneSmallBoxes'):
+  with tf.compat.v1.name_scope(scope, 'PruneSmallBoxes'):
     height, width = height_width(boxlist)
     is_valid = tf.logical_and(tf.greater_equal(width, min_side),
                               tf.greater_equal(height, min_side))
-    return gather(boxlist, tf.reshape(tf.where(is_valid), [-1]))
+    return gather(boxlist, tf.reshape(tf.compat.v1.where(is_valid), [-1]))
 
 
 def change_coordinate_frame(boxlist, window, scope=None):
@@ -381,7 +381,7 @@ def change_coordinate_frame(boxlist, window, scope=None):
   Returns:
     Returns a BoxList object with N boxes.
   """
-  with tf.name_scope(scope, 'ChangeCoordinateFrame'):
+  with tf.compat.v1.name_scope(scope, 'ChangeCoordinateFrame'):
     win_height = window[2] - window[0]
     win_width = window[3] - window[1]
     boxlist_new = scale(box_list.BoxList(
@@ -412,12 +412,12 @@ def sq_dist(boxlist1, boxlist2, scope=None):
   Returns:
     a tensor with shape [N, M] representing pairwise distances
   """
-  with tf.name_scope(scope, 'SqDist'):
-    sqnorm1 = tf.reduce_sum(tf.square(boxlist1.get()), 1, keep_dims=True)
-    sqnorm2 = tf.reduce_sum(tf.square(boxlist2.get()), 1, keep_dims=True)
+  with tf.compat.v1.name_scope(scope, 'SqDist'):
+    sqnorm1 = tf.reduce_sum(input_tensor=tf.square(boxlist1.get()), axis=1, keepdims=True)
+    sqnorm2 = tf.reduce_sum(input_tensor=tf.square(boxlist2.get()), axis=1, keepdims=True)
     innerprod = tf.matmul(boxlist1.get(), boxlist2.get(),
                           transpose_a=False, transpose_b=True)
-    return sqnorm1 + tf.transpose(sqnorm2) - 2.0 * innerprod
+    return sqnorm1 + tf.transpose(a=sqnorm2) - 2.0 * innerprod
 
 
 def boolean_mask(boxlist, indicator, fields=None, scope=None):
@@ -443,18 +443,18 @@ def boolean_mask(boxlist, indicator, fields=None, scope=None):
   Raises:
     ValueError: if `indicator` is not a rank-1 boolean tensor.
   """
-  with tf.name_scope(scope, 'BooleanMask'):
+  with tf.compat.v1.name_scope(scope, 'BooleanMask'):
     if indicator.shape.ndims != 1:
       raise ValueError('indicator should have rank 1')
     if indicator.dtype != tf.bool:
       raise ValueError('indicator should be a boolean tensor')
-    subboxlist = box_list.BoxList(tf.boolean_mask(boxlist.get(), indicator))
+    subboxlist = box_list.BoxList(tf.boolean_mask(tensor=boxlist.get(), mask=indicator))
     if fields is None:
       fields = boxlist.get_extra_fields()
     for field in fields:
       if not boxlist.has_field(field):
         raise ValueError('boxlist must contain all specified fields')
-      subfieldlist = tf.boolean_mask(boxlist.get_field(field), indicator)
+      subfieldlist = tf.boolean_mask(tensor=boxlist.get_field(field), mask=indicator)
       subboxlist.add_field(field, subfieldlist)
     return subboxlist
 
@@ -482,7 +482,7 @@ def gather(boxlist, indices, fields=None, scope=None):
     ValueError: if specified field is not contained in boxlist or if the
       indices are not of type int32
   """
-  with tf.name_scope(scope, 'Gather'):
+  with tf.compat.v1.name_scope(scope, 'Gather'):
     if len(indices.shape.as_list()) != 1:
       raise ValueError('indices should have rank 1')
     if indices.dtype != tf.int32 and indices.dtype != tf.int64:
@@ -520,7 +520,7 @@ def concatenate(boxlists, fields=None, scope=None):
       contains non BoxList objects), or if requested fields are not contained in
       all boxlists
   """
-  with tf.name_scope(scope, 'Concatenate'):
+  with tf.compat.v1.name_scope(scope, 'Concatenate'):
     if not isinstance(boxlists, list):
       raise ValueError('boxlists should be a list')
     if not boxlists:
@@ -570,7 +570,7 @@ def sort_by_field(boxlist, field, order=SortOrder.descend, scope=None):
     ValueError: if specified field does not exist
     ValueError: if the order is not either descend or ascend
   """
-  with tf.name_scope(scope, 'SortByField'):
+  with tf.compat.v1.name_scope(scope, 'SortByField'):
     if order != SortOrder.descend and order != SortOrder.ascend:
       raise ValueError('Invalid sort order')
 
@@ -579,7 +579,7 @@ def sort_by_field(boxlist, field, order=SortOrder.descend, scope=None):
       raise ValueError('Field should have rank 1')
 
     num_boxes = boxlist.num_boxes()
-    num_entries = tf.size(field_to_sort)
+    num_entries = tf.size(input=field_to_sort)
     length_assert = tf.Assert(
         tf.equal(num_boxes, num_entries),
         ['Incorrect field size: actual vs expected.', num_entries, num_boxes])
@@ -591,7 +591,7 @@ def sort_by_field(boxlist, field, order=SortOrder.descend, scope=None):
         _, sorted_indices = tf.nn.top_k(field_to_sort, num_boxes, sorted=True)
 
     if order == SortOrder.ascend:
-      sorted_indices = tf.reverse_v2(sorted_indices, [0])
+      sorted_indices = tf.reverse(sorted_indices, [0])
 
     return gather(boxlist, sorted_indices)
 
@@ -614,9 +614,9 @@ def visualize_boxes_in_image(image, boxlist, normalized=False, scope=None):
   Returns:
     image_and_boxes: an image tensor with shape [height, width, 3]
   """
-  with tf.name_scope(scope, 'VisualizeBoxesInImage'):
+  with tf.compat.v1.name_scope(scope, 'VisualizeBoxesInImage'):
     if not normalized:
-      height, width, _ = tf.unstack(tf.shape(image))
+      height, width, _ = tf.unstack(tf.shape(input=image))
       boxlist = scale(boxlist,
                       1.0 / tf.cast(height, tf.float32),
                       1.0 / tf.cast(width, tf.float32))
@@ -641,13 +641,13 @@ def filter_field_value_equals(boxlist, field, value, scope=None):
     ValueError: if boxlist not a BoxList object or if it does not have
       the specified field.
   """
-  with tf.name_scope(scope, 'FilterFieldValueEquals'):
+  with tf.compat.v1.name_scope(scope, 'FilterFieldValueEquals'):
     if not isinstance(boxlist, box_list.BoxList):
       raise ValueError('boxlist must be a BoxList')
     if not boxlist.has_field(field):
       raise ValueError('boxlist must contain the specified field')
     filter_field = boxlist.get_field(field)
-    gather_index = tf.reshape(tf.where(tf.equal(filter_field, value)), [-1])
+    gather_index = tf.reshape(tf.compat.v1.where(tf.equal(filter_field, value)), [-1])
     return gather(boxlist, gather_index)
 
 
@@ -672,7 +672,7 @@ def filter_greater_than(boxlist, thresh, scope=None):
     ValueError: if boxlist not a BoxList object or if it does not
       have a scores field
   """
-  with tf.name_scope(scope, 'FilterGreaterThan'):
+  with tf.compat.v1.name_scope(scope, 'FilterGreaterThan'):
     if not isinstance(boxlist, box_list.BoxList):
       raise ValueError('boxlist must be a BoxList')
     if not boxlist.has_field('scores'):
@@ -684,7 +684,7 @@ def filter_greater_than(boxlist, thresh, scope=None):
       raise ValueError('Scores should have rank 1 or have shape '
                        'consistent with [None, 1]')
     high_score_indices = tf.cast(tf.reshape(
-        tf.where(tf.greater(scores, thresh)),
+        tf.compat.v1.where(tf.greater(scores, thresh)),
         [-1]), tf.int32)
     return gather(boxlist, high_score_indices)
 
@@ -709,7 +709,7 @@ def non_max_suppression(boxlist, thresh, max_output_size, scope=None):
   Raises:
     ValueError: if thresh is not in [0, 1]
   """
-  with tf.name_scope(scope, 'NonMaxSuppression'):
+  with tf.compat.v1.name_scope(scope, 'NonMaxSuppression'):
     if not 0 <= thresh <= 1.0:
       raise ValueError('thresh must be between 0 and 1')
     if not isinstance(boxlist, box_list.BoxList):
@@ -760,12 +760,12 @@ def to_normalized_coordinates(boxlist, height, width,
   Returns:
     boxlist with normalized coordinates in [0, 1].
   """
-  with tf.name_scope(scope, 'ToNormalizedCoordinates'):
+  with tf.compat.v1.name_scope(scope, 'ToNormalizedCoordinates'):
     height = tf.cast(height, tf.float32)
     width = tf.cast(width, tf.float32)
 
     if check_range:
-      max_val = tf.reduce_max(boxlist.get())
+      max_val = tf.reduce_max(input_tensor=boxlist.get())
       max_assert = tf.Assert(tf.greater(max_val, 1.01),
                              ['max value is lower than 1.01: ', max_val])
       with tf.control_dependencies([max_assert]):
@@ -799,13 +799,13 @@ def to_absolute_coordinates(boxlist,
     boxlist with absolute coordinates in terms of the image size.
 
   """
-  with tf.name_scope(scope, 'ToAbsoluteCoordinates'):
+  with tf.compat.v1.name_scope(scope, 'ToAbsoluteCoordinates'):
     height = tf.cast(height, tf.float32)
     width = tf.cast(width, tf.float32)
 
     # Ensure range of input boxes is correct.
     if check_range:
-      box_maximum = tf.reduce_max(boxlist.get())
+      box_maximum = tf.reduce_max(input_tensor=boxlist.get())
       max_assert = tf.Assert(
           tf.greater_equal(maximum_normalized_coordinate, box_maximum),
           ['maximum box coordinate value is larger '
@@ -935,19 +935,19 @@ def box_voting(selected_boxes, pool_boxes, iou_thresh=0.5):
     raise ValueError('pool_boxes must have a \'scores\' field')
 
   iou_ = iou(selected_boxes, pool_boxes)
-  match_indicator = tf.to_float(tf.greater(iou_, iou_thresh))
-  num_matches = tf.reduce_sum(match_indicator, 1)
+  match_indicator = tf.cast(tf.greater(iou_, iou_thresh), dtype=tf.float32)
+  num_matches = tf.reduce_sum(input_tensor=match_indicator, axis=1)
   # TODO(kbanoop): Handle the case where some boxes in selected_boxes do not
   # match to any boxes in pool_boxes. For such boxes without any matches, we
   # should return the original boxes without voting.
   match_assert = tf.Assert(
-      tf.reduce_all(tf.greater(num_matches, 0)),
+      tf.reduce_all(input_tensor=tf.greater(num_matches, 0)),
       ['Each box in selected_boxes must match with at least one box '
        'in pool_boxes.'])
 
   scores = tf.expand_dims(pool_boxes.get_field('scores'), 1)
   scores_assert = tf.Assert(
-      tf.reduce_all(tf.greater_equal(scores, 0)),
+      tf.reduce_all(input_tensor=tf.greater_equal(scores, 0)),
       ['Scores must be non negative.'])
 
   with tf.control_dependencies([scores_assert, match_assert]):
@@ -975,7 +975,7 @@ def pad_or_clip_box_list(boxlist, num_boxes, scope=None):
   Returns:
     BoxList with all fields padded or clipped.
   """
-  with tf.name_scope(scope, 'PadOrClipBoxList'):
+  with tf.compat.v1.name_scope(scope, 'PadOrClipBoxList'):
     subboxlist = box_list.BoxList(shape_utils.pad_or_clip_tensor(
         boxlist.get(), num_boxes))
     for field in boxlist.get_extra_fields():
@@ -1004,21 +1004,21 @@ def select_random_box(boxlist,
     valid: A bool tensor indicating whether a valid bounding box is returned
       (True) or whether the default box is returned (False).
   """
-  with tf.name_scope(scope, 'SelectRandomBox'):
+  with tf.compat.v1.name_scope(scope, 'SelectRandomBox'):
     bboxes = boxlist.get()
     combined_shape = shape_utils.combined_static_and_dynamic_shape(bboxes)
     number_of_boxes = combined_shape[0]
     default_box = default_box or tf.constant([[-1., -1., -1., -1.]])
 
     def select_box():
-      random_index = tf.random_uniform([],
+      random_index = tf.random.uniform([],
                                        maxval=number_of_boxes,
                                        dtype=tf.int32,
                                        seed=seed)
       return tf.expand_dims(bboxes[random_index], axis=0), tf.constant(True)
 
   return tf.cond(
-      tf.greater_equal(number_of_boxes, 1),
+      pred=tf.greater_equal(number_of_boxes, 1),
       true_fn=select_box,
       false_fn=lambda: (default_box, tf.constant(False)))
 
@@ -1040,22 +1040,22 @@ def get_minimal_coverage_box(boxlist,
     boxes in the box list. If the boxlist does not contain any boxes, the
     default box is returned.
   """
-  with tf.name_scope(scope, 'CreateCoverageBox'):
+  with tf.compat.v1.name_scope(scope, 'CreateCoverageBox'):
     num_boxes = boxlist.num_boxes()
 
     def coverage_box(bboxes):
       y_min, x_min, y_max, x_max = tf.split(
           value=bboxes, num_or_size_splits=4, axis=1)
-      y_min_coverage = tf.reduce_min(y_min, axis=0)
-      x_min_coverage = tf.reduce_min(x_min, axis=0)
-      y_max_coverage = tf.reduce_max(y_max, axis=0)
-      x_max_coverage = tf.reduce_max(x_max, axis=0)
+      y_min_coverage = tf.reduce_min(input_tensor=y_min, axis=0)
+      x_min_coverage = tf.reduce_min(input_tensor=x_min, axis=0)
+      y_max_coverage = tf.reduce_max(input_tensor=y_max, axis=0)
+      x_max_coverage = tf.reduce_max(input_tensor=x_max, axis=0)
       return tf.stack(
           [y_min_coverage, x_min_coverage, y_max_coverage, x_max_coverage],
           axis=1)
 
     default_box = default_box or tf.constant([[0., 0., 1., 1.]])
     return tf.cond(
-        tf.greater_equal(num_boxes, 1),
+        pred=tf.greater_equal(num_boxes, 1),
         true_fn=lambda: coverage_box(boxlist.get()),
         false_fn=lambda: default_box)
diff --git a/research/object_detection/core/box_list_ops_test.py b/research/object_detection/core/box_list_ops_test.py
index bb76cfd3..59001ae5 100644
--- a/research/object_detection/core/box_list_ops_test.py
+++ b/research/object_detection/core/box_list_ops_test.py
@@ -434,7 +434,7 @@ class BoxListOpsTest(tf.test.TestCase):
     corners = tf.constant([4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]
                           ])
     weights = tf.constant([.5, .3, .7, .1, .9], tf.float32)
-    indices = tf.reshape(tf.where(tf.greater(weights, 0.4)), [-1])
+    indices = tf.reshape(tf.compat.v1.where(tf.greater(weights, 0.4)), [-1])
     expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]
     expected_weights = [.5, .7, .9]
 
@@ -524,8 +524,8 @@ class BoxListOpsTest(tf.test.TestCase):
                            [0, 0, 3, 2]], tf.float32)
     boxes = box_list.BoxList(corners)
     image_and_boxes = box_list_ops.visualize_boxes_in_image(image, boxes)
-    image_and_boxes_bw = tf.to_float(
-        tf.greater(tf.reduce_sum(image_and_boxes, 2), 0.0))
+    image_and_boxes_bw = tf.cast(
+        tf.greater(tf.reduce_sum(input_tensor=image_and_boxes, axis=2), 0.0), dtype=tf.float32)
     exp_result = [[1, 1, 1, 0],
                   [1, 1, 1, 0],
                   [1, 1, 1, 0],
@@ -846,7 +846,7 @@ class CoordinatesConversionTest(tf.test.TestCase):
     img = tf.ones((128, 100, 100, 3))
     boxlist = box_list.BoxList(coordinates)
     normalized_boxlist = box_list_ops.to_normalized_coordinates(
-        boxlist, tf.shape(img)[1], tf.shape(img)[2])
+        boxlist, tf.shape(input=img)[1], tf.shape(input=img)[2])
     expected_boxes = [[0, 0, 1, 1],
                       [0.25, 0.25, 0.75, 0.75]]
 
@@ -860,7 +860,7 @@ class CoordinatesConversionTest(tf.test.TestCase):
     img = tf.ones((128, 100, 100, 3))
     boxlist = box_list.BoxList(coordinates)
     normalized_boxlist = box_list_ops.to_normalized_coordinates(
-        boxlist, tf.shape(img)[1], tf.shape(img)[2])
+        boxlist, tf.shape(input=img)[1], tf.shape(input=img)[2])
 
     with self.test_session() as sess:
       with self.assertRaisesOpError('assertion failed'):
@@ -872,8 +872,8 @@ class CoordinatesConversionTest(tf.test.TestCase):
     img = tf.ones((128, 100, 100, 3))
     boxlist = box_list.BoxList(coordinates)
     absolute_boxlist = box_list_ops.to_absolute_coordinates(boxlist,
-                                                            tf.shape(img)[1],
-                                                            tf.shape(img)[2])
+                                                            tf.shape(input=img)[1],
+                                                            tf.shape(input=img)[2])
     expected_boxes = [[0, 0, 100, 100],
                       [25, 25, 75, 75]]
 
@@ -887,8 +887,8 @@ class CoordinatesConversionTest(tf.test.TestCase):
     img = tf.ones((128, 100, 100, 3))
     boxlist = box_list.BoxList(coordinates)
     absolute_boxlist = box_list_ops.to_absolute_coordinates(boxlist,
-                                                            tf.shape(img)[1],
-                                                            tf.shape(img)[2])
+                                                            tf.shape(input=img)[1],
+                                                            tf.shape(input=img)[2])
 
     with self.test_session() as sess:
       with self.assertRaisesOpError('assertion failed'):
@@ -903,11 +903,11 @@ class CoordinatesConversionTest(tf.test.TestCase):
 
     boxlist = box_list.BoxList(tf.constant(coordinates, tf.float32))
     boxlist = box_list_ops.to_normalized_coordinates(boxlist,
-                                                     tf.shape(img)[1],
-                                                     tf.shape(img)[2])
+                                                     tf.shape(input=img)[1],
+                                                     tf.shape(input=img)[2])
     boxlist = box_list_ops.to_absolute_coordinates(boxlist,
-                                                   tf.shape(img)[1],
-                                                   tf.shape(img)[2])
+                                                   tf.shape(input=img)[1],
+                                                   tf.shape(input=img)[2])
 
     with self.test_session() as sess:
       out = sess.run(boxlist.get())
@@ -921,11 +921,11 @@ class CoordinatesConversionTest(tf.test.TestCase):
 
     boxlist = box_list.BoxList(tf.constant(coordinates, tf.float32))
     boxlist = box_list_ops.to_absolute_coordinates(boxlist,
-                                                   tf.shape(img)[1],
-                                                   tf.shape(img)[2])
+                                                   tf.shape(input=img)[1],
+                                                   tf.shape(input=img)[2])
     boxlist = box_list_ops.to_normalized_coordinates(boxlist,
-                                                     tf.shape(img)[1],
-                                                     tf.shape(img)[2])
+                                                     tf.shape(input=img)[1],
+                                                     tf.shape(input=img)[2])
 
     with self.test_session() as sess:
       out = sess.run(boxlist.get())
@@ -938,8 +938,8 @@ class CoordinatesConversionTest(tf.test.TestCase):
     boxlist = box_list.BoxList(coordinates)
     absolute_boxlist = box_list_ops.to_absolute_coordinates(
         boxlist,
-        tf.shape(img)[1],
-        tf.shape(img)[2],
+        tf.shape(input=img)[1],
+        tf.shape(input=img)[2],
         maximum_normalized_coordinate=1.1)
 
     with self.test_session() as sess:
diff --git a/research/object_detection/core/box_list_test.py b/research/object_detection/core/box_list_test.py
index edc00ebb..b9d0b742 100644
--- a/research/object_detection/core/box_list_test.py
+++ b/research/object_detection/core/box_list_test.py
@@ -43,7 +43,7 @@ class BoxListTest(tf.test.TestCase):
 
   def test_create_box_list_with_dynamic_shape(self):
     data = tf.constant([[0, 0, 1, 1], [1, 1, 2, 3], [3, 4, 5, 5]], tf.float32)
-    indices = tf.reshape(tf.where(tf.greater([1, 0, 1], 0)), [-1])
+    indices = tf.reshape(tf.compat.v1.where(tf.greater([1, 0, 1], 0)), [-1])
     data = tf.gather(data, indices)
     assert data.get_shape().as_list() == [None, 4]
     expected_num_boxes = 2
@@ -81,7 +81,7 @@ class BoxListTest(tf.test.TestCase):
     self.assertEquals(type(boxes.num_boxes_static()), int)
 
   def test_num_boxes_static_for_uninferrable_shape(self):
-    placeholder = tf.placeholder(tf.float32, shape=[None, 4])
+    placeholder = tf.compat.v1.placeholder(tf.float32, shape=[None, 4])
     boxes = box_list.BoxList(placeholder)
     self.assertEquals(boxes.num_boxes_static(), None)
 
diff --git a/research/object_detection/core/box_predictor.py b/research/object_detection/core/box_predictor.py
index b98bb1f1..1f3da98b 100644
--- a/research/object_detection/core/box_predictor.py
+++ b/research/object_detection/core/box_predictor.py
@@ -98,7 +98,7 @@ class BoxPredictor(object):
                        format(len(image_features),
                               len(num_predictions_per_location)))
     if scope is not None:
-      with tf.variable_scope(scope):
+      with tf.compat.v1.variable_scope(scope):
         return self._predict(image_features, num_predictions_per_location,
                              **params)
     return self._predict(image_features, num_predictions_per_location,
diff --git a/research/object_detection/core/freezable_batch_norm_test.py b/research/object_detection/core/freezable_batch_norm_test.py
index 504b9e71..bf8fa59d 100644
--- a/research/object_detection/core/freezable_batch_norm_test.py
+++ b/research/object_detection/core/freezable_batch_norm_test.py
@@ -69,7 +69,7 @@ class FreezableBatchNormTest(tf.test.TestCase):
           scale=testing_var,
           size=(1000, 10))
 
-      out_tensor = norm(tf.convert_to_tensor(test_data, dtype=tf.float32))
+      out_tensor = norm(tf.convert_to_tensor(value=test_data, dtype=tf.float32))
       out = tf.keras.backend.eval(out_tensor)
 
       out -= tf.keras.backend.eval(norm.beta)
@@ -104,7 +104,7 @@ class FreezableBatchNormTest(tf.test.TestCase):
           scale=testing_var,
           size=(1000, 10))
 
-      out_tensor = norm(tf.convert_to_tensor(test_data, dtype=tf.float32))
+      out_tensor = norm(tf.convert_to_tensor(value=test_data, dtype=tf.float32))
       out = tf.keras.backend.eval(out_tensor)
 
       out -= tf.keras.backend.eval(norm.beta)
diff --git a/research/object_detection/core/keypoint_ops.py b/research/object_detection/core/keypoint_ops.py
index e520845f..204e360b 100644
--- a/research/object_detection/core/keypoint_ops.py
+++ b/research/object_detection/core/keypoint_ops.py
@@ -35,7 +35,7 @@ def scale(keypoints, y_scale, x_scale, scope=None):
   Returns:
     new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]
   """
-  with tf.name_scope(scope, 'Scale'):
+  with tf.compat.v1.name_scope(scope, 'Scale'):
     y_scale = tf.cast(y_scale, tf.float32)
     x_scale = tf.cast(x_scale, tf.float32)
     new_keypoints = keypoints * [[[y_scale, x_scale]]]
@@ -56,7 +56,7 @@ def clip_to_window(keypoints, window, scope=None):
   Returns:
     new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]
   """
-  with tf.name_scope(scope, 'ClipToWindow'):
+  with tf.compat.v1.name_scope(scope, 'ClipToWindow'):
     y, x = tf.split(value=keypoints, num_or_size_splits=2, axis=2)
     win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)
     y = tf.maximum(tf.minimum(y, win_y_max), win_y_min)
@@ -81,7 +81,7 @@ def prune_outside_window(keypoints, window, scope=None):
   Returns:
     new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]
   """
-  with tf.name_scope(scope, 'PruneOutsideWindow'):
+  with tf.compat.v1.name_scope(scope, 'PruneOutsideWindow'):
     y, x = tf.split(value=keypoints, num_or_size_splits=2, axis=2)
     win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)
 
@@ -89,8 +89,8 @@ def prune_outside_window(keypoints, window, scope=None):
         tf.logical_and(y >= win_y_min, y <= win_y_max),
         tf.logical_and(x >= win_x_min, x <= win_x_max))
 
-    new_y = tf.where(valid_indices, y, np.nan * tf.ones_like(y))
-    new_x = tf.where(valid_indices, x, np.nan * tf.ones_like(x))
+    new_y = tf.compat.v1.where(valid_indices, y, np.nan * tf.ones_like(y))
+    new_x = tf.compat.v1.where(valid_indices, x, np.nan * tf.ones_like(x))
     new_keypoints = tf.concat([new_y, new_x], 2)
 
     return new_keypoints
@@ -117,7 +117,7 @@ def change_coordinate_frame(keypoints, window, scope=None):
   Returns:
     new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]
   """
-  with tf.name_scope(scope, 'ChangeCoordinateFrame'):
+  with tf.compat.v1.name_scope(scope, 'ChangeCoordinateFrame'):
     win_height = window[2] - window[0]
     win_width = window[3] - window[1]
     new_keypoints = scale(keypoints - [window[0], window[1]], 1.0 / win_height,
@@ -149,12 +149,12 @@ def to_normalized_coordinates(keypoints, height, width,
     tensor of shape [num_instances, num_keypoints, 2] with normalized
     coordinates in [0, 1].
   """
-  with tf.name_scope(scope, 'ToNormalizedCoordinates'):
+  with tf.compat.v1.name_scope(scope, 'ToNormalizedCoordinates'):
     height = tf.cast(height, tf.float32)
     width = tf.cast(width, tf.float32)
 
     if check_range:
-      max_val = tf.reduce_max(keypoints)
+      max_val = tf.reduce_max(input_tensor=keypoints)
       max_assert = tf.Assert(tf.greater(max_val, 1.01),
                              ['max value is lower than 1.01: ', max_val])
       with tf.control_dependencies([max_assert]):
@@ -183,13 +183,13 @@ def to_absolute_coordinates(keypoints, height, width,
     in terms of the image size.
 
   """
-  with tf.name_scope(scope, 'ToAbsoluteCoordinates'):
+  with tf.compat.v1.name_scope(scope, 'ToAbsoluteCoordinates'):
     height = tf.cast(height, tf.float32)
     width = tf.cast(width, tf.float32)
 
     # Ensure range of input keypoints is correct.
     if check_range:
-      max_val = tf.reduce_max(keypoints)
+      max_val = tf.reduce_max(input_tensor=keypoints)
       max_assert = tf.Assert(tf.greater_equal(1.01, max_val),
                              ['maximum keypoint coordinate value is larger '
                               'than 1.01: ', max_val])
@@ -221,13 +221,13 @@ def flip_horizontal(keypoints, flip_point, flip_permutation, scope=None):
   Returns:
     new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]
   """
-  with tf.name_scope(scope, 'FlipHorizontal'):
-    keypoints = tf.transpose(keypoints, [1, 0, 2])
+  with tf.compat.v1.name_scope(scope, 'FlipHorizontal'):
+    keypoints = tf.transpose(a=keypoints, perm=[1, 0, 2])
     keypoints = tf.gather(keypoints, flip_permutation)
     v, u = tf.split(value=keypoints, num_or_size_splits=2, axis=2)
     u = flip_point * 2.0 - u
     new_keypoints = tf.concat([v, u], 2)
-    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])
+    new_keypoints = tf.transpose(a=new_keypoints, perm=[1, 0, 2])
     return new_keypoints
 
 
@@ -253,13 +253,13 @@ def flip_vertical(keypoints, flip_point, flip_permutation, scope=None):
   Returns:
     new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]
   """
-  with tf.name_scope(scope, 'FlipVertical'):
-    keypoints = tf.transpose(keypoints, [1, 0, 2])
+  with tf.compat.v1.name_scope(scope, 'FlipVertical'):
+    keypoints = tf.transpose(a=keypoints, perm=[1, 0, 2])
     keypoints = tf.gather(keypoints, flip_permutation)
     v, u = tf.split(value=keypoints, num_or_size_splits=2, axis=2)
     v = flip_point * 2.0 - v
     new_keypoints = tf.concat([v, u], 2)
-    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])
+    new_keypoints = tf.transpose(a=new_keypoints, perm=[1, 0, 2])
     return new_keypoints
 
 
@@ -273,10 +273,10 @@ def rot90(keypoints, scope=None):
   Returns:
     new_keypoints: a tensor of shape [num_instances, num_keypoints, 2]
   """
-  with tf.name_scope(scope, 'Rot90'):
-    keypoints = tf.transpose(keypoints, [1, 0, 2])
+  with tf.compat.v1.name_scope(scope, 'Rot90'):
+    keypoints = tf.transpose(a=keypoints, perm=[1, 0, 2])
     v, u = tf.split(value=keypoints[:, :, ::-1], num_or_size_splits=2, axis=2)
     v = 1.0 - v
     new_keypoints = tf.concat([v, u], 2)
-    new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])
+    new_keypoints = tf.transpose(a=new_keypoints, perm=[1, 0, 2])
     return new_keypoints
diff --git a/research/object_detection/core/losses.py b/research/object_detection/core/losses.py
index 2c274b6d..138ea8b3 100644
--- a/research/object_detection/core/losses.py
+++ b/research/object_detection/core/losses.py
@@ -65,10 +65,10 @@ class Loss(object):
     Returns:
       loss: a tensor representing the value of the loss function.
     """
-    with tf.name_scope(scope, 'Loss',
+    with tf.compat.v1.name_scope(scope, 'Loss',
                        [prediction_tensor, target_tensor, params]) as scope:
       if ignore_nan_targets:
-        target_tensor = tf.where(tf.is_nan(target_tensor),
+        target_tensor = tf.compat.v1.where(tf.math.is_nan(target_tensor),
                                  prediction_tensor,
                                  target_tensor)
       return self._compute_loss(prediction_tensor, target_tensor, **params)
@@ -113,7 +113,7 @@ class WeightedL2LocalizationLoss(Loss):
     weighted_diff = (prediction_tensor - target_tensor) * tf.expand_dims(
         weights, 2)
     square_diff = 0.5 * tf.square(weighted_diff)
-    return tf.reduce_sum(square_diff, 2)
+    return tf.reduce_sum(input_tensor=square_diff, axis=2)
 
 
 class WeightedSmoothL1LocalizationLoss(Loss):
@@ -148,13 +148,13 @@ class WeightedSmoothL1LocalizationLoss(Loss):
       loss: a float tensor of shape [batch_size, num_anchors] tensor
         representing the value of the loss function.
     """
-    return tf.reduce_sum(tf.losses.huber_loss(
+    return tf.reduce_sum(input_tensor=tf.compat.v1.losses.huber_loss(
         target_tensor,
         prediction_tensor,
         delta=self._delta,
         weights=tf.expand_dims(weights, axis=2),
         loss_collection=None,
-        reduction=tf.losses.Reduction.NONE
+        reduction=tf.compat.v1.losses.Reduction.NONE
     ), axis=2)
 
 
@@ -214,7 +214,7 @@ class WeightedSigmoidClassificationLoss(Loss):
     if class_indices is not None:
       weights *= tf.reshape(
           ops.indices_to_dense_vector(class_indices,
-                                      tf.shape(prediction_tensor)[2]),
+                                      tf.shape(input=prediction_tensor)[2]),
           [1, 1, -1])
     per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(
         labels=target_tensor, logits=prediction_tensor))
@@ -262,7 +262,7 @@ class SigmoidFocalClassificationLoss(Loss):
     if class_indices is not None:
       weights *= tf.reshape(
           ops.indices_to_dense_vector(class_indices,
-                                      tf.shape(prediction_tensor)[2]),
+                                      tf.shape(input=prediction_tensor)[2]),
           [1, 1, -1])
     per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(
         labels=target_tensor, logits=prediction_tensor))
@@ -313,9 +313,9 @@ class WeightedSoftmaxClassificationLoss(Loss):
     prediction_tensor = tf.divide(
         prediction_tensor, self._logit_scale, name='scale_logit')
     per_row_cross_ent = (tf.nn.softmax_cross_entropy_with_logits(
-        labels=tf.reshape(target_tensor, [-1, num_classes]),
+        labels=tf.stop_gradient(tf.reshape(target_tensor, [-1, num_classes])),
         logits=tf.reshape(prediction_tensor, [-1, num_classes])))
-    return tf.reshape(per_row_cross_ent, tf.shape(weights)) * weights
+    return tf.reshape(per_row_cross_ent, tf.shape(input=weights)) * weights
 
 
 class WeightedSoftmaxClassificationAgainstLogitsLoss(Loss):
@@ -361,9 +361,9 @@ class WeightedSoftmaxClassificationAgainstLogitsLoss(Loss):
                                   name='scale_logits')
 
     per_row_cross_ent = (tf.nn.softmax_cross_entropy_with_logits(
-        labels=tf.reshape(target_tensor, [-1, num_classes]),
+        labels=tf.stop_gradient(tf.reshape(target_tensor, [-1, num_classes])),
         logits=tf.reshape(prediction_tensor, [-1, num_classes])))
-    return tf.reshape(per_row_cross_ent, tf.shape(weights)) * weights
+    return tf.reshape(per_row_cross_ent, tf.shape(input=weights)) * weights
 
 
 class BootstrappedSigmoidClassificationLoss(Loss):
@@ -570,11 +570,11 @@ class HardExampleMiner(object):
         num_positives_list.append(num_positives)
         num_negatives_list.append(num_negatives)
       mined_location_losses.append(
-          tf.reduce_sum(tf.gather(location_losses[ind], selected_indices)))
+          tf.reduce_sum(input_tensor=tf.gather(location_losses[ind], selected_indices)))
       mined_cls_losses.append(
-          tf.reduce_sum(tf.gather(cls_losses[ind], selected_indices)))
-    location_loss = tf.reduce_sum(tf.stack(mined_location_losses))
-    cls_loss = tf.reduce_sum(tf.stack(mined_cls_losses))
+          tf.reduce_sum(input_tensor=tf.gather(cls_losses[ind], selected_indices)))
+    location_loss = tf.reduce_sum(input_tensor=tf.stack(mined_location_losses))
+    cls_loss = tf.reduce_sum(input_tensor=tf.stack(mined_cls_losses))
     if match and self._max_negatives_per_positive:
       self._num_positives_list = num_positives_list
       self._num_negatives_list = num_negatives_list
@@ -583,10 +583,10 @@ class HardExampleMiner(object):
   def summarize(self):
     """Summarize the number of positives and negatives after mining."""
     if self._num_positives_list and self._num_negatives_list:
-      avg_num_positives = tf.reduce_mean(tf.to_float(self._num_positives_list))
-      avg_num_negatives = tf.reduce_mean(tf.to_float(self._num_negatives_list))
-      tf.summary.scalar('HardExampleMiner/NumPositives', avg_num_positives)
-      tf.summary.scalar('HardExampleMiner/NumNegatives', avg_num_negatives)
+      avg_num_positives = tf.reduce_mean(input_tensor=tf.cast(self._num_positives_list, dtype=tf.float32))
+      avg_num_negatives = tf.reduce_mean(input_tensor=tf.cast(self._num_negatives_list, dtype=tf.float32))
+      tf.compat.v1.summary.scalar('HardExampleMiner/NumPositives', avg_num_positives)
+      tf.compat.v1.summary.scalar('HardExampleMiner/NumNegatives', avg_num_negatives)
 
   def _subsample_selection_to_desired_neg_pos_ratio(self,
                                                     indices,
@@ -628,14 +628,14 @@ class HardExampleMiner(object):
     """
     positives_indicator = tf.gather(match.matched_column_indicator(), indices)
     negatives_indicator = tf.gather(match.unmatched_column_indicator(), indices)
-    num_positives = tf.reduce_sum(tf.to_int32(positives_indicator))
+    num_positives = tf.reduce_sum(input_tensor=tf.cast(positives_indicator, dtype=tf.int32))
     max_negatives = tf.maximum(min_negatives_per_image,
-                               tf.to_int32(max_negatives_per_positive *
-                                           tf.to_float(num_positives)))
+                               tf.cast(max_negatives_per_positive *
+                                           tf.cast(num_positives, dtype=tf.float32), dtype=tf.int32))
     topk_negatives_indicator = tf.less_equal(
-        tf.cumsum(tf.to_int32(negatives_indicator)), max_negatives)
-    subsampled_selection_indices = tf.where(
+        tf.cumsum(tf.cast(negatives_indicator, dtype=tf.int32)), max_negatives)
+    subsampled_selection_indices = tf.compat.v1.where(
         tf.logical_or(positives_indicator, topk_negatives_indicator))
-    num_negatives = tf.size(subsampled_selection_indices) - num_positives
+    num_negatives = tf.size(input=subsampled_selection_indices) - num_positives
     return (tf.reshape(tf.gather(indices, subsampled_selection_indices), [-1]),
             num_positives, num_negatives)
diff --git a/research/object_detection/core/losses_test.py b/research/object_detection/core/losses_test.py
index 173d6e98..86920dd2 100644
--- a/research/object_detection/core/losses_test.py
+++ b/research/object_detection/core/losses_test.py
@@ -36,7 +36,7 @@ class WeightedL2LocalizationLossTest(tf.test.TestCase):
                            [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
                            [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], tf.float32)
     loss_op = losses.WeightedL2LocalizationLoss()
-    loss = tf.reduce_sum(loss_op(prediction_tensor, target_tensor,
+    loss = tf.reduce_sum(input_tensor=loss_op(prediction_tensor, target_tensor,
                                  weights=weights))
 
     expected_loss = (3 * 5 * 4) / 2.0
@@ -72,7 +72,7 @@ class WeightedL2LocalizationLossTest(tf.test.TestCase):
     loss_op = losses.WeightedL2LocalizationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights,
                    ignore_nan_targets=True)
-    loss = tf.reduce_sum(loss)
+    loss = tf.reduce_sum(input_tensor=loss)
 
     expected_loss = (3 * 5 * 4) / 2.0
     with self.test_session() as sess:
@@ -97,7 +97,7 @@ class WeightedSmoothL1LocalizationLossTest(tf.test.TestCase):
                            [0, 3, 0]], tf.float32)
     loss_op = losses.WeightedSmoothL1LocalizationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    loss = tf.reduce_sum(input_tensor=loss)
 
     exp_loss = 7.695
     with self.test_session() as sess:
@@ -117,7 +117,7 @@ class WeightedIOULocalizationLossTest(tf.test.TestCase):
     weights = [[1.0, .5, 2.0]]
     loss_op = losses.WeightedIOULocalizationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    loss = tf.reduce_sum(input_tensor=loss)
     exp_loss = 2.0
     with self.test_session() as sess:
       loss_output = sess.run(loss)
@@ -147,7 +147,7 @@ class WeightedSigmoidClassificationLossTest(tf.test.TestCase):
                            [1, 1, 1, 0]], tf.float32)
     loss_op = losses.WeightedSigmoidClassificationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    loss = tf.reduce_sum(input_tensor=loss)
 
     exp_loss = -2 * math.log(.5)
     with self.test_session() as sess:
@@ -175,7 +175,7 @@ class WeightedSigmoidClassificationLossTest(tf.test.TestCase):
                            [1, 1, 1, 0]], tf.float32)
     loss_op = losses.WeightedSigmoidClassificationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss, axis=2)
+    loss = tf.reduce_sum(input_tensor=loss, axis=2)
 
     exp_loss = np.matrix([[0, 0, -math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
@@ -207,7 +207,7 @@ class WeightedSigmoidClassificationLossTest(tf.test.TestCase):
     loss_op = losses.WeightedSigmoidClassificationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights,
                    class_indices=class_indices)
-    loss = tf.reduce_sum(loss, axis=2)
+    loss = tf.reduce_sum(input_tensor=loss, axis=2)
 
     exp_loss = np.matrix([[0, 0, -math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
@@ -238,9 +238,9 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
     weights = tf.constant([[1, 1, 1, 1, 1, 1]], tf.float32)
     focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
     sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+    focal_loss = tf.reduce_sum(input_tensor=focal_loss_op(prediction_tensor, target_tensor,
                                              weights=weights), axis=2)
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+    sigmoid_loss = tf.reduce_sum(input_tensor=sigmoid_loss_op(prediction_tensor,
                                                  target_tensor,
                                                  weights=weights), axis=2)
 
@@ -264,9 +264,9 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
     weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)
     focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
     sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+    focal_loss = tf.reduce_sum(input_tensor=focal_loss_op(prediction_tensor, target_tensor,
                                              weights=weights), axis=2)
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+    sigmoid_loss = tf.reduce_sum(input_tensor=sigmoid_loss_op(prediction_tensor,
                                                  target_tensor,
                                                  weights=weights), axis=2)
 
@@ -290,9 +290,9 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
     weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)
     focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
     sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+    focal_loss = tf.reduce_sum(input_tensor=focal_loss_op(prediction_tensor, target_tensor,
                                              weights=weights))
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+    sigmoid_loss = tf.reduce_sum(input_tensor=sigmoid_loss_op(prediction_tensor,
                                                  target_tensor,
                                                  weights=weights))
 
@@ -316,9 +316,9 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
     weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)
     focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=1.0)
     sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+    focal_loss = tf.reduce_sum(input_tensor=focal_loss_op(prediction_tensor, target_tensor,
                                              weights=weights), axis=2)
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+    sigmoid_loss = tf.reduce_sum(input_tensor=sigmoid_loss_op(prediction_tensor,
                                                  target_tensor,
                                                  weights=weights), axis=2)
 
@@ -344,9 +344,9 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
     weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)
     focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=0.0)
     sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+    focal_loss = tf.reduce_sum(input_tensor=focal_loss_op(prediction_tensor, target_tensor,
                                              weights=weights), axis=2)
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+    sigmoid_loss = tf.reduce_sum(input_tensor=sigmoid_loss_op(prediction_tensor,
                                                  target_tensor,
                                                  weights=weights), axis=2)
 
@@ -440,7 +440,7 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                            [1, 1, 1, 1]], tf.float32)
     focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=1.0, gamma=0.0)
 
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+    focal_loss = tf.reduce_sum(input_tensor=focal_loss_op(prediction_tensor, target_tensor,
                                              weights=weights))
     with self.test_session() as sess:
       focal_loss = sess.run(focal_loss)
@@ -472,7 +472,7 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                            [1, 1, 1, 1]], tf.float32)
     focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.75, gamma=0.0)
 
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+    focal_loss = tf.reduce_sum(input_tensor=focal_loss_op(prediction_tensor, target_tensor,
                                              weights=weights))
     with self.test_session() as sess:
       focal_loss = sess.run(focal_loss)
@@ -508,7 +508,7 @@ class WeightedSoftmaxClassificationLossTest(tf.test.TestCase):
                            [1, 1, 1, 0]], tf.float32)
     loss_op = losses.WeightedSoftmaxClassificationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    loss = tf.reduce_sum(input_tensor=loss)
 
     exp_loss = - 1.5 * math.log(.5)
     with self.test_session() as sess:
@@ -600,7 +600,7 @@ class WeightedSoftmaxClassificationAgainstLogitsLossTest(tf.test.TestCase):
                            [1, 1, 1, 1]], tf.float32)
     loss_op = losses.WeightedSoftmaxClassificationAgainstLogitsLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    loss = tf.reduce_sum(input_tensor=loss)
 
     exp_loss = - 1.5 * math.log(.5)
     with self.test_session() as sess:
@@ -706,7 +706,7 @@ class BootstrappedSigmoidClassificationLossTest(tf.test.TestCase):
     loss_op = losses.BootstrappedSigmoidClassificationLoss(
         alpha, bootstrap_type='soft')
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    loss = tf.reduce_sum(input_tensor=loss)
     exp_loss = -math.log(.5)
     with self.test_session() as sess:
       loss_output = sess.run(loss)
@@ -735,7 +735,7 @@ class BootstrappedSigmoidClassificationLossTest(tf.test.TestCase):
     loss_op = losses.BootstrappedSigmoidClassificationLoss(
         alpha, bootstrap_type='hard')
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    loss = tf.reduce_sum(input_tensor=loss)
     exp_loss = -math.log(.5)
     with self.test_session() as sess:
       loss_output = sess.run(loss)
@@ -764,7 +764,7 @@ class BootstrappedSigmoidClassificationLossTest(tf.test.TestCase):
     loss_op = losses.BootstrappedSigmoidClassificationLoss(
         alpha, bootstrap_type='hard')
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss, axis=2)
+    loss = tf.reduce_sum(input_tensor=loss, axis=2)
     exp_loss = np.matrix([[0, 0, -math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
     with self.test_session() as sess:
diff --git a/research/object_detection/core/matcher.py b/research/object_detection/core/matcher.py
index 4c0a9c81..db7a2cd2 100644
--- a/research/object_detection/core/matcher.py
+++ b/research/object_detection/core/matcher.py
@@ -88,7 +88,7 @@ class Match(object):
     Returns:
       column_indices: int32 tensor of shape [K] with column indices.
     """
-    return self._reshape_and_cast(tf.where(tf.greater(self._match_results, -1)))
+    return self._reshape_and_cast(tf.compat.v1.where(tf.greater(self._match_results, -1)))
 
   def matched_column_indicator(self):
     """Returns column indices that are matched.
@@ -100,7 +100,7 @@ class Match(object):
 
   def num_matched_columns(self):
     """Returns number (int32 scalar tensor) of matched columns."""
-    return tf.size(self.matched_column_indices())
+    return tf.size(input=self.matched_column_indices())
 
   def unmatched_column_indices(self):
     """Returns column indices that do not match any row.
@@ -110,7 +110,7 @@ class Match(object):
     Returns:
       column_indices: int32 tensor of shape [K] with column indices.
     """
-    return self._reshape_and_cast(tf.where(tf.equal(self._match_results, -1)))
+    return self._reshape_and_cast(tf.compat.v1.where(tf.equal(self._match_results, -1)))
 
   def unmatched_column_indicator(self):
     """Returns column indices that are unmatched.
@@ -122,7 +122,7 @@ class Match(object):
 
   def num_unmatched_columns(self):
     """Returns number (int32 scalar tensor) of unmatched columns."""
-    return tf.size(self.unmatched_column_indices())
+    return tf.size(input=self.unmatched_column_indices())
 
   def ignored_column_indices(self):
     """Returns column indices that are ignored (neither Matched nor Unmatched).
@@ -132,7 +132,7 @@ class Match(object):
     Returns:
       column_indices: int32 tensor of shape [K] with column indices.
     """
-    return self._reshape_and_cast(tf.where(self.ignored_column_indicator()))
+    return self._reshape_and_cast(tf.compat.v1.where(self.ignored_column_indicator()))
 
   def ignored_column_indicator(self):
     """Returns boolean column indicator where True means the colum is ignored.
@@ -145,7 +145,7 @@ class Match(object):
 
   def num_ignored_columns(self):
     """Returns number (int32 scalar tensor) of matched columns."""
-    return tf.size(self.ignored_column_indices())
+    return tf.size(input=self.ignored_column_indices())
 
   def unmatched_or_ignored_column_indices(self):
     """Returns column indices that are unmatched or ignored.
@@ -155,7 +155,7 @@ class Match(object):
     Returns:
       column_indices: int32 tensor of shape [K] with column indices.
     """
-    return self._reshape_and_cast(tf.where(tf.greater(0, self._match_results)))
+    return self._reshape_and_cast(tf.compat.v1.where(tf.greater(0, self._match_results)))
 
   def matched_row_indices(self):
     """Returns row indices that match some column.
@@ -235,7 +235,7 @@ class Matcher(object):
     Returns:
       A Match object with the results of matching.
     """
-    with tf.name_scope(scope, 'Match', [similarity_matrix, params]) as scope:
+    with tf.compat.v1.name_scope(scope, 'Match', [similarity_matrix, params]) as scope:
       return Match(self._match(similarity_matrix, **params),
                    self._use_matmul_gather)
 
diff --git a/research/object_detection/core/matcher_test.py b/research/object_detection/core/matcher_test.py
index 05607834..e30b90bd 100644
--- a/research/object_detection/core/matcher_test.py
+++ b/research/object_detection/core/matcher_test.py
@@ -129,7 +129,7 @@ class MatchTest(tf.test.TestCase):
     # Note: deliberately setting to small number so not always
     # all possibilities appear (matched, unmatched, ignored)
     num_matches = 10
-    match_results = tf.random_uniform(
+    match_results = tf.random.uniform(
         [num_matches], minval=-2, maxval=5, dtype=tf.int32)
     match = matcher.Match(match_results)
     matched_column_indices = match.matched_column_indices()
diff --git a/research/object_detection/core/minibatch_sampler.py b/research/object_detection/core/minibatch_sampler.py
index dc622221..c5c14a27 100644
--- a/research/object_detection/core/minibatch_sampler.py
+++ b/research/object_detection/core/minibatch_sampler.py
@@ -77,14 +77,14 @@ class MinibatchSampler(object):
     Returns:
       a boolean tensor with the same shape as input (indicator) tensor
     """
-    indices = tf.where(indicator)
-    indices = tf.random_shuffle(indices)
+    indices = tf.compat.v1.where(indicator)
+    indices = tf.random.shuffle(indices)
     indices = tf.reshape(indices, [-1])
 
-    num_samples = tf.minimum(tf.size(indices), num_samples)
+    num_samples = tf.minimum(tf.size(input=indices), num_samples)
     selected_indices = tf.slice(indices, [0], tf.reshape(num_samples, [1]))
 
     selected_indicator = ops.indices_to_dense_vector(selected_indices,
-                                                     tf.shape(indicator)[0])
+                                                     tf.shape(input=indicator)[0])
 
     return tf.equal(selected_indicator, 1)
diff --git a/research/object_detection/core/minibatch_sampler_test.py b/research/object_detection/core/minibatch_sampler_test.py
index 7420ae5d..bffd45f6 100644
--- a/research/object_detection/core/minibatch_sampler_test.py
+++ b/research/object_detection/core/minibatch_sampler_test.py
@@ -36,7 +36,7 @@ class MinibatchSamplerTest(tf.test.TestCase):
 
   def test_subsample_when_more_true_elements_than_num_samples_no_shape(self):
     np_indicator = [True, False, True, False, True, True, False]
-    indicator = tf.placeholder(tf.bool)
+    indicator = tf.compat.v1.placeholder(tf.bool)
     feed_dict = {indicator: np_indicator}
 
     samples = minibatch_sampler.MinibatchSampler.subsample_indicator(
diff --git a/research/object_detection/core/post_processing.py b/research/object_detection/core/post_processing.py
index 09a31689..2acb60c3 100644
--- a/research/object_detection/core/post_processing.py
+++ b/research/object_detection/core/post_processing.py
@@ -108,8 +108,8 @@ def multiclass_non_max_suppression(boxes,
     raise ValueError('if change_coordinate_frame is True, then a clip_window'
                      'must be specified.')
 
-  with tf.name_scope(scope, 'MultiClassNonMaxSuppression'):
-    num_scores = tf.shape(scores)[0]
+  with tf.compat.v1.name_scope(scope, 'MultiClassNonMaxSuppression'):
+    num_scores = tf.shape(input=scores)[0]
     num_classes = scores.get_shape()[1]
 
     selected_boxes_list = []
@@ -258,15 +258,15 @@ def batch_multiclass_non_max_suppression(boxes,
                      'must be specified.')
   original_masks = masks
   original_additional_fields = additional_fields
-  with tf.name_scope(scope, 'BatchMultiClassNonMaxSuppression'):
+  with tf.compat.v1.name_scope(scope, 'BatchMultiClassNonMaxSuppression'):
     boxes_shape = boxes.shape
     batch_size = boxes_shape[0].value
     num_anchors = boxes_shape[1].value
 
     if batch_size is None:
-      batch_size = tf.shape(boxes)[0]
+      batch_size = tf.shape(input=boxes)[0]
     if num_anchors is None:
-      num_anchors = tf.shape(boxes)[1]
+      num_anchors = tf.shape(input=boxes)[1]
 
     # If num valid boxes aren't provided, create one and mark all boxes as
     # valid.
@@ -281,10 +281,10 @@ def batch_multiclass_non_max_suppression(boxes,
 
     if clip_window is None:
       clip_window = tf.stack([
-          tf.reduce_min(boxes[:, :, :, 0]),
-          tf.reduce_min(boxes[:, :, :, 1]),
-          tf.reduce_max(boxes[:, :, :, 2]),
-          tf.reduce_max(boxes[:, :, :, 3])
+          tf.reduce_min(input_tensor=boxes[:, :, :, 0]),
+          tf.reduce_min(input_tensor=boxes[:, :, :, 1]),
+          tf.reduce_max(input_tensor=boxes[:, :, :, 2]),
+          tf.reduce_max(input_tensor=boxes[:, :, :, 3])
       ])
     if clip_window.shape.ndims == 1:
       clip_window = tf.tile(tf.expand_dims(clip_window, 0), [batch_size, 1])
diff --git a/research/object_detection/core/post_processing_test.py b/research/object_detection/core/post_processing_test.py
index 85c92437..fcc19f1d 100644
--- a/research/object_detection/core/post_processing_test.py
+++ b/research/object_detection/core/post_processing_test.py
@@ -124,7 +124,7 @@ class MulticlassNonMaxSuppressionTest(tf.test.TestCase):
                           [.5, 0.01], [.3, 0.01],
                           [.01, .85], [.01, .5]])
 
-    num_boxes = tf.shape(boxes)[0]
+    num_boxes = tf.shape(input=boxes)[0]
     heatmap_height = 5
     heatmap_width = 5
     num_keypoints = 17
@@ -832,9 +832,9 @@ class MulticlassNonMaxSuppressionTest(tf.test.TestCase):
       self.assertAllClose(num_detections, [2, 3])
 
   def test_batch_multiclass_nms_with_dynamic_batch_size(self):
-    boxes_placeholder = tf.placeholder(tf.float32, shape=(None, None, 2, 4))
-    scores_placeholder = tf.placeholder(tf.float32, shape=(None, None, 2))
-    masks_placeholder = tf.placeholder(tf.float32, shape=(None, None, 2, 2, 2))
+    boxes_placeholder = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 2, 4))
+    scores_placeholder = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 2))
+    masks_placeholder = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 2, 2, 2))
 
     boxes = np.array([[[[0, 0, 1, 1], [0, 0, 4, 5]],
                        [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],
diff --git a/research/object_detection/core/prefetcher.py b/research/object_detection/core/prefetcher.py
index e690c599..a36c2d7f 100644
--- a/research/object_detection/core/prefetcher.py
+++ b/research/object_detection/core/prefetcher.py
@@ -48,14 +48,14 @@ def prefetch(tensor_dict, capacity):
   names = list(tensor_dict.keys())
   dtypes = [t.dtype for t in tensor_dict.values()]
   shapes = [t.get_shape() for t in tensor_dict.values()]
-  prefetch_queue = tf.PaddingFIFOQueue(capacity, dtypes=dtypes,
+  prefetch_queue = tf.queue.PaddingFIFOQueue(capacity, dtypes=dtypes,
                                        shapes=shapes,
                                        names=names,
                                        name='prefetch_queue')
   enqueue_op = prefetch_queue.enqueue(tensor_dict)
-  tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(
+  tf.compat.v1.train.queue_runner.add_queue_runner(tf.compat.v1.train.queue_runner.QueueRunner(
       prefetch_queue, [enqueue_op]))
-  tf.summary.scalar('queue/%s/fraction_of_%d_full' % (prefetch_queue.name,
+  tf.compat.v1.summary.scalar('queue/%s/fraction_of_%d_full' % (prefetch_queue.name,
                                                       capacity),
-                    tf.to_float(prefetch_queue.size()) * (1. / capacity))
+                    tf.cast(prefetch_queue.size(), dtype=tf.float32) * (1. / capacity))
   return prefetch_queue
diff --git a/research/object_detection/core/prefetcher_test.py b/research/object_detection/core/prefetcher_test.py
index 63f557e3..4fa5fa7d 100644
--- a/research/object_detection/core/prefetcher_test.py
+++ b/research/object_detection/core/prefetcher_test.py
@@ -30,11 +30,11 @@ class PrefetcherTest(tf.test.TestCase):
       num_batches = 5
       examples = tf.Variable(tf.constant(0, dtype=tf.int64))
       counter = examples.count_up_to(num_batches)
-      image = tf.random_normal([batch_size, image_size,
+      image = tf.random.normal([batch_size, image_size,
                                 image_size, 3],
                                dtype=tf.float32,
                                name='images')
-      label = tf.random_uniform([batch_size, 1], 0, 10,
+      label = tf.random.uniform([batch_size, 1], 0, 10,
                                 dtype=tf.int32, name='labels')
 
       prefetch_queue = prefetcher.prefetch(tensor_dict={'counter': counter,
@@ -48,7 +48,7 @@ class PrefetcherTest(tf.test.TestCase):
       self.assertAllEqual(tensor_dict['label'].get_shape().as_list(),
                           [batch_size, 1])
 
-      tf.initialize_all_variables().run()
+      tf.compat.v1.initialize_all_variables().run()
       with slim.queues.QueueRunners(sess):
         for _ in range(num_batches):
           results = sess.run(tensor_dict)
@@ -65,13 +65,13 @@ class PrefetcherTest(tf.test.TestCase):
       num_batches = 5
       examples = tf.Variable(tf.constant(0, dtype=tf.int64))
       counter = examples.count_up_to(num_batches)
-      image = tf.random_normal([batch_size,
+      image = tf.random.normal([batch_size,
                                 tf.Variable(image_size),
                                 tf.Variable(image_size), 3],
                                dtype=tf.float32,
                                name='image')
       image.set_shape([batch_size, None, None, 3])
-      label = tf.random_uniform([batch_size, tf.Variable(1)], 0,
+      label = tf.random.uniform([batch_size, tf.Variable(1)], 0,
                                 10, dtype=tf.int32, name='label')
       label.set_shape([batch_size, None])
 
@@ -86,7 +86,7 @@ class PrefetcherTest(tf.test.TestCase):
       self.assertAllEqual(tensor_dict['label'].get_shape().as_list(),
                           [batch_size, None])
 
-      tf.initialize_all_variables().run()
+      tf.compat.v1.initialize_all_variables().run()
       with slim.queues.QueueRunners(sess):
         for _ in range(num_batches):
           results = sess.run(tensor_dict)
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 3e525989..6d501a11 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -103,7 +103,7 @@ def _apply_with_random_selector(x,
     selector as a python integer, but sel is sampled dynamically.
   """
   generator_func = functools.partial(
-      tf.random_uniform, [], maxval=num_cases, dtype=tf.int32)
+      tf.random.uniform, [], maxval=num_cases, dtype=tf.int32)
   rand_sel = _get_or_create_preprocess_rand_vars(
       generator_func, preprocessor_cache.PreprocessorCache.SELECTOR,
       preprocess_vars_cache, key)
@@ -140,7 +140,7 @@ def _apply_with_random_selector_tuples(x,
   """
   num_inputs = len(x)
   generator_func = functools.partial(
-      tf.random_uniform, [], maxval=num_cases, dtype=tf.int32)
+      tf.random.uniform, [], maxval=num_cases, dtype=tf.int32)
   rand_sel = _get_or_create_preprocess_rand_vars(
       generator_func, preprocessor_cache.PreprocessorCache.SELECTOR_TUPLES,
       preprocess_vars_cache, key)
@@ -200,7 +200,7 @@ def _random_integer(minval, maxval, seed):
   Returns:
     A random 0-D tensor between minval and maxval.
   """
-  return tf.random_uniform(
+  return tf.random.uniform(
       [], minval=minval, maxval=maxval, dtype=tf.int32, seed=seed)
 
 
@@ -222,8 +222,8 @@ def _rgb_to_grayscale(images, name=None):
   Returns:
     The converted grayscale image(s).
   """
-  with tf.name_scope(name, 'rgb_to_grayscale', [images]) as name:
-    images = tf.convert_to_tensor(images, name='images')
+  with tf.compat.v1.name_scope(name, 'rgb_to_grayscale', [images]) as name:
+    images = tf.convert_to_tensor(value=images, name='images')
     # Remember original dtype to so we can convert back if needed
     orig_dtype = images.dtype
     flt_image = tf.image.convert_image_dtype(images, tf.float32)
@@ -233,7 +233,7 @@ def _rgb_to_grayscale(images, name=None):
     rgb_weights = [0.2989, 0.5870, 0.1140]
     rank_1 = tf.expand_dims(tf.rank(images) - 1, 0)
     gray_float = tf.reduce_sum(
-        flt_image * rgb_weights, rank_1, keep_dims=True)
+        input_tensor=flt_image * rgb_weights, axis=rank_1, keepdims=True)
     gray_float.set_shape(images.get_shape()[:-1].concatenate([1]))
     return tf.image.convert_image_dtype(gray_float, orig_dtype, name=name)
 
@@ -256,12 +256,12 @@ def normalize_image(image, original_minval, original_maxval, target_minval,
   Returns:
     image: image which is the same shape as input image.
   """
-  with tf.name_scope('NormalizeImage', values=[image]):
+  with tf.compat.v1.name_scope('NormalizeImage', values=[image]):
     original_minval = float(original_minval)
     original_maxval = float(original_maxval)
     target_minval = float(target_minval)
     target_maxval = float(target_maxval)
-    image = tf.to_float(image)
+    image = tf.cast(image, dtype=tf.float32)
     image = tf.subtract(image, original_minval)
     image = tf.multiply(image, (target_maxval - target_minval) /
                         (original_maxval - original_minval))
@@ -312,10 +312,10 @@ def retain_boxes_above_threshold(boxes,
     retained_masks: [num_retained_instance, height, width]
     retained_keypoints: [num_retained_instance, num_keypoints, 2]
   """
-  with tf.name_scope('RetainBoxesAboveThreshold',
+  with tf.compat.v1.name_scope('RetainBoxesAboveThreshold',
                      values=[boxes, labels, label_scores]):
-    indices = tf.where(
-        tf.logical_or(label_scores > threshold, tf.is_nan(label_scores)))
+    indices = tf.compat.v1.where(
+        tf.logical_or(label_scores > threshold, tf.math.is_nan(label_scores)))
     indices = tf.squeeze(indices, axis=1)
     retained_boxes = tf.gather(boxes, indices)
     retained_labels = tf.gather(labels, indices)
@@ -436,7 +436,7 @@ def _rot90_masks(masks):
     rotated masks: rank 3 float32 tensor with shape
       [num_instances, height, width] representing instance masks.
   """
-  masks = tf.transpose(masks, [0, 2, 1])
+  masks = tf.transpose(a=masks, perm=[0, 2, 1])
   return masks[:, ::-1, :]
 
 
@@ -499,10 +499,10 @@ def random_horizontal_flip(image,
     raise ValueError(
         'keypoints are provided but keypoints_flip_permutation is not provided')
 
-  with tf.name_scope('RandomHorizontalFlip', values=[image, boxes]):
+  with tf.compat.v1.name_scope('RandomHorizontalFlip', values=[image, boxes]):
     result = []
     # random variable defining whether to do flip or not
-    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    generator_func = functools.partial(tf.random.uniform, [], seed=seed)
     do_a_flip_random = _get_or_create_preprocess_rand_vars(
         generator_func,
         preprocessor_cache.PreprocessorCache.HORIZONTAL_FLIP,
@@ -510,28 +510,28 @@ def random_horizontal_flip(image,
     do_a_flip_random = tf.greater(do_a_flip_random, 0.5)
 
     # flip image
-    image = tf.cond(do_a_flip_random, lambda: _flip_image(image), lambda: image)
+    image = tf.cond(pred=do_a_flip_random, true_fn=lambda: _flip_image(image), false_fn=lambda: image)
     result.append(image)
 
     # flip boxes
     if boxes is not None:
-      boxes = tf.cond(do_a_flip_random, lambda: _flip_boxes_left_right(boxes),
-                      lambda: boxes)
+      boxes = tf.cond(pred=do_a_flip_random, true_fn=lambda: _flip_boxes_left_right(boxes),
+                      false_fn=lambda: boxes)
       result.append(boxes)
 
     # flip masks
     if masks is not None:
-      masks = tf.cond(do_a_flip_random, lambda: _flip_masks_left_right(masks),
-                      lambda: masks)
+      masks = tf.cond(pred=do_a_flip_random, true_fn=lambda: _flip_masks_left_right(masks),
+                      false_fn=lambda: masks)
       result.append(masks)
 
     # flip keypoints
     if keypoints is not None and keypoint_flip_permutation is not None:
       permutation = keypoint_flip_permutation
       keypoints = tf.cond(
-          do_a_flip_random,
-          lambda: keypoint_ops.flip_horizontal(keypoints, 0.5, permutation),
-          lambda: keypoints)
+          pred=do_a_flip_random,
+          true_fn=lambda: keypoint_ops.flip_horizontal(keypoints, 0.5, permutation),
+          false_fn=lambda: keypoints)
       result.append(keypoints)
 
     return tuple(result)
@@ -596,38 +596,38 @@ def random_vertical_flip(image,
     raise ValueError(
         'keypoints are provided but keypoints_flip_permutation is not provided')
 
-  with tf.name_scope('RandomVerticalFlip', values=[image, boxes]):
+  with tf.compat.v1.name_scope('RandomVerticalFlip', values=[image, boxes]):
     result = []
     # random variable defining whether to do flip or not
-    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    generator_func = functools.partial(tf.random.uniform, [], seed=seed)
     do_a_flip_random = _get_or_create_preprocess_rand_vars(
         generator_func, preprocessor_cache.PreprocessorCache.VERTICAL_FLIP,
         preprocess_vars_cache)
     do_a_flip_random = tf.greater(do_a_flip_random, 0.5)
 
     # flip image
-    image = tf.cond(do_a_flip_random, lambda: _flip_image(image), lambda: image)
+    image = tf.cond(pred=do_a_flip_random, true_fn=lambda: _flip_image(image), false_fn=lambda: image)
     result.append(image)
 
     # flip boxes
     if boxes is not None:
-      boxes = tf.cond(do_a_flip_random, lambda: _flip_boxes_up_down(boxes),
-                      lambda: boxes)
+      boxes = tf.cond(pred=do_a_flip_random, true_fn=lambda: _flip_boxes_up_down(boxes),
+                      false_fn=lambda: boxes)
       result.append(boxes)
 
     # flip masks
     if masks is not None:
-      masks = tf.cond(do_a_flip_random, lambda: _flip_masks_up_down(masks),
-                      lambda: masks)
+      masks = tf.cond(pred=do_a_flip_random, true_fn=lambda: _flip_masks_up_down(masks),
+                      false_fn=lambda: masks)
       result.append(masks)
 
     # flip keypoints
     if keypoints is not None and keypoint_flip_permutation is not None:
       permutation = keypoint_flip_permutation
       keypoints = tf.cond(
-          do_a_flip_random,
-          lambda: keypoint_ops.flip_vertical(keypoints, 0.5, permutation),
-          lambda: keypoints)
+          pred=do_a_flip_random,
+          true_fn=lambda: keypoint_ops.flip_vertical(keypoints, 0.5, permutation),
+          false_fn=lambda: keypoints)
       result.append(keypoints)
 
     return tuple(result)
@@ -685,39 +685,39 @@ def random_rotation90(image,
     image_rotated = tf.image.rot90(image)
     return image_rotated
 
-  with tf.name_scope('RandomRotation90', values=[image, boxes]):
+  with tf.compat.v1.name_scope('RandomRotation90', values=[image, boxes]):
     result = []
 
     # random variable defining whether to rotate by 90 degrees or not
-    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    generator_func = functools.partial(tf.random.uniform, [], seed=seed)
     do_a_rot90_random = _get_or_create_preprocess_rand_vars(
         generator_func, preprocessor_cache.PreprocessorCache.ROTATION90,
         preprocess_vars_cache)
     do_a_rot90_random = tf.greater(do_a_rot90_random, 0.5)
 
     # flip image
-    image = tf.cond(do_a_rot90_random, lambda: _rot90_image(image),
-                    lambda: image)
+    image = tf.cond(pred=do_a_rot90_random, true_fn=lambda: _rot90_image(image),
+                    false_fn=lambda: image)
     result.append(image)
 
     # flip boxes
     if boxes is not None:
-      boxes = tf.cond(do_a_rot90_random, lambda: _rot90_boxes(boxes),
-                      lambda: boxes)
+      boxes = tf.cond(pred=do_a_rot90_random, true_fn=lambda: _rot90_boxes(boxes),
+                      false_fn=lambda: boxes)
       result.append(boxes)
 
     # flip masks
     if masks is not None:
-      masks = tf.cond(do_a_rot90_random, lambda: _rot90_masks(masks),
-                      lambda: masks)
+      masks = tf.cond(pred=do_a_rot90_random, true_fn=lambda: _rot90_masks(masks),
+                      false_fn=lambda: masks)
       result.append(masks)
 
     # flip keypoints
     if keypoints is not None:
       keypoints = tf.cond(
-          do_a_rot90_random,
-          lambda: keypoint_ops.rot90(keypoints),
-          lambda: keypoints)
+          pred=do_a_rot90_random,
+          true_fn=lambda: keypoint_ops.rot90(keypoints),
+          false_fn=lambda: keypoints)
       result.append(keypoints)
 
     return tuple(result)
@@ -748,9 +748,9 @@ def random_pixel_value_scale(image,
   Returns:
     image: image which is the same shape as input image.
   """
-  with tf.name_scope('RandomPixelValueScale', values=[image]):
+  with tf.compat.v1.name_scope('RandomPixelValueScale', values=[image]):
     generator_func = functools.partial(
-        tf.random_uniform, tf.shape(image),
+        tf.random.uniform, tf.shape(input=image),
         minval=minval, maxval=maxval,
         dtype=tf.float32, seed=seed)
     color_coef = _get_or_create_preprocess_rand_vars(
@@ -790,29 +790,29 @@ def random_image_scale(image,
     masks: If masks is not none, resized masks which are the same rank as input
       masks will be returned.
   """
-  with tf.name_scope('RandomImageScale', values=[image]):
+  with tf.compat.v1.name_scope('RandomImageScale', values=[image]):
     result = []
-    image_shape = tf.shape(image)
+    image_shape = tf.shape(input=image)
     image_height = image_shape[0]
     image_width = image_shape[1]
     generator_func = functools.partial(
-        tf.random_uniform, [],
+        tf.random.uniform, [],
         minval=min_scale_ratio, maxval=max_scale_ratio,
         dtype=tf.float32, seed=seed)
     size_coef = _get_or_create_preprocess_rand_vars(
         generator_func, preprocessor_cache.PreprocessorCache.IMAGE_SCALE,
         preprocess_vars_cache)
 
-    image_newysize = tf.to_int32(
-        tf.multiply(tf.to_float(image_height), size_coef))
-    image_newxsize = tf.to_int32(
-        tf.multiply(tf.to_float(image_width), size_coef))
-    image = tf.image.resize_images(
-        image, [image_newysize, image_newxsize], align_corners=True)
+    image_newysize = tf.cast(
+        tf.multiply(tf.cast(image_height, dtype=tf.float32), size_coef), dtype=tf.int32)
+    image_newxsize = tf.cast(
+        tf.multiply(tf.cast(image_width, dtype=tf.float32), size_coef), dtype=tf.int32)
+    image = tf.image.resize(
+        image, [image_newysize, image_newxsize])
     result.append(image)
     if masks:
-      masks = tf.image.resize_nearest_neighbor(
-          masks, [image_newysize, image_newxsize], align_corners=True)
+      masks = tf.image.resize(
+          masks, [image_newysize, image_newxsize], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
       result.append(masks)
     return tuple(result)
 
@@ -842,16 +842,16 @@ def random_rgb_to_gray(image,
     image_gray3 = tf.image.grayscale_to_rgb(image_gray1)
     return image_gray3
 
-  with tf.name_scope('RandomRGBtoGray', values=[image]):
+  with tf.compat.v1.name_scope('RandomRGBtoGray', values=[image]):
     # random variable defining whether to change to grayscale or not
-    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    generator_func = functools.partial(tf.random.uniform, [], seed=seed)
     do_gray_random = _get_or_create_preprocess_rand_vars(
         generator_func, preprocessor_cache.PreprocessorCache.RGB_TO_GRAY,
         preprocess_vars_cache)
 
     image = tf.cond(
-        tf.greater(do_gray_random, probability), lambda: image,
-        lambda: _image_to_gray(image))
+        pred=tf.greater(do_gray_random, probability), true_fn=lambda: image,
+        false_fn=lambda: _image_to_gray(image))
 
   return image
 
@@ -878,8 +878,8 @@ def random_adjust_brightness(image,
     image: image which is the same shape as input image.
     boxes: boxes which is the same shape as input boxes.
   """
-  with tf.name_scope('RandomAdjustBrightness', values=[image]):
-    generator_func = functools.partial(tf.random_uniform, [],
+  with tf.compat.v1.name_scope('RandomAdjustBrightness', values=[image]):
+    generator_func = functools.partial(tf.random.uniform, [],
                                        -max_delta, max_delta, seed=seed)
     delta = _get_or_create_preprocess_rand_vars(
         generator_func,
@@ -916,8 +916,8 @@ def random_adjust_contrast(image,
   Returns:
     image: image which is the same shape as input image.
   """
-  with tf.name_scope('RandomAdjustContrast', values=[image]):
-    generator_func = functools.partial(tf.random_uniform, [],
+  with tf.compat.v1.name_scope('RandomAdjustContrast', values=[image]):
+    generator_func = functools.partial(tf.random.uniform, [],
                                        min_delta, max_delta, seed=seed)
     contrast_factor = _get_or_create_preprocess_rand_vars(
         generator_func,
@@ -949,8 +949,8 @@ def random_adjust_hue(image,
   Returns:
     image: image which is the same shape as input image.
   """
-  with tf.name_scope('RandomAdjustHue', values=[image]):
-    generator_func = functools.partial(tf.random_uniform, [],
+  with tf.compat.v1.name_scope('RandomAdjustHue', values=[image]):
+    generator_func = functools.partial(tf.random.uniform, [],
                                        -max_delta, max_delta, seed=seed)
     delta = _get_or_create_preprocess_rand_vars(
         generator_func, preprocessor_cache.PreprocessorCache.ADJUST_HUE,
@@ -985,8 +985,8 @@ def random_adjust_saturation(image,
   Returns:
     image: image which is the same shape as input image.
   """
-  with tf.name_scope('RandomAdjustSaturation', values=[image]):
-    generator_func = functools.partial(tf.random_uniform, [],
+  with tf.compat.v1.name_scope('RandomAdjustSaturation', values=[image]):
+    generator_func = functools.partial(tf.random.uniform, [],
                                        min_delta, max_delta, seed=seed)
     saturation_factor = _get_or_create_preprocess_rand_vars(
         generator_func,
@@ -1018,7 +1018,7 @@ def random_distort_color(image, color_ordering=0, preprocess_vars_cache=None):
   Raises:
     ValueError: if color_ordering is not in {0, 1}.
   """
-  with tf.name_scope('RandomDistortColor', values=[image]):
+  with tf.compat.v1.name_scope('RandomDistortColor', values=[image]):
     if color_ordering == 0:
       image = random_adjust_brightness(
           image, max_delta=32. / 255.,
@@ -1079,7 +1079,7 @@ def random_jitter_boxes(boxes, ratio=0.05, seed=None):
     Returns:
       jittered_box: jittered box.
     """
-    rand_numbers = tf.random_uniform(
+    rand_numbers = tf.random.uniform(
         [1, 1, 4], minval=-ratio, maxval=ratio, dtype=tf.float32, seed=seed)
     box_width = tf.subtract(box[0, 0, 3], box[0, 0, 1])
     box_height = tf.subtract(box[0, 0, 2], box[0, 0, 0])
@@ -1089,9 +1089,9 @@ def random_jitter_boxes(boxes, ratio=0.05, seed=None):
     jittered_box = tf.clip_by_value(jittered_box, 0.0, 1.0)
     return jittered_box
 
-  with tf.name_scope('RandomJitterBoxes', values=[boxes]):
+  with tf.compat.v1.name_scope('RandomJitterBoxes', values=[boxes]):
     # boxes are [N, 4]. Lets first make them [N, 1, 1, 4]
-    boxes_shape = tf.shape(boxes)
+    boxes_shape = tf.shape(input=boxes)
     boxes = tf.expand_dims(boxes, 1)
     boxes = tf.expand_dims(boxes, 2)
 
@@ -1171,8 +1171,8 @@ def _strict_random_crop_image(image,
     keypoints: rank 3 float32 tensor with shape
                [num_instances, num_keypoints, 2]
   """
-  with tf.name_scope('RandomCropImage', values=[image, boxes]):
-    image_shape = tf.shape(image)
+  with tf.compat.v1.name_scope('RandomCropImage', values=[image, boxes]):
+    image_shape = tf.shape(input=image)
 
     # boxes are [N, 4]. Lets first make them [N, 1, 4].
     boxes_expanded = tf.expand_dims(
@@ -1202,7 +1202,7 @@ def _strict_random_crop_image(image,
     new_image.set_shape([None, None, image.get_shape()[2]])
 
     # [1, 4]
-    im_box_rank2 = tf.squeeze(im_box, squeeze_dims=[0])
+    im_box_rank2 = tf.squeeze(im_box, axis=[0])
     # [4]
     im_box_rank1 = tf.squeeze(im_box)
 
@@ -1369,7 +1369,7 @@ def random_crop_image(image,
   if random_coef < sys.float_info.min:
     result = strict_random_crop_image_fn()
   else:
-    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    generator_func = functools.partial(tf.random.uniform, [], seed=seed)
     do_a_crop_random = _get_or_create_preprocess_rand_vars(
         generator_func, preprocessor_cache.PreprocessorCache.CROP_IMAGE,
         preprocess_vars_cache)
@@ -1386,8 +1386,8 @@ def random_crop_image(image,
     if keypoints is not None:
       outputs.append(keypoints)
 
-    result = tf.cond(do_a_crop_random, strict_random_crop_image_fn,
-                     lambda: tuple(outputs))
+    result = tf.cond(pred=do_a_crop_random, true_fn=strict_random_crop_image_fn,
+                     false_fn=lambda: tuple(outputs))
   return result
 
 
@@ -1435,9 +1435,9 @@ def random_pad_image(image,
            form.
   """
   if pad_color is None:
-    pad_color = tf.reduce_mean(image, axis=[0, 1])
+    pad_color = tf.reduce_mean(input_tensor=image, axis=[0, 1])
 
-  image_shape = tf.shape(image)
+  image_shape = tf.shape(input=image)
   image_height = image_shape[0]
   image_width = image_shape[1]
 
@@ -1452,24 +1452,24 @@ def random_pad_image(image,
                               tf.stack([image_height, image_width]))
 
   target_height = tf.cond(
-      max_image_size[0] > min_image_size[0],
-      lambda: _random_integer(min_image_size[0], max_image_size[0], seed),
-      lambda: max_image_size[0])
+      pred=max_image_size[0] > min_image_size[0],
+      true_fn=lambda: _random_integer(min_image_size[0], max_image_size[0], seed),
+      false_fn=lambda: max_image_size[0])
 
   target_width = tf.cond(
-      max_image_size[1] > min_image_size[1],
-      lambda: _random_integer(min_image_size[1], max_image_size[1], seed),
-      lambda: max_image_size[1])
+      pred=max_image_size[1] > min_image_size[1],
+      true_fn=lambda: _random_integer(min_image_size[1], max_image_size[1], seed),
+      false_fn=lambda: max_image_size[1])
 
   offset_height = tf.cond(
-      target_height > image_height,
-      lambda: _random_integer(0, target_height - image_height, seed),
-      lambda: tf.constant(0, dtype=tf.int32))
+      pred=target_height > image_height,
+      true_fn=lambda: _random_integer(0, target_height - image_height, seed),
+      false_fn=lambda: tf.constant(0, dtype=tf.int32))
 
   offset_width = tf.cond(
-      target_width > image_width,
-      lambda: _random_integer(0, target_width - image_width, seed),
-      lambda: tf.constant(0, dtype=tf.int32))
+      pred=target_width > image_width,
+      true_fn=lambda: _random_integer(0, target_width - image_width, seed),
+      false_fn=lambda: tf.constant(0, dtype=tf.int32))
 
   gen_func = lambda: (target_height, target_width, offset_height, offset_width)
   params = _get_or_create_preprocess_rand_vars(
@@ -1496,13 +1496,13 @@ def random_pad_image(image,
   new_image += image_color_padded
 
   # setting boxes
-  new_window = tf.to_float(
+  new_window = tf.cast(
       tf.stack([
           -offset_height, -offset_width, target_height - offset_height,
           target_width - offset_width
-      ]))
-  new_window /= tf.to_float(
-      tf.stack([image_height, image_width, image_height, image_width]))
+      ]), dtype=tf.float32)
+  new_window /= tf.cast(
+      tf.stack([image_height, image_width, image_height, image_width]), dtype=tf.float32)
   boxlist = box_list.BoxList(boxes)
   new_boxlist = box_list_ops.change_coordinate_frame(boxlist, new_window)
   new_boxes = new_boxlist.get()
@@ -1584,7 +1584,7 @@ def random_crop_pad_image(image,
     cropped_multiclass_scores: cropped_multiclass_scores.
 
   """
-  image_size = tf.shape(image)
+  image_size = tf.shape(input=image)
   image_height = image_size[0]
   image_width = image_size[1]
   result = random_crop_image(
@@ -1603,12 +1603,12 @@ def random_crop_pad_image(image,
 
   cropped_image, cropped_boxes, cropped_labels = result[:3]
 
-  min_image_size = tf.to_int32(
-      tf.to_float(tf.stack([image_height, image_width])) *
-      min_padded_size_ratio)
-  max_image_size = tf.to_int32(
-      tf.to_float(tf.stack([image_height, image_width])) *
-      max_padded_size_ratio)
+  min_image_size = tf.cast(
+      tf.cast(tf.stack([image_height, image_width]), dtype=tf.float32) *
+      min_padded_size_ratio, dtype=tf.int32)
+  max_image_size = tf.cast(
+      tf.cast(tf.stack([image_height, image_width]), dtype=tf.float32) *
+      max_padded_size_ratio, dtype=tf.int32)
 
   padded_image, padded_boxes = random_pad_image(
       cropped_image,
@@ -1706,23 +1706,23 @@ def random_crop_to_aspect_ratio(image,
   if len(image.get_shape()) != 3:
     raise ValueError('Image should be 3D tensor')
 
-  with tf.name_scope('RandomCropToAspectRatio', values=[image]):
-    image_shape = tf.shape(image)
+  with tf.compat.v1.name_scope('RandomCropToAspectRatio', values=[image]):
+    image_shape = tf.shape(input=image)
     orig_height = image_shape[0]
     orig_width = image_shape[1]
-    orig_aspect_ratio = tf.to_float(orig_width) / tf.to_float(orig_height)
+    orig_aspect_ratio = tf.cast(orig_width, dtype=tf.float32) / tf.cast(orig_height, dtype=tf.float32)
     new_aspect_ratio = tf.constant(aspect_ratio, dtype=tf.float32)
     def target_height_fn():
-      return tf.to_int32(tf.round(tf.to_float(orig_width) / new_aspect_ratio))
+      return tf.cast(tf.round(tf.cast(orig_width, dtype=tf.float32) / new_aspect_ratio), dtype=tf.int32)
 
-    target_height = tf.cond(orig_aspect_ratio >= new_aspect_ratio,
-                            lambda: orig_height, target_height_fn)
+    target_height = tf.cond(pred=orig_aspect_ratio >= new_aspect_ratio,
+                            true_fn=lambda: orig_height, false_fn=target_height_fn)
 
     def target_width_fn():
-      return tf.to_int32(tf.round(tf.to_float(orig_height) * new_aspect_ratio))
+      return tf.cast(tf.round(tf.cast(orig_height, dtype=tf.float32) * new_aspect_ratio), dtype=tf.int32)
 
-    target_width = tf.cond(orig_aspect_ratio <= new_aspect_ratio,
-                           lambda: orig_width, target_width_fn)
+    target_width = tf.cond(pred=orig_aspect_ratio <= new_aspect_ratio,
+                           true_fn=lambda: orig_width, false_fn=target_width_fn)
 
     # either offset_height = 0 and offset_width is randomly chosen from
     # [0, offset_width - target_width), or else offset_width = 0 and
@@ -1740,10 +1740,10 @@ def random_crop_to_aspect_ratio(image,
         image, offset_height, offset_width, target_height, target_width)
 
     im_box = tf.stack([
-        tf.to_float(offset_height) / tf.to_float(orig_height),
-        tf.to_float(offset_width) / tf.to_float(orig_width),
-        tf.to_float(offset_height + target_height) / tf.to_float(orig_height),
-        tf.to_float(offset_width + target_width) / tf.to_float(orig_width)
+        tf.cast(offset_height, dtype=tf.float32) / tf.cast(orig_height, dtype=tf.float32),
+        tf.cast(offset_width, dtype=tf.float32) / tf.cast(orig_width, dtype=tf.float32),
+        tf.cast(offset_height + target_height, dtype=tf.float32) / tf.cast(orig_height, dtype=tf.float32),
+        tf.cast(offset_width + target_width, dtype=tf.float32) / tf.cast(orig_width, dtype=tf.float32)
     ])
 
     boxlist = box_list.BoxList(boxes)
@@ -1857,20 +1857,20 @@ def random_pad_to_aspect_ratio(image,
   if len(image.get_shape()) != 3:
     raise ValueError('Image should be 3D tensor')
 
-  with tf.name_scope('RandomPadToAspectRatio', values=[image]):
-    image_shape = tf.shape(image)
-    image_height = tf.to_float(image_shape[0])
-    image_width = tf.to_float(image_shape[1])
+  with tf.compat.v1.name_scope('RandomPadToAspectRatio', values=[image]):
+    image_shape = tf.shape(input=image)
+    image_height = tf.cast(image_shape[0], dtype=tf.float32)
+    image_width = tf.cast(image_shape[1], dtype=tf.float32)
     image_aspect_ratio = image_width / image_height
     new_aspect_ratio = tf.constant(aspect_ratio, dtype=tf.float32)
     target_height = tf.cond(
-        image_aspect_ratio <= new_aspect_ratio,
-        lambda: image_height,
-        lambda: image_width / new_aspect_ratio)
+        pred=image_aspect_ratio <= new_aspect_ratio,
+        true_fn=lambda: image_height,
+        false_fn=lambda: image_width / new_aspect_ratio)
     target_width = tf.cond(
-        image_aspect_ratio >= new_aspect_ratio,
-        lambda: image_width,
-        lambda: image_height * new_aspect_ratio)
+        pred=image_aspect_ratio >= new_aspect_ratio,
+        true_fn=lambda: image_width,
+        false_fn=lambda: image_height * new_aspect_ratio)
 
     min_height = tf.maximum(
         min_padded_size_ratio[0] * image_height, target_height)
@@ -1886,7 +1886,7 @@ def random_pad_to_aspect_ratio(image,
         max_scale,
         tf.maximum(min_height / target_height, min_width / target_width))
 
-    generator_func = functools.partial(tf.random_uniform, [],
+    generator_func = functools.partial(tf.random.uniform, [],
                                        min_scale, max_scale, seed=seed)
     scale = _get_or_create_preprocess_rand_vars(
         generator_func,
@@ -1897,7 +1897,7 @@ def random_pad_to_aspect_ratio(image,
     target_width = tf.round(scale * target_width)
 
     new_image = tf.image.pad_to_bounding_box(
-        image, 0, 0, tf.to_int32(target_height), tf.to_int32(target_width))
+        image, 0, 0, tf.cast(target_height, dtype=tf.int32), tf.cast(target_width, dtype=tf.int32))
 
     im_box = tf.stack([
         0.0,
@@ -1914,8 +1914,8 @@ def random_pad_to_aspect_ratio(image,
     if masks is not None:
       new_masks = tf.expand_dims(masks, -1)
       new_masks = tf.image.pad_to_bounding_box(new_masks, 0, 0,
-                                               tf.to_int32(target_height),
-                                               tf.to_int32(target_width))
+                                               tf.cast(target_height, dtype=tf.int32),
+                                               tf.cast(target_width, dtype=tf.int32))
       new_masks = tf.squeeze(new_masks, [-1])
       result.append(new_masks)
 
@@ -1966,15 +1966,15 @@ def random_black_patches(image,
     Returns:
       image with a randomly added black box
     """
-    image_shape = tf.shape(image)
+    image_shape = tf.shape(input=image)
     image_height = image_shape[0]
     image_width = image_shape[1]
-    box_size = tf.to_int32(
+    box_size = tf.cast(
         tf.multiply(
-            tf.minimum(tf.to_float(image_height), tf.to_float(image_width)),
-            size_to_image_ratio))
+            tf.minimum(tf.cast(image_height, dtype=tf.float32), tf.cast(image_width, dtype=tf.float32)),
+            size_to_image_ratio), dtype=tf.int32)
 
-    generator_func = functools.partial(tf.random_uniform, [], minval=0.0,
+    generator_func = functools.partial(tf.random.uniform, [], minval=0.0,
                                        maxval=(1.0 - size_to_image_ratio),
                                        seed=random_seed)
     normalized_y_min = _get_or_create_preprocess_rand_vars(
@@ -1986,17 +1986,17 @@ def random_black_patches(image,
         preprocessor_cache.PreprocessorCache.ADD_BLACK_PATCH,
         preprocess_vars_cache, key=str(idx) + 'x')
 
-    y_min = tf.to_int32(normalized_y_min * tf.to_float(image_height))
-    x_min = tf.to_int32(normalized_x_min * tf.to_float(image_width))
+    y_min = tf.cast(normalized_y_min * tf.cast(image_height, dtype=tf.float32), dtype=tf.int32)
+    x_min = tf.cast(normalized_x_min * tf.cast(image_width, dtype=tf.float32), dtype=tf.int32)
     black_box = tf.ones([box_size, box_size, 3], dtype=tf.float32)
     mask = 1.0 - tf.image.pad_to_bounding_box(black_box, y_min, x_min,
                                               image_height, image_width)
     image = tf.multiply(image, mask)
     return image
 
-  with tf.name_scope('RandomBlackPatchInImage', values=[image]):
+  with tf.compat.v1.name_scope('RandomBlackPatchInImage', values=[image]):
     for idx in range(max_black_patches):
-      generator_func = functools.partial(tf.random_uniform, [],
+      generator_func = functools.partial(tf.random.uniform, [],
                                          minval=0.0, maxval=1.0,
                                          dtype=tf.float32, seed=random_seed)
       random_prob = _get_or_create_preprocess_rand_vars(
@@ -2004,8 +2004,8 @@ def random_black_patches(image,
           preprocessor_cache.PreprocessorCache.BLACK_PATCHES,
           preprocess_vars_cache, key=idx)
       image = tf.cond(
-          tf.greater(random_prob, probability), lambda: image,
-          functools.partial(add_black_patch_to_image, image=image, idx=idx))
+          pred=tf.greater(random_prob, probability), true_fn=lambda: image,
+          false_fn=functools.partial(add_black_patch_to_image, image=image, idx=idx))
     return image
 
 
@@ -2018,8 +2018,8 @@ def image_to_float(image):
   Returns:
     image: image in tf.float32 format.
   """
-  with tf.name_scope('ImageToFloat', values=[image]):
-    image = tf.to_float(image)
+  with tf.compat.v1.name_scope('ImageToFloat', values=[image]):
+    image = tf.cast(image, dtype=tf.float32)
     return image
 
 
@@ -2040,7 +2040,7 @@ def random_resize_method(image, target_size, preprocess_vars_cache=None):
 
   resized_image = _apply_with_random_selector(
       image,
-      lambda x, method: tf.image.resize_images(x, target_size, method),
+      lambda x, method: tf.image.resize(x, target_size, method),
       num_cases=4,
       preprocess_vars_cache=preprocess_vars_cache,
       key=preprocessor_cache.PreprocessorCache.RESIZE_METHOD)
@@ -2086,9 +2086,9 @@ def _compute_new_static_size(image, min_dimension, max_dimension):
 
 def _compute_new_dynamic_size(image, min_dimension, max_dimension):
   """Compute new dynamic shape for resize_to_range method."""
-  image_shape = tf.shape(image)
-  orig_height = tf.to_float(image_shape[0])
-  orig_width = tf.to_float(image_shape[1])
+  image_shape = tf.shape(input=image)
+  orig_height = tf.cast(image_shape[0], dtype=tf.float32)
+  orig_width = tf.cast(image_shape[1], dtype=tf.float32)
   num_channels = image_shape[2]
   orig_min_dim = tf.minimum(orig_height, orig_width)
   # Calculates the larger of the possible sizes
@@ -2098,8 +2098,8 @@ def _compute_new_dynamic_size(image, min_dimension, max_dimension):
   # dimension equal to min_dimension, save for floating point rounding errors.
   # For reasonably-sized images, taking the nearest integer will reliably
   # eliminate this error.
-  large_height = tf.to_int32(tf.round(orig_height * large_scale_factor))
-  large_width = tf.to_int32(tf.round(orig_width * large_scale_factor))
+  large_height = tf.cast(tf.round(orig_height * large_scale_factor), dtype=tf.int32)
+  large_width = tf.cast(tf.round(orig_width * large_scale_factor), dtype=tf.int32)
   large_size = tf.stack([large_height, large_width])
   if max_dimension:
     # Calculates the smaller of the possible sizes, use that if the larger
@@ -2111,12 +2111,12 @@ def _compute_new_dynamic_size(image, min_dimension, max_dimension):
     # dimension equal to max_dimension, save for floating point rounding
     # errors. For reasonably-sized images, taking the nearest integer will
     # reliably eliminate this error.
-    small_height = tf.to_int32(tf.round(orig_height * small_scale_factor))
-    small_width = tf.to_int32(tf.round(orig_width * small_scale_factor))
+    small_height = tf.cast(tf.round(orig_height * small_scale_factor), dtype=tf.int32)
+    small_width = tf.cast(tf.round(orig_width * small_scale_factor), dtype=tf.int32)
     small_size = tf.stack([small_height, small_width])
     new_size = tf.cond(
-        tf.to_float(tf.reduce_max(large_size)) > max_dimension,
-        lambda: small_size, lambda: large_size)
+        pred=tf.cast(tf.reduce_max(input_tensor=large_size), dtype=tf.float32) > max_dimension,
+        true_fn=lambda: small_size, false_fn=lambda: large_size)
   else:
     new_size = large_size
   return tf.stack(tf.unstack(new_size) + [num_channels])
@@ -2175,13 +2175,13 @@ def resize_to_range(image,
   if len(image.get_shape()) != 3:
     raise ValueError('Image should be 3D tensor')
 
-  with tf.name_scope('ResizeToRange', values=[image, min_dimension]):
+  with tf.compat.v1.name_scope('ResizeToRange', values=[image, min_dimension]):
     if image.get_shape().is_fully_defined():
       new_size = _compute_new_static_size(image, min_dimension, max_dimension)
     else:
       new_size = _compute_new_dynamic_size(image, min_dimension, max_dimension)
-    new_image = tf.image.resize_images(
-        image, new_size[:-1], method=method, align_corners=align_corners)
+    new_image = tf.image.resize(
+        image, new_size[:-1], method=method)
 
     if pad_to_max_dimension:
       channels = tf.unstack(new_image, axis=2)
@@ -2191,7 +2191,7 @@ def resize_to_range(image,
       new_image = tf.stack(
           [
               tf.pad(
-                  channels[i], [[0, max_dimension - new_size[0]],
+                  tensor=channels[i], paddings=[[0, max_dimension - new_size[0]],
                                 [0, max_dimension - new_size[1]]],
                   constant_values=per_channel_pad_value[i])
               for i in range(len(channels))
@@ -2202,11 +2202,10 @@ def resize_to_range(image,
     result = [new_image]
     if masks is not None:
       new_masks = tf.expand_dims(masks, 3)
-      new_masks = tf.image.resize_images(
+      new_masks = tf.image.resize(
           new_masks,
           new_size[:-1],
-          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
-          align_corners=align_corners)
+          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
       if pad_to_max_dimension:
         new_masks = tf.image.pad_to_bounding_box(
             new_masks, 0, 0, max_dimension, max_dimension)
@@ -2245,27 +2244,27 @@ def resize_to_min_dimension(image, masks=None, min_dimension=600):
   if len(image.get_shape()) != 3:
     raise ValueError('Image should be 3D tensor')
 
-  with tf.name_scope('ResizeGivenMinDimension', values=[image, min_dimension]):
-    image_height = tf.shape(image)[0]
-    image_width = tf.shape(image)[1]
-    num_channels = tf.shape(image)[2]
+  with tf.compat.v1.name_scope('ResizeGivenMinDimension', values=[image, min_dimension]):
+    image_height = tf.shape(input=image)[0]
+    image_width = tf.shape(input=image)[1]
+    num_channels = tf.shape(input=image)[2]
     min_image_dimension = tf.minimum(image_height, image_width)
     min_target_dimension = tf.maximum(min_image_dimension, min_dimension)
-    target_ratio = tf.to_float(min_target_dimension) / tf.to_float(
-        min_image_dimension)
-    target_height = tf.to_int32(tf.to_float(image_height) * target_ratio)
-    target_width = tf.to_int32(tf.to_float(image_width) * target_ratio)
-    image = tf.image.resize_bilinear(
+    target_ratio = tf.cast(min_target_dimension, dtype=tf.float32) / tf.cast(
+        min_image_dimension, dtype=tf.float32)
+    target_height = tf.cast(tf.cast(image_height, dtype=tf.float32) * target_ratio, dtype=tf.int32)
+    target_width = tf.cast(tf.cast(image_width, dtype=tf.float32) * target_ratio, dtype=tf.int32)
+    image = tf.image.resize(
         tf.expand_dims(image, axis=0),
-        size=[target_height, target_width],
-        align_corners=True)
+        method=tf.image.ResizeMethod.BILINEAR,
+        size=[target_height, target_width])
     result = [tf.squeeze(image, axis=0)]
 
     if masks is not None:
-      masks = tf.image.resize_nearest_neighbor(
+      masks = tf.image.resize(
           tf.expand_dims(masks, axis=3),
-          size=[target_height, target_width],
-          align_corners=True)
+          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
+          size=[target_height, target_width])
       result.append(tf.squeeze(masks, axis=3))
 
     result.append(tf.stack([target_height, target_width, num_channels]))
@@ -2293,8 +2292,8 @@ def scale_boxes_to_pixel_coordinates(image, boxes, keypoints=None):
       coordinates.
   """
   boxlist = box_list.BoxList(boxes)
-  image_height = tf.shape(image)[0]
-  image_width = tf.shape(image)[1]
+  image_height = tf.shape(input=image)[0]
+  image_width = tf.shape(input=image)[1]
   scaled_boxes = box_list_ops.scale(boxlist, image_height, image_width).get()
   result = [image, scaled_boxes]
   if keypoints is not None:
@@ -2334,22 +2333,21 @@ def resize_image(image,
     resized_image_shape: A 1D tensor of shape [3] containing the shape of the
       resized image.
   """
-  with tf.name_scope(
+  with tf.compat.v1.name_scope(
       'ResizeImage',
       values=[image, new_height, new_width, method, align_corners]):
-    new_image = tf.image.resize_images(
+    new_image = tf.image.resize(
         image, tf.stack([new_height, new_width]),
-        method=method,
-        align_corners=align_corners)
+        method=method)
     image_shape = shape_utils.combined_static_and_dynamic_shape(image)
     result = [new_image]
     if masks is not None:
-      num_instances = tf.shape(masks)[0]
+      num_instances = tf.shape(input=masks)[0]
       new_size = tf.stack([new_height, new_width])
       def resize_masks_branch():
         new_masks = tf.expand_dims(masks, 3)
-        new_masks = tf.image.resize_nearest_neighbor(
-            new_masks, new_size, align_corners=align_corners)
+        new_masks = tf.image.resize(
+            new_masks, new_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
         new_masks = tf.squeeze(new_masks, axis=3)
         return new_masks
 
@@ -2361,8 +2359,8 @@ def resize_image(image,
         new_masks = tf.reshape(masks, [-1, new_size[0], new_size[1]])
         return new_masks
 
-      masks = tf.cond(num_instances > 0, resize_masks_branch,
-                      reshape_masks_branch)
+      masks = tf.cond(pred=num_instances > 0, true_fn=resize_masks_branch,
+                      false_fn=reshape_masks_branch)
       result.append(masks)
 
     result.append(tf.stack([new_height, new_width, image_shape[2]]))
@@ -2381,7 +2379,7 @@ def subtract_channel_mean(image, means=None):
     ValueError: if images is not a 4D tensor or if the number of means is not
       equal to the number of channels.
   """
-  with tf.name_scope('SubtractChannelMean', values=[image, means]):
+  with tf.compat.v1.name_scope('SubtractChannelMean', values=[image, means]):
     if len(image.get_shape()) != 3:
       raise ValueError('Input must be of size [height, width, channels]')
     if len(means) != image.get_shape()[-1]:
@@ -2406,12 +2404,12 @@ def one_hot_encoding(labels, num_classes=None):
   Raises:
     ValueError: if num_classes is not specified.
   """
-  with tf.name_scope('OneHotEncoding', values=[labels]):
+  with tf.compat.v1.name_scope('OneHotEncoding', values=[labels]):
     if num_classes is None:
       raise ValueError('num_classes must be specified')
 
     labels = tf.one_hot(labels, num_classes, 1, 0)
-    return tf.reduce_max(labels, 0)
+    return tf.reduce_max(input_tensor=labels, axis=0)
 
 
 def rgb_to_gray(image):
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 47a8562f..64cffdec 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -536,13 +536,13 @@ class PreprocessorTest(tf.test.TestCase):
     images = tensor_dict[fields.InputDataFields.image]
     boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
 
-    boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)
-    boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)
+    boxes_diff1 = tf.math.squared_difference(boxes, boxes_expected1)
+    boxes_diff2 = tf.math.squared_difference(boxes, boxes_expected2)
     boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)
     boxes_diff_expected = tf.zeros_like(boxes_diff)
 
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
+    images_diff1 = tf.math.squared_difference(images, images_expected1)
+    images_diff2 = tf.math.squared_difference(images, images_expected2)
     images_diff = tf.multiply(images_diff1, images_diff2)
     images_diff_expected = tf.zeros_like(images_diff)
 
@@ -566,8 +566,8 @@ class PreprocessorTest(tf.test.TestCase):
     images = tensor_dict[fields.InputDataFields.image]
     boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
 
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
+    images_diff1 = tf.math.squared_difference(images, images_expected1)
+    images_diff2 = tf.math.squared_difference(images, images_expected2)
     images_diff = tf.multiply(images_diff1, images_diff2)
     images_diff_expected = tf.zeros_like(images_diff)
 
@@ -592,7 +592,7 @@ class PreprocessorTest(tf.test.TestCase):
     preprocess_options = [(preprocessor.random_horizontal_flip, {})]
     image_height = 3
     image_width = 3
-    images = tf.random_uniform([1, image_height, image_width, 3])
+    images = tf.random.uniform([1, image_height, image_width, 3])
     boxes = self.createTestBoxes()
     masks = self.createTestMasks()
     keypoints = self.createTestKeypoints()
@@ -633,13 +633,13 @@ class PreprocessorTest(tf.test.TestCase):
     images = tensor_dict[fields.InputDataFields.image]
     boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
 
-    boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)
-    boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)
+    boxes_diff1 = tf.math.squared_difference(boxes, boxes_expected1)
+    boxes_diff2 = tf.math.squared_difference(boxes, boxes_expected2)
     boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)
     boxes_diff_expected = tf.zeros_like(boxes_diff)
 
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
+    images_diff1 = tf.math.squared_difference(images, images_expected1)
+    images_diff2 = tf.math.squared_difference(images, images_expected2)
     images_diff = tf.multiply(images_diff1, images_diff2)
     images_diff_expected = tf.zeros_like(images_diff)
 
@@ -663,8 +663,8 @@ class PreprocessorTest(tf.test.TestCase):
     images = tensor_dict[fields.InputDataFields.image]
     boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
 
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
+    images_diff1 = tf.math.squared_difference(images, images_expected1)
+    images_diff2 = tf.math.squared_difference(images, images_expected2)
     images_diff = tf.multiply(images_diff1, images_diff2)
     images_diff_expected = tf.zeros_like(images_diff)
 
@@ -689,7 +689,7 @@ class PreprocessorTest(tf.test.TestCase):
     preprocess_options = [(preprocessor.random_vertical_flip, {})]
     image_height = 3
     image_width = 3
-    images = tf.random_uniform([1, image_height, image_width, 3])
+    images = tf.random.uniform([1, image_height, image_width, 3])
     boxes = self.createTestBoxes()
     masks = self.createTestMasks()
     keypoints = self.createTestKeypoints()
@@ -730,13 +730,13 @@ class PreprocessorTest(tf.test.TestCase):
     images = tensor_dict[fields.InputDataFields.image]
     boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
 
-    boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)
-    boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)
+    boxes_diff1 = tf.math.squared_difference(boxes, boxes_expected1)
+    boxes_diff2 = tf.math.squared_difference(boxes, boxes_expected2)
     boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)
     boxes_diff_expected = tf.zeros_like(boxes_diff)
 
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
+    images_diff1 = tf.math.squared_difference(images, images_expected1)
+    images_diff2 = tf.math.squared_difference(images, images_expected2)
     images_diff = tf.multiply(images_diff1, images_diff2)
     images_diff_expected = tf.zeros_like(images_diff)
 
@@ -760,8 +760,8 @@ class PreprocessorTest(tf.test.TestCase):
     images = tensor_dict[fields.InputDataFields.image]
     boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
 
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
+    images_diff1 = tf.math.squared_difference(images, images_expected1)
+    images_diff2 = tf.math.squared_difference(images, images_expected2)
     images_diff = tf.multiply(images_diff1, images_diff2)
     images_diff_expected = tf.zeros_like(images_diff)
 
@@ -783,7 +783,7 @@ class PreprocessorTest(tf.test.TestCase):
     preprocess_options = [(preprocessor.random_rotation90, {})]
     image_height = 3
     image_width = 3
-    images = tf.random_uniform([1, image_height, image_width, 3])
+    images = tf.random.uniform([1, image_height, image_width, 3])
     boxes = self.createTestBoxes()
     masks = self.createTestMasks()
     keypoints = self.createTestKeypoints()
@@ -818,8 +818,8 @@ class PreprocessorTest(tf.test.TestCase):
     images = self.createTestImages()
     tensor_dict = {fields.InputDataFields.image: images}
     tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images_min = tf.to_float(images) * 0.9 / 255.0
-    images_max = tf.to_float(images) * 1.1 / 255.0
+    images_min = tf.cast(images, dtype=tf.float32) * 0.9 / 255.0
+    images_max = tf.cast(images, dtype=tf.float32) * 1.1 / 255.0
     images = tensor_dict[fields.InputDataFields.image]
     values_greater = tf.greater_equal(images, images_min)
     values_less = tf.less_equal(images, images_max)
@@ -850,8 +850,8 @@ class PreprocessorTest(tf.test.TestCase):
     tensor_dict = {fields.InputDataFields.image: images_original}
     tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
     images_scaled = tensor_dict[fields.InputDataFields.image]
-    images_original_shape = tf.shape(images_original)
-    images_scaled_shape = tf.shape(images_scaled)
+    images_original_shape = tf.shape(input=images_original)
+    images_scaled_shape = tf.shape(input=images_scaled)
     with self.test_session() as sess:
       (images_original_shape_, images_scaled_shape_) = sess.run(
           [images_original_shape, images_scaled_shape])
@@ -881,20 +881,20 @@ class PreprocessorTest(tf.test.TestCase):
         value=images_gray, num_or_size_splits=3, axis=3)
     images_r, images_g, images_b = tf.split(
         value=images_original, num_or_size_splits=3, axis=3)
-    images_r_diff1 = tf.squared_difference(tf.to_float(images_r),
-                                           tf.to_float(images_gray_r))
-    images_r_diff2 = tf.squared_difference(tf.to_float(images_gray_r),
-                                           tf.to_float(images_gray_g))
+    images_r_diff1 = tf.math.squared_difference(tf.cast(images_r, dtype=tf.float32),
+                                           tf.cast(images_gray_r, dtype=tf.float32))
+    images_r_diff2 = tf.math.squared_difference(tf.cast(images_gray_r, dtype=tf.float32),
+                                           tf.cast(images_gray_g, dtype=tf.float32))
     images_r_diff = tf.multiply(images_r_diff1, images_r_diff2)
-    images_g_diff1 = tf.squared_difference(tf.to_float(images_g),
-                                           tf.to_float(images_gray_g))
-    images_g_diff2 = tf.squared_difference(tf.to_float(images_gray_g),
-                                           tf.to_float(images_gray_b))
+    images_g_diff1 = tf.math.squared_difference(tf.cast(images_g, dtype=tf.float32),
+                                           tf.cast(images_gray_g, dtype=tf.float32))
+    images_g_diff2 = tf.math.squared_difference(tf.cast(images_gray_g, dtype=tf.float32),
+                                           tf.cast(images_gray_b, dtype=tf.float32))
     images_g_diff = tf.multiply(images_g_diff1, images_g_diff2)
-    images_b_diff1 = tf.squared_difference(tf.to_float(images_b),
-                                           tf.to_float(images_gray_b))
-    images_b_diff2 = tf.squared_difference(tf.to_float(images_gray_b),
-                                           tf.to_float(images_gray_r))
+    images_b_diff1 = tf.math.squared_difference(tf.cast(images_b, dtype=tf.float32),
+                                           tf.cast(images_gray_b, dtype=tf.float32))
+    images_b_diff2 = tf.math.squared_difference(tf.cast(images_gray_b, dtype=tf.float32),
+                                           tf.cast(images_gray_r, dtype=tf.float32))
     images_b_diff = tf.multiply(images_b_diff1, images_b_diff2)
     image_zero1 = tf.constant(0, dtype=tf.float32, shape=[1, 4, 4, 1])
     with self.test_session() as sess:
@@ -925,8 +925,8 @@ class PreprocessorTest(tf.test.TestCase):
     tensor_dict = {fields.InputDataFields.image: images_original}
     tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
     images_bright = tensor_dict[fields.InputDataFields.image]
-    image_original_shape = tf.shape(images_original)
-    image_bright_shape = tf.shape(images_bright)
+    image_original_shape = tf.shape(input=images_original)
+    image_bright_shape = tf.shape(input=images_bright)
     with self.test_session() as sess:
       (image_original_shape_, image_bright_shape_) = sess.run(
           [image_original_shape, image_bright_shape])
@@ -959,8 +959,8 @@ class PreprocessorTest(tf.test.TestCase):
     tensor_dict = {fields.InputDataFields.image: images_original}
     tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
     images_contrast = tensor_dict[fields.InputDataFields.image]
-    image_original_shape = tf.shape(images_original)
-    image_contrast_shape = tf.shape(images_contrast)
+    image_original_shape = tf.shape(input=images_original)
+    image_contrast_shape = tf.shape(input=images_contrast)
     with self.test_session() as sess:
       (image_original_shape_, image_contrast_shape_) = sess.run(
           [image_original_shape, image_contrast_shape])
@@ -993,8 +993,8 @@ class PreprocessorTest(tf.test.TestCase):
     tensor_dict = {fields.InputDataFields.image: images_original}
     tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
     images_hue = tensor_dict[fields.InputDataFields.image]
-    image_original_shape = tf.shape(images_original)
-    image_hue_shape = tf.shape(images_hue)
+    image_original_shape = tf.shape(input=images_original)
+    image_hue_shape = tf.shape(input=images_hue)
     with self.test_session() as sess:
       (image_original_shape_, image_hue_shape_) = sess.run(
           [image_original_shape, image_hue_shape])
@@ -1024,11 +1024,11 @@ class PreprocessorTest(tf.test.TestCase):
     }))
     preprocessing_options.append((preprocessor.random_distort_color, {}))
     images_original = self.createTestImages()
-    images_original_shape = tf.shape(images_original)
+    images_original_shape = tf.shape(input=images_original)
     tensor_dict = {fields.InputDataFields.image: images_original}
     tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
     images_distorted_color = tensor_dict[fields.InputDataFields.image]
-    images_distorted_color_shape = tf.shape(images_distorted_color)
+    images_distorted_color_shape = tf.shape(input=images_distorted_color)
     with self.test_session() as sess:
       (images_original_shape_, images_distorted_color_shape_) = sess.run(
           [images_original_shape, images_distorted_color_shape])
@@ -1052,11 +1052,11 @@ class PreprocessorTest(tf.test.TestCase):
     preprocessing_options = []
     preprocessing_options.append((preprocessor.random_jitter_boxes, {}))
     boxes = self.createTestBoxes()
-    boxes_shape = tf.shape(boxes)
+    boxes_shape = tf.shape(input=boxes)
     tensor_dict = {fields.InputDataFields.groundtruth_boxes: boxes}
     tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
     distorted_boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
-    distorted_boxes_shape = tf.shape(distorted_boxes)
+    distorted_boxes_shape = tf.shape(input=distorted_boxes)
 
     with self.test_session() as sess:
       (boxes_shape_, distorted_boxes_shape_) = sess.run(
@@ -1220,10 +1220,10 @@ class PreprocessorTest(tf.test.TestCase):
         fields.InputDataFields.groundtruth_classes]
     distorted_label_scores = distorted_tensor_dict[
         fields.InputDataFields.groundtruth_label_scores]
-    boxes_shape = tf.shape(boxes)
-    distorted_boxes_shape = tf.shape(distorted_boxes)
-    images_shape = tf.shape(images)
-    distorted_images_shape = tf.shape(distorted_images)
+    boxes_shape = tf.shape(input=boxes)
+    distorted_boxes_shape = tf.shape(input=distorted_boxes)
+    images_shape = tf.shape(input=images)
+    distorted_images_shape = tf.shape(input=distorted_images)
 
     with self.test_session() as sess:
       (boxes_shape_, distorted_boxes_shape_, images_shape_,
@@ -1376,7 +1376,7 @@ class PreprocessorTest(tf.test.TestCase):
     image = self.createColorfulTestImage()[0]
     boxes = self.createTestBoxes()
     labels = self.createTestLabels()
-    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
+    masks = tf.random.uniform([2, 200, 400], dtype=tf.float32)
     with mock.patch.object(
         tf.image,
         'sample_distorted_bounding_box'
@@ -1440,7 +1440,7 @@ class PreprocessorTest(tf.test.TestCase):
     image = self.createColorfulTestImage()
     boxes = self.createTestBoxes()
     labels = self.createTestLabels()
-    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
+    masks = tf.random.uniform([2, 200, 400], dtype=tf.float32)
 
     tensor_dict = {
         fields.InputDataFields.image: image,
@@ -1721,7 +1721,7 @@ class PreprocessorTest(tf.test.TestCase):
     image = self.createColorfulTestImage()
     boxes = self.createTestBoxes()
     labels = self.createTestLabels()
-    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
+    masks = tf.random.uniform([2, 200, 400], dtype=tf.float32)
 
     tensor_dict = {
         fields.InputDataFields.image: image,
@@ -1853,7 +1853,7 @@ class PreprocessorTest(tf.test.TestCase):
     image = self.createColorfulTestImage()
     boxes = self.createTestBoxes()
     labels = self.createTestLabels()
-    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
+    masks = tf.random.uniform([2, 200, 400], dtype=tf.float32)
 
     tensor_dict = {
         fields.InputDataFields.image: image,
@@ -1974,10 +1974,10 @@ class PreprocessorTest(tf.test.TestCase):
     padded_images = padded_tensor_dict[fields.InputDataFields.image]
     padded_boxes = padded_tensor_dict[
         fields.InputDataFields.groundtruth_boxes]
-    boxes_shape = tf.shape(boxes)
-    padded_boxes_shape = tf.shape(padded_boxes)
-    images_shape = tf.shape(images)
-    padded_images_shape = tf.shape(padded_images)
+    boxes_shape = tf.shape(input=boxes)
+    padded_boxes_shape = tf.shape(input=padded_boxes)
+    images_shape = tf.shape(input=images)
+    padded_images_shape = tf.shape(input=padded_images)
 
     with self.test_session() as sess:
       (boxes_shape_, padded_boxes_shape_, images_shape_,
@@ -2033,10 +2033,10 @@ class PreprocessorTest(tf.test.TestCase):
     padded_images = padded_tensor_dict[fields.InputDataFields.image]
     padded_boxes = padded_tensor_dict[
         fields.InputDataFields.groundtruth_boxes]
-    boxes_shape = tf.shape(boxes)
-    padded_boxes_shape = tf.shape(padded_boxes)
-    images_shape = tf.shape(images)
-    padded_images_shape = tf.shape(padded_images)
+    boxes_shape = tf.shape(input=boxes)
+    padded_boxes_shape = tf.shape(input=padded_boxes)
+    images_shape = tf.shape(input=images)
+    padded_images_shape = tf.shape(input=padded_images)
 
     with self.test_session() as sess:
       (boxes_shape_, padded_boxes_shape_, images_shape_,
@@ -2074,10 +2074,10 @@ class PreprocessorTest(tf.test.TestCase):
     cropped_images = cropped_tensor_dict[fields.InputDataFields.image]
     cropped_boxes = cropped_tensor_dict[
         fields.InputDataFields.groundtruth_boxes]
-    boxes_shape = tf.shape(boxes)
-    cropped_boxes_shape = tf.shape(cropped_boxes)
-    images_shape = tf.shape(images)
-    cropped_images_shape = tf.shape(cropped_images)
+    boxes_shape = tf.shape(input=boxes)
+    cropped_boxes_shape = tf.shape(input=cropped_boxes)
+    images_shape = tf.shape(input=images)
+    cropped_images_shape = tf.shape(input=cropped_images)
 
     with self.test_session() as sess:
       (boxes_shape_, cropped_boxes_shape_, images_shape_,
@@ -2109,10 +2109,10 @@ class PreprocessorTest(tf.test.TestCase):
     padded_images = padded_tensor_dict[fields.InputDataFields.image]
     padded_boxes = padded_tensor_dict[
         fields.InputDataFields.groundtruth_boxes]
-    boxes_shape = tf.shape(boxes)
-    padded_boxes_shape = tf.shape(padded_boxes)
-    images_shape = tf.shape(images)
-    padded_images_shape = tf.shape(padded_images)
+    boxes_shape = tf.shape(input=boxes)
+    padded_boxes_shape = tf.shape(input=padded_boxes)
+    images_shape = tf.shape(input=images)
+    padded_images_shape = tf.shape(input=padded_images)
 
     with self.test_session() as sess:
       (boxes_shape_, padded_boxes_shape_, images_shape_,
@@ -2155,8 +2155,8 @@ class PreprocessorTest(tf.test.TestCase):
     blacked_tensor_dict = preprocessor.preprocess(tensor_dict,
                                                   preprocessing_options)
     blacked_images = blacked_tensor_dict[fields.InputDataFields.image]
-    images_shape = tf.shape(images)
-    blacked_images_shape = tf.shape(blacked_images)
+    images_shape = tf.shape(input=images)
+    blacked_images_shape = tf.shape(input=blacked_images)
 
     with self.test_session() as sess:
       (images_shape_, blacked_images_shape_) = sess.run(
@@ -2195,7 +2195,7 @@ class PreprocessorTest(tf.test.TestCase):
     resized_tensor_dict = preprocessor.preprocess(tensor_dict,
                                                   preprocessing_options)
     resized_images = resized_tensor_dict[fields.InputDataFields.image]
-    resized_images_shape = tf.shape(resized_images)
+    resized_images_shape = tf.shape(input=resized_images)
     expected_images_shape = tf.constant([1, 75, 150, 3], dtype=tf.int32)
 
     with self.test_session() as sess:
@@ -2218,12 +2218,12 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.random_uniform(in_image_shape)
-      in_masks = tf.random_uniform(in_masks_shape)
+      in_image = tf.random.uniform(in_image_shape)
+      in_masks = tf.random.uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_image(
           in_image, in_masks, new_height=height, new_width=width)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
+      out_image_shape = tf.shape(input=out_image)
+      out_masks_shape = tf.shape(input=out_masks)
 
       with self.test_session() as sess:
         out_image_shape, out_masks_shape = sess.run(
@@ -2245,12 +2245,12 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.random_uniform(in_image_shape)
-      in_masks = tf.random_uniform(in_masks_shape)
+      in_image = tf.random.uniform(in_image_shape)
+      in_masks = tf.random.uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_image(
           in_image, in_masks, new_height=height, new_width=width)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
+      out_image_shape = tf.shape(input=out_image)
+      out_masks_shape = tf.shape(input=out_masks)
 
       with self.test_session() as sess:
         out_image_shape, out_masks_shape = sess.run(
@@ -2272,12 +2272,12 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.random_uniform(in_image_shape)
-      in_masks = tf.random_uniform(in_masks_shape)
+      in_image = tf.random.uniform(in_image_shape)
+      in_masks = tf.random.uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_image(
           in_image, in_masks, new_height=height, new_width=width)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
+      out_image_shape = tf.shape(input=out_image)
+      out_masks_shape = tf.shape(input=out_masks)
 
       with self.test_session() as sess:
         out_image_shape, out_masks_shape = sess.run(
@@ -2293,7 +2293,7 @@ class PreprocessorTest(tf.test.TestCase):
     expected_shape_list = [[75, 50, 3], [50, 100, 3], [30, 100, 3]]
 
     for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
-      in_image = tf.random_uniform(in_shape)
+      in_image = tf.random.uniform(in_shape)
       out_image, _ = preprocessor.resize_to_range(
           in_image, min_dimension=min_dim, max_dimension=max_dim)
       self.assertAllEqual(out_image.get_shape().as_list(), expected_shape)
@@ -2306,10 +2306,10 @@ class PreprocessorTest(tf.test.TestCase):
     expected_shape_list = [[75, 50, 3], [50, 100, 3], [30, 100, 3]]
 
     for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
+      in_image = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3))
       out_image, _ = preprocessor.resize_to_range(
           in_image, min_dimension=min_dim, max_dimension=max_dim)
-      out_image_shape = tf.shape(out_image)
+      out_image_shape = tf.shape(input=out_image)
       with self.test_session() as sess:
         out_image_shape = sess.run(out_image_shape,
                                    feed_dict={in_image:
@@ -2323,14 +2323,14 @@ class PreprocessorTest(tf.test.TestCase):
     expected_shape_list = [[100, 100, 3], [100, 100, 3], [100, 100, 3]]
 
     for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
+      in_image = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3))
       out_image, _ = preprocessor.resize_to_range(
           in_image,
           min_dimension=min_dim,
           max_dimension=max_dim,
           pad_to_max_dimension=True)
       self.assertAllEqual(out_image.shape.as_list(), expected_shape)
-      out_image_shape = tf.shape(out_image)
+      out_image_shape = tf.shape(input=out_image)
       with self.test_session() as sess:
         out_image_shape = sess.run(
             out_image_shape, feed_dict={in_image: np.random.randn(*in_shape)})
@@ -2344,7 +2344,7 @@ class PreprocessorTest(tf.test.TestCase):
     min_dim = 1
     max_dim = 2
 
-    in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
+    in_image = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3))
     out_image, _ = preprocessor.resize_to_range(
         in_image,
         min_dimension=min_dim,
@@ -2370,8 +2370,8 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.random_uniform(in_image_shape)
-      in_masks = tf.random_uniform(in_masks_shape)
+      in_image = tf.random.uniform(in_image_shape)
+      in_masks = tf.random.uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_to_range(
           in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
       self.assertAllEqual(out_masks.get_shape().as_list(), expected_mask_shape)
@@ -2390,16 +2390,16 @@ class PreprocessorTest(tf.test.TestCase):
          expected_image_shape, in_masks_shape, expected_mask_shape) in zip(
              in_image_shape_list, expected_image_shape_list,
              in_masks_shape_list, expected_masks_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
+      in_image = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3))
+      in_masks = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None))
       out_image, out_masks, _ = preprocessor.resize_to_range(
           in_image,
           in_masks,
           min_dimension=min_dim,
           max_dimension=max_dim,
           pad_to_max_dimension=True)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
+      out_image_shape = tf.shape(input=out_image)
+      out_masks_shape = tf.shape(input=out_masks)
 
       with self.test_session() as sess:
         out_image_shape, out_masks_shape = sess.run(
@@ -2425,13 +2425,13 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
-      in_masks = tf.random_uniform(in_masks_shape)
+      in_image = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3))
+      in_masks = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None))
+      in_masks = tf.random.uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_to_range(
           in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
+      out_image_shape = tf.shape(input=out_image)
+      out_masks_shape = tf.shape(input=out_masks)
 
       with self.test_session() as sess:
         out_image_shape, out_masks_shape = sess.run(
@@ -2457,12 +2457,12 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.random_uniform(in_image_shape)
-      in_masks = tf.random_uniform(in_masks_shape)
+      in_image = tf.random.uniform(in_image_shape)
+      in_masks = tf.random.uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_to_range(
           in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
+      out_image_shape = tf.shape(input=out_image)
+      out_masks_shape = tf.shape(input=out_masks)
 
       with self.test_session() as sess:
         out_image_shape, out_masks_shape = sess.run(
@@ -2471,7 +2471,7 @@ class PreprocessorTest(tf.test.TestCase):
         self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToRange4DImageTensor(self):
-    image = tf.random_uniform([1, 200, 300, 3])
+    image = tf.random.uniform([1, 200, 300, 3])
     with self.assertRaises(ValueError):
       preprocessor.resize_to_range(image, 500, 600)
 
@@ -2483,10 +2483,10 @@ class PreprocessorTest(tf.test.TestCase):
     expected_shape_list = [[320, 320, 3], [320, 320, 3]]
 
     for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
-      in_image = tf.random_uniform(in_shape)
+      in_image = tf.random.uniform(in_shape)
       out_image, _ = preprocessor.resize_to_range(
           in_image, min_dimension=min_dim, max_dimension=max_dim)
-      out_image_shape = tf.shape(out_image)
+      out_image_shape = tf.shape(input=out_image)
 
       with self.test_session() as sess:
         out_image_shape = sess.run(out_image_shape)
@@ -2504,13 +2504,13 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
-      in_masks = tf.random_uniform(in_masks_shape)
+      in_image = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3))
+      in_masks = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None))
+      in_masks = tf.random.uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_to_min_dimension(
           in_image, in_masks, min_dimension=min_dim)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
+      out_image_shape = tf.shape(input=out_image)
+      out_masks_shape = tf.shape(input=out_masks)
 
       with self.test_session() as sess:
         out_image_shape, out_masks_shape = sess.run(
@@ -2535,12 +2535,12 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.random_uniform(in_image_shape)
-      in_masks = tf.random_uniform(in_masks_shape)
+      in_image = tf.random.uniform(in_image_shape)
+      in_masks = tf.random.uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_to_min_dimension(
           in_image, in_masks, min_dimension=min_dim)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
+      out_image_shape = tf.shape(input=out_image)
+      out_masks_shape = tf.shape(input=out_masks)
 
       with self.test_session() as sess:
         out_image_shape, out_masks_shape = sess.run(
@@ -2549,7 +2549,7 @@ class PreprocessorTest(tf.test.TestCase):
         self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToMinDimensionRaisesErrorOn4DImage(self):
-    image = tf.random_uniform([1, 200, 300, 3])
+    image = tf.random.uniform([1, 200, 300, 3])
     with self.assertRaises(ValueError):
       preprocessor.resize_to_min_dimension(image, 500)
 
@@ -2562,7 +2562,7 @@ class PreprocessorTest(tf.test.TestCase):
     expected_boxes = [[6., 8., 24., 24.],
                       [30., 12., 54., 28.]]
 
-    in_image = tf.random_uniform(in_shape)
+    in_image = tf.random.uniform(in_shape)
     in_boxes = tf.constant(in_boxes)
     _, out_boxes = preprocessor.scale_boxes_to_pixel_coordinates(
         in_image, boxes=in_boxes)
@@ -2583,7 +2583,7 @@ class PreprocessorTest(tf.test.TestCase):
         [[24., 16.], [30., 20.], [36., 24.]],
     ]
 
-    in_image = tf.random_uniform(in_shape)
+    in_image = tf.random.uniform(in_shape)
     _, out_boxes, out_keypoints = preprocessor.scale_boxes_to_pixel_coordinates(
         in_image, boxes=in_boxes, keypoints=in_keypoints)
     with self.test_session() as sess:
diff --git a/research/object_detection/core/region_similarity_calculator.py b/research/object_detection/core/region_similarity_calculator.py
index 793c7d38..a7d52e02 100644
--- a/research/object_detection/core/region_similarity_calculator.py
+++ b/research/object_detection/core/region_similarity_calculator.py
@@ -48,7 +48,7 @@ class RegionSimilarityCalculator(object):
     Returns:
       a (float32) tensor of shape [N, M] with pairwise similarity score.
     """
-    with tf.name_scope(scope, 'Compare', [boxlist1, boxlist2]) as scope:
+    with tf.compat.v1.name_scope(scope, 'Compare', [boxlist1, boxlist2]) as scope:
       return self._compare(boxlist1, boxlist2)
 
   @abstractmethod
@@ -147,8 +147,8 @@ class ThresholdedIouSimilarity(RegionSimilarityCalculator):
     ious = box_list_ops.iou(boxlist1, boxlist2)
     scores = boxlist1.get_field(fields.BoxListFields.scores)
     scores = tf.expand_dims(scores, axis=1)
-    row_replicated_scores = tf.tile(scores, [1, tf.shape(ious)[-1]])
-    thresholded_ious = tf.where(ious > self._iou_threshold,
+    row_replicated_scores = tf.tile(scores, [1, tf.shape(input=ious)[-1]])
+    thresholded_ious = tf.compat.v1.where(ious > self._iou_threshold,
                                 row_replicated_scores, tf.zeros_like(ious))
 
     return thresholded_ious
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index 1e49b1cd..a84c922d 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -252,7 +252,7 @@ class TargetAssigner(object):
     unmatched_ignored_reg_targets = tf.tile(
         self._default_regression_target(), [match_results_shape[0], 1])
     matched_anchors_mask = match.matched_column_indicator()
-    reg_targets = tf.where(matched_anchors_mask,
+    reg_targets = tf.compat.v1.where(matched_anchors_mask,
                            matched_reg_targets,
                            unmatched_ignored_reg_targets)
     return reg_targets
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index 787ef0fc..45a28848 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -142,47 +142,47 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     """
     self.keys_to_features = {
         'image/encoded':
-            tf.FixedLenFeature((), tf.string, default_value=''),
+            tf.io.FixedLenFeature((), tf.string, default_value=''),
         'image/format':
-            tf.FixedLenFeature((), tf.string, default_value='jpeg'),
+            tf.io.FixedLenFeature((), tf.string, default_value='jpeg'),
         'image/filename':
-            tf.FixedLenFeature((), tf.string, default_value=''),
+            tf.io.FixedLenFeature((), tf.string, default_value=''),
         'image/key/sha256':
-            tf.FixedLenFeature((), tf.string, default_value=''),
+            tf.io.FixedLenFeature((), tf.string, default_value=''),
         'image/source_id':
-            tf.FixedLenFeature((), tf.string, default_value=''),
+            tf.io.FixedLenFeature((), tf.string, default_value=''),
         'image/height':
-            tf.FixedLenFeature((), tf.int64, default_value=1),
+            tf.io.FixedLenFeature((), tf.int64, default_value=1),
         'image/width':
-            tf.FixedLenFeature((), tf.int64, default_value=1),
+            tf.io.FixedLenFeature((), tf.int64, default_value=1),
         # Image-level labels.
         'image/class/text':
-            tf.VarLenFeature(tf.string),
+            tf.io.VarLenFeature(tf.string),
         'image/class/label':
-            tf.VarLenFeature(tf.int64),
+            tf.io.VarLenFeature(tf.int64),
         # Object boxes and classes.
         'image/object/bbox/xmin':
-            tf.VarLenFeature(tf.float32),
+            tf.io.VarLenFeature(tf.float32),
         'image/object/bbox/xmax':
-            tf.VarLenFeature(tf.float32),
+            tf.io.VarLenFeature(tf.float32),
         'image/object/bbox/ymin':
-            tf.VarLenFeature(tf.float32),
+            tf.io.VarLenFeature(tf.float32),
         'image/object/bbox/ymax':
-            tf.VarLenFeature(tf.float32),
+            tf.io.VarLenFeature(tf.float32),
         'image/object/class/label':
-            tf.VarLenFeature(tf.int64),
+            tf.io.VarLenFeature(tf.int64),
         'image/object/class/text':
-            tf.VarLenFeature(tf.string),
+            tf.io.VarLenFeature(tf.string),
         'image/object/area':
-            tf.VarLenFeature(tf.float32),
+            tf.io.VarLenFeature(tf.float32),
         'image/object/is_crowd':
-            tf.VarLenFeature(tf.int64),
+            tf.io.VarLenFeature(tf.int64),
         'image/object/difficult':
-            tf.VarLenFeature(tf.int64),
+            tf.io.VarLenFeature(tf.int64),
         'image/object/group_of':
-            tf.VarLenFeature(tf.int64),
+            tf.io.VarLenFeature(tf.int64),
         'image/object/weight':
-            tf.VarLenFeature(tf.float32),
+            tf.io.VarLenFeature(tf.float32),
     }
     # We are checking `dct_method` instead of passing it directly in order to
     # ensure TF version 1.6 compatibility.
@@ -232,7 +232,7 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     }
     if num_additional_channels > 0:
       self.keys_to_features[
-          'image/additional_channels/encoded'] = tf.FixedLenFeature(
+          'image/additional_channels/encoded'] = tf.io.FixedLenFeature(
               (num_additional_channels,), tf.string)
       self.items_to_handlers[
           fields.InputDataFields.
@@ -240,9 +240,9 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     self._num_keypoints = num_keypoints
     if num_keypoints > 0:
       self.keys_to_features['image/object/keypoint/x'] = (
-          tf.VarLenFeature(tf.float32))
+          tf.io.VarLenFeature(tf.float32))
       self.keys_to_features['image/object/keypoint/y'] = (
-          tf.VarLenFeature(tf.float32))
+          tf.io.VarLenFeature(tf.float32))
       self.items_to_handlers[fields.InputDataFields.groundtruth_keypoints] = (
           slim_example_decoder.ItemHandlerCallback(
               ['image/object/keypoint/y', 'image/object/keypoint/x'],
@@ -251,14 +251,14 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       if instance_mask_type in (input_reader_pb2.DEFAULT,
                                 input_reader_pb2.NUMERICAL_MASKS):
         self.keys_to_features['image/object/mask'] = (
-            tf.VarLenFeature(tf.float32))
+            tf.io.VarLenFeature(tf.float32))
         self.items_to_handlers[
             fields.InputDataFields.groundtruth_instance_masks] = (
                 slim_example_decoder.ItemHandlerCallback(
                     ['image/object/mask', 'image/height', 'image/width'],
                     self._reshape_instance_masks))
       elif instance_mask_type == input_reader_pb2.PNG_MASKS:
-        self.keys_to_features['image/object/mask'] = tf.VarLenFeature(tf.string)
+        self.keys_to_features['image/object/mask'] = tf.io.VarLenFeature(tf.string)
         self.items_to_handlers[
             fields.InputDataFields.groundtruth_instance_masks] = (
                 slim_example_decoder.ItemHandlerCallback(
@@ -353,25 +353,25 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     tensor_dict[is_crowd] = tf.cast(tensor_dict[is_crowd], dtype=tf.bool)
     tensor_dict[fields.InputDataFields.image].set_shape([None, None, 3])
     tensor_dict[fields.InputDataFields.num_groundtruth_boxes] = tf.shape(
-        tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]
+        input=tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]
 
     if fields.InputDataFields.image_additional_channels in tensor_dict:
       channels = tensor_dict[fields.InputDataFields.image_additional_channels]
       channels = tf.squeeze(channels, axis=3)
-      channels = tf.transpose(channels, perm=[1, 2, 0])
+      channels = tf.transpose(a=channels, perm=[1, 2, 0])
       tensor_dict[fields.InputDataFields.image_additional_channels] = channels
 
     def default_groundtruth_weights():
       return tf.ones(
-          [tf.shape(tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]],
+          [tf.shape(input=tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]],
           dtype=tf.float32)
 
     tensor_dict[fields.InputDataFields.groundtruth_weights] = tf.cond(
-        tf.greater(
+        pred=tf.greater(
             tf.shape(
-                tensor_dict[fields.InputDataFields.groundtruth_weights])[0],
-            0), lambda: tensor_dict[fields.InputDataFields.groundtruth_weights],
-        default_groundtruth_weights)
+                input=tensor_dict[fields.InputDataFields.groundtruth_weights])[0],
+            0), true_fn=lambda: tensor_dict[fields.InputDataFields.groundtruth_weights],
+        false_fn=default_groundtruth_weights)
     return tensor_dict
 
   def _reshape_keypoints(self, keys_to_tensors):
@@ -389,11 +389,11 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     """
     y = keys_to_tensors['image/object/keypoint/y']
     if isinstance(y, tf.SparseTensor):
-      y = tf.sparse_tensor_to_dense(y)
+      y = tf.sparse.to_dense(y)
     y = tf.expand_dims(y, 1)
     x = keys_to_tensors['image/object/keypoint/x']
     if isinstance(x, tf.SparseTensor):
-      x = tf.sparse_tensor_to_dense(x)
+      x = tf.sparse.to_dense(x)
     x = tf.expand_dims(x, 1)
     keypoints = tf.concat([y, x], 1)
     keypoints = tf.reshape(keypoints, [-1, self._num_keypoints, 2])
@@ -417,8 +417,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     to_shape = tf.cast(tf.stack([-1, height, width]), tf.int32)
     masks = keys_to_tensors['image/object/mask']
     if isinstance(masks, tf.SparseTensor):
-      masks = tf.sparse_tensor_to_dense(masks)
-    masks = tf.reshape(tf.to_float(tf.greater(masks, 0.0)), to_shape)
+      masks = tf.sparse.to_dense(masks)
+    masks = tf.reshape(tf.cast(tf.greater(masks, 0.0), dtype=tf.float32), to_shape)
     return tf.cast(masks, tf.float32)
 
   def _decode_png_instance_masks(self, keys_to_tensors):
@@ -439,15 +439,15 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       image = tf.squeeze(
           tf.image.decode_image(image_buffer, channels=1), axis=2)
       image.set_shape([None, None])
-      image = tf.to_float(tf.greater(image, 0))
+      image = tf.cast(tf.greater(image, 0), dtype=tf.float32)
       return image
 
     png_masks = keys_to_tensors['image/object/mask']
     height = keys_to_tensors['image/height']
     width = keys_to_tensors['image/width']
     if isinstance(png_masks, tf.SparseTensor):
-      png_masks = tf.sparse_tensor_to_dense(png_masks, default_value='')
+      png_masks = tf.sparse.to_dense(png_masks, default_value='')
     return tf.cond(
-        tf.greater(tf.size(png_masks), 0),
-        lambda: tf.map_fn(decode_png_mask, png_masks, dtype=tf.float32),
-        lambda: tf.zeros(tf.to_int32(tf.stack([0, height, width]))))
+        pred=tf.greater(tf.size(input=png_masks), 0),
+        true_fn=lambda: tf.map_fn(decode_png_mask, png_masks, dtype=tf.float32),
+        false_fn=lambda: tf.zeros(tf.cast(tf.stack([0, height, width]), dtype=tf.int32)))
diff --git a/research/object_detection/data_decoders/tf_example_decoder_test.py b/research/object_detection/data_decoders/tf_example_decoder_test.py
index 5d38fa9e..74d12761 100644
--- a/research/object_detection/data_decoders/tf_example_decoder_test.py
+++ b/research/object_detection/data_decoders/tf_example_decoder_test.py
@@ -100,7 +100,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
     example_decoder = tf_example_decoder.TfExampleDecoder(
         num_additional_channels=2)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
@@ -202,7 +202,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[fields.InputDataFields.image].
                          get_shape().as_list()), [None, None, 3])
@@ -222,7 +222,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
@@ -241,7 +241,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[fields.InputDataFields.image].
                          get_shape().as_list()), [None, None, 3])
@@ -272,7 +272,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
     example_decoder = tf_example_decoder.TfExampleDecoder(
         load_instance_masks=True, instance_mask_type=input_reader_pb2.PNG_MASKS)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
@@ -297,7 +297,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
     example_decoder = tf_example_decoder.TfExampleDecoder(
         load_instance_masks=True, instance_mask_type=input_reader_pb2.PNG_MASKS)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
@@ -322,7 +322,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes].
                          get_shape().as_list()), [None, 4])
@@ -358,7 +358,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder(num_keypoints=3)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes].
                          get_shape().as_list()), [None, 4])
@@ -398,7 +398,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes].
                          get_shape().as_list()), [None, 4])
@@ -421,7 +421,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
@@ -453,18 +453,18 @@ class TfExampleDecoderTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
 
     example_decoder = tf_example_decoder.TfExampleDecoder(
         label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
                         [None])
 
-    init = tf.tables_initializer()
+    init = tf.compat.v1.tables_initializer()
     with self.test_session() as sess:
       sess.run(init)
       tensor_dict = sess.run(tensor_dict)
@@ -498,17 +498,17 @@ class TfExampleDecoderTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
     example_decoder = tf_example_decoder.TfExampleDecoder(
         label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
                          .get_shape().as_list()), [None])
 
     with self.test_session() as sess:
-      sess.run(tf.tables_initializer())
+      sess.run(tf.compat.v1.tables_initializer())
       tensor_dict = sess.run(tensor_dict)
 
     self.assertAllEqual([2, -1],
@@ -540,17 +540,17 @@ class TfExampleDecoderTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
     example_decoder = tf_example_decoder.TfExampleDecoder(
         label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
                          .get_shape().as_list()), [None])
 
     with self.test_session() as sess:
-      sess.run(tf.tables_initializer())
+      sess.run(tf.compat.v1.tables_initializer())
       tensor_dict = sess.run(tensor_dict)
 
     self.assertAllEqual([3, 1],
@@ -568,7 +568,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_area].
                          get_shape().as_list()), [2])
@@ -590,7 +590,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_is_crowd].get_shape().as_list()),
@@ -614,7 +614,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_difficult].get_shape().as_list()),
@@ -639,7 +639,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
         })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_group_of].get_shape().as_list()),
@@ -663,7 +663,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
         })).SerializeToString()
 
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_weights].get_shape().as_list()),
@@ -708,7 +708,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
             object_classes)})).SerializeToString()
     example_decoder = tf_example_decoder.TfExampleDecoder(
         load_instance_masks=True)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
 
     self.assertAllEqual((
         tensor_dict[fields.InputDataFields.groundtruth_instance_masks].
@@ -758,7 +758,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
         'image/object/class/label': self._Int64Feature(
             object_classes)})).SerializeToString()
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
     self.assertTrue(fields.InputDataFields.groundtruth_instance_masks
                     not in tensor_dict)
 
@@ -773,7 +773,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
                 'image/class/label': self._Int64Feature([1, 2]),
             })).SerializeToString()
     example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
     self.assertTrue(
@@ -799,13 +799,13 @@ class TfExampleDecoderTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
     example_decoder = tf_example_decoder.TfExampleDecoder(
         label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(value=example))
     with self.test_session() as sess:
-      sess.run(tf.tables_initializer())
+      sess.run(tf.compat.v1.tables_initializer())
       tensor_dict = sess.run(tensor_dict)
     self.assertTrue(
         fields.InputDataFields.groundtruth_image_classes in tensor_dict)
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record.py b/research/object_detection/dataset_tools/create_coco_tf_record.py
index 37033e0a..4d2c8739 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record.py
@@ -67,7 +67,7 @@ tf.flags.DEFINE_string('output_dir', '/tmp/', 'Output data directory.')
 
 FLAGS = flags.FLAGS
 
-tf.logging.set_verbosity(tf.logging.INFO)
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
 
 def create_tf_example(image,
@@ -110,7 +110,7 @@ def create_tf_example(image,
   image_id = image['id']
 
   full_path = os.path.join(image_dir, filename)
-  with tf.gfile.GFile(full_path, 'rb') as fid:
+  with tf.io.gfile.GFile(full_path, 'rb') as fid:
     encoded_jpg = fid.read()
   encoded_jpg_io = io.BytesIO(encoded_jpg)
   image = PIL.Image.open(encoded_jpg_io)
@@ -204,7 +204,7 @@ def _create_tf_record_from_coco_annotations(
     num_shards: number of output file shards.
   """
   with contextlib2.ExitStack() as tf_record_close_stack, \
-      tf.gfile.GFile(annotations_file, 'r') as fid:
+      tf.io.gfile.GFile(annotations_file, 'r') as fid:
     output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
         tf_record_close_stack, output_path, num_shards)
     groundtruth_data = json.load(fid)
@@ -214,7 +214,7 @@ def _create_tf_record_from_coco_annotations(
 
     annotations_index = {}
     if 'annotations' in groundtruth_data:
-      tf.logging.info(
+      tf.compat.v1.logging.info(
           'Found groundtruth annotations. Building annotations index.')
       for annotation in groundtruth_data['annotations']:
         image_id = annotation['image_id']
@@ -227,20 +227,20 @@ def _create_tf_record_from_coco_annotations(
       if image_id not in annotations_index:
         missing_annotation_count += 1
         annotations_index[image_id] = []
-    tf.logging.info('%d images are missing annotations.',
+    tf.compat.v1.logging.info('%d images are missing annotations.',
                     missing_annotation_count)
 
     total_num_annotations_skipped = 0
     for idx, image in enumerate(images):
       if idx % 100 == 0:
-        tf.logging.info('On image %d of %d', idx, len(images))
+        tf.compat.v1.logging.info('On image %d of %d', idx, len(images))
       annotations_list = annotations_index[image['id']]
       _, tf_example, num_annotations_skipped = create_tf_example(
           image, annotations_list, image_dir, category_index, include_masks)
       total_num_annotations_skipped += num_annotations_skipped
       shard_idx = idx % num_shards
       output_tfrecords[shard_idx].write(tf_example.SerializeToString())
-    tf.logging.info('Finished writing, skipped %d annotations.',
+    tf.compat.v1.logging.info('Finished writing, skipped %d annotations.',
                     total_num_annotations_skipped)
 
 
@@ -252,8 +252,8 @@ def main(_):
   assert FLAGS.val_annotations_file, '`val_annotations_file` missing.'
   assert FLAGS.testdev_annotations_file, '`testdev_annotations_file` missing.'
 
-  if not tf.gfile.IsDirectory(FLAGS.output_dir):
-    tf.gfile.MakeDirs(FLAGS.output_dir)
+  if not tf.io.gfile.isdir(FLAGS.output_dir):
+    tf.io.gfile.makedirs(FLAGS.output_dir)
   train_output_path = os.path.join(FLAGS.output_dir, 'coco_train.record')
   val_output_path = os.path.join(FLAGS.output_dir, 'coco_val.record')
   testdev_output_path = os.path.join(FLAGS.output_dir, 'coco_testdev.record')
@@ -279,4 +279,4 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/dataset_tools/create_kitti_tf_record.py b/research/object_detection/dataset_tools/create_kitti_tf_record.py
index c612db99..364f5ae0 100644
--- a/research/object_detection/dataset_tools/create_kitti_tf_record.py
+++ b/research/object_detection/dataset_tools/create_kitti_tf_record.py
@@ -104,12 +104,12 @@ def convert_kitti_to_tfrecords(data_dir, output_path, classes_to_use,
                            'training',
                            'image_2')
 
-  train_writer = tf.python_io.TFRecordWriter('%s_train.tfrecord'%
+  train_writer = tf.io.TFRecordWriter('%s_train.tfrecord'%
                                              output_path)
-  val_writer = tf.python_io.TFRecordWriter('%s_val.tfrecord'%
+  val_writer = tf.io.TFRecordWriter('%s_val.tfrecord'%
                                            output_path)
 
-  images = sorted(tf.gfile.ListDirectory(image_dir))
+  images = sorted(tf.io.gfile.listdir(image_dir))
   for img_name in images:
     img_num = int(img_name.split('.')[0])
     is_validation_img = img_num < validation_set_size
@@ -147,7 +147,7 @@ def prepare_example(image_path, annotations, label_map_dict):
   Returns:
     example: The converted tf.Example.
   """
-  with tf.gfile.GFile(image_path, 'rb') as fid:
+  with tf.io.gfile.GFile(image_path, 'rb') as fid:
     encoded_png = fid.read()
   encoded_png_io = io.BytesIO(encoded_png)
   image = pil.open(encoded_png_io)
@@ -307,4 +307,4 @@ def main(_):
       validation_set_size=FLAGS.validation_set_size)
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/dataset_tools/create_oid_tf_record.py b/research/object_detection/dataset_tools/create_oid_tf_record.py
index 26d9699c..37851828 100644
--- a/research/object_detection/dataset_tools/create_oid_tf_record.py
+++ b/research/object_detection/dataset_tools/create_oid_tf_record.py
@@ -64,7 +64,7 @@ FLAGS = tf.flags.FLAGS
 
 
 def main(_):
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
   required_flags = [
       'input_box_annotations_csv', 'input_images_directory', 'input_label_map',
@@ -82,14 +82,14 @@ def main(_):
         columns={'Confidence': 'ConfidenceImageLabel'}, inplace=True)
   else:
     all_label_annotations = None
-  all_images = tf.gfile.Glob(
+  all_images = tf.io.gfile.glob(
       os.path.join(FLAGS.input_images_directory, '*.jpg'))
   all_image_ids = [os.path.splitext(os.path.basename(v))[0] for v in all_images]
   all_image_ids = pd.DataFrame({'ImageID': all_image_ids})
   all_annotations = pd.concat(
       [all_box_annotations, all_image_ids, all_label_annotations])
 
-  tf.logging.log(tf.logging.INFO, 'Found %d images...', len(all_image_ids))
+  tf.compat.v1.logging.log(tf.compat.v1.logging.INFO, 'Found %d images...', len(all_image_ids))
 
   with contextlib2.ExitStack() as tf_record_close_stack:
     output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
@@ -97,13 +97,13 @@ def main(_):
         FLAGS.num_shards)
 
     for counter, image_data in enumerate(all_annotations.groupby('ImageID')):
-      tf.logging.log_every_n(tf.logging.INFO, 'Processed %d images...', 1000,
+      tf.compat.v1.logging.log_every_n(tf.compat.v1.logging.INFO, 'Processed %d images...', 1000,
                              counter)
 
       image_id, image_annotations = image_data
       # In OID image file names are formed by appending ".jpg" to the image ID.
       image_path = os.path.join(FLAGS.input_images_directory, image_id + '.jpg')
-      with tf.gfile.Open(image_path) as image_file:
+      with tf.io.gfile.GFile(image_path) as image_file:
         encoded_image = image_file.read()
 
       tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(
@@ -114,4 +114,4 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/dataset_tools/create_pascal_tf_record.py b/research/object_detection/dataset_tools/create_pascal_tf_record.py
index 813071c9..b6b1a7f7 100644
--- a/research/object_detection/dataset_tools/create_pascal_tf_record.py
+++ b/research/object_detection/dataset_tools/create_pascal_tf_record.py
@@ -84,7 +84,7 @@ def dict_to_tf_example(data,
   """
   img_path = os.path.join(data['folder'], image_subdirectory, data['filename'])
   full_path = os.path.join(dataset_directory, img_path)
-  with tf.gfile.GFile(full_path, 'rb') as fid:
+  with tf.io.gfile.GFile(full_path, 'rb') as fid:
     encoded_jpg = fid.read()
   encoded_jpg_io = io.BytesIO(encoded_jpg)
   image = PIL.Image.open(encoded_jpg_io)
@@ -155,7 +155,7 @@ def main(_):
   if FLAGS.year != 'merged':
     years = [FLAGS.year]
 
-  writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
+  writer = tf.io.TFRecordWriter(FLAGS.output_path)
 
   label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)
 
@@ -169,7 +169,7 @@ def main(_):
       if idx % 100 == 0:
         logging.info('On image %d of %d', idx, len(examples_list))
       path = os.path.join(annotations_dir, example + '.xml')
-      with tf.gfile.GFile(path, 'r') as fid:
+      with tf.io.gfile.GFile(path, 'r') as fid:
         xml_str = fid.read()
       xml = etree.fromstring(xml_str)
       data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']
@@ -182,4 +182,4 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/dataset_tools/create_pet_tf_record.py b/research/object_detection/dataset_tools/create_pet_tf_record.py
index 9b3b55c6..82fbe1cc 100644
--- a/research/object_detection/dataset_tools/create_pet_tf_record.py
+++ b/research/object_detection/dataset_tools/create_pet_tf_record.py
@@ -106,7 +106,7 @@ def dict_to_tf_example(data,
     ValueError: if the image pointed to by data['filename'] is not a valid JPEG
   """
   img_path = os.path.join(image_subdirectory, data['filename'])
-  with tf.gfile.GFile(img_path, 'rb') as fid:
+  with tf.io.gfile.GFile(img_path, 'rb') as fid:
     encoded_jpg = fid.read()
   encoded_jpg_io = io.BytesIO(encoded_jpg)
   image = PIL.Image.open(encoded_jpg_io)
@@ -114,7 +114,7 @@ def dict_to_tf_example(data,
     raise ValueError('Image format not JPEG')
   key = hashlib.sha256(encoded_jpg).hexdigest()
 
-  with tf.gfile.GFile(mask_path, 'rb') as fid:
+  with tf.io.gfile.GFile(mask_path, 'rb') as fid:
     encoded_mask_png = fid.read()
   encoded_png_io = io.BytesIO(encoded_mask_png)
   mask = PIL.Image.open(encoded_png_io)
@@ -245,7 +245,7 @@ def create_tf_record(output_filename,
       if not os.path.exists(xml_path):
         logging.warning('Could not find %s, ignoring example.', xml_path)
         continue
-      with tf.gfile.GFile(xml_path, 'r') as fid:
+      with tf.io.gfile.GFile(xml_path, 'r') as fid:
         xml_str = fid.read()
       xml = etree.fromstring(xml_str)
       data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']
@@ -315,4 +315,4 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
index 64f2550b..e69de29b 100644
--- a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
+++ b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
@@ -1,199 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-r"""An executable to expand hierarchically image-level labels and boxes.
-
-Example usage:
-python models/research/object_detection/dataset_tools/\
-oid_hierarchical_labels_expansion.py \
---json_hierarchy_file=<path to JSON hierarchy> \
---input_annotations=<input csv file> \
---output_annotations=<output csv file> \
---annotation_type=<1 (for boxes) or 2 (for image-level labels)>
-"""
-
-import argparse
-import json
-
-
-def _update_dict(initial_dict, update):
-  """Updates dictionary with update content.
-
-  Args:
-   initial_dict: initial dictionary.
-   update: updated dictionary.
-  """
-
-  for key, value_list in update.iteritems():
-    if key in initial_dict:
-      initial_dict[key].extend(value_list)
-    else:
-      initial_dict[key] = value_list
-
-
-def _build_plain_hierarchy(hierarchy, skip_root=False):
-  """Expands tree hierarchy representation to parent-child dictionary.
-
-  Args:
-   hierarchy: labels hierarchy as JSON file.
-   skip_root: if true skips root from the processing (done for the case when all
-     classes under hierarchy are collected under virtual node).
-
-  Returns:
-    keyed_parent - dictionary of parent - all its children nodes.
-    keyed_child  - dictionary of children - all its parent nodes
-    children - all children of the current node.
-  """
-  all_children = []
-  all_keyed_parent = {}
-  all_keyed_child = {}
-  if 'Subcategory' in hierarchy:
-    for node in hierarchy['Subcategory']:
-      keyed_parent, keyed_child, children = _build_plain_hierarchy(node)
-      # Update is not done through dict.update() since some children have multi-
-      # ple parents in the hiearchy.
-      _update_dict(all_keyed_parent, keyed_parent)
-      _update_dict(all_keyed_child, keyed_child)
-      all_children.extend(children)
-
-  if not skip_root:
-    all_keyed_parent[hierarchy['LabelName']] = all_children
-    all_children = [hierarchy['LabelName']] + all_children
-    for child, _ in all_keyed_child.iteritems():
-      all_keyed_child[child].append(hierarchy['LabelName'])
-    all_keyed_child[hierarchy['LabelName']] = []
-
-  return all_keyed_parent, all_keyed_child, all_children
-
-
-class OIDHierarchicalLabelsExpansion(object):
-  """ Main class to perform labels hierachical expansion."""
-
-  def __init__(self, hierarchy):
-    """Constructor.
-
-    Args:
-      hierarchy: labels hierarchy as JSON object.
-    """
-
-    self._hierarchy_keyed_parent, self._hierarchy_keyed_child, _ = (
-        _build_plain_hierarchy(hierarchy, skip_root=True))
-
-  def expand_boxes_from_csv(self, csv_row):
-    """Expands a row containing bounding boxes from CSV file.
-
-    Args:
-      csv_row: a single row of Open Images released groundtruth file.
-
-    Returns:
-      a list of strings (including the initial row) corresponding to the ground
-      truth expanded to multiple annotation for evaluation with Open Images
-      Challenge 2018 metric.
-    """
-    # Row header is expected to be exactly:
-    # ImageID,Source,LabelName,Confidence,XMin,XMax,YMin,YMax,IsOccluded,
-    # IsTruncated,IsGroupOf,IsDepiction,IsInside
-    cvs_row_splitted = csv_row.split(',')
-    assert len(cvs_row_splitted) == 13
-    result = [csv_row]
-    assert cvs_row_splitted[2] in self._hierarchy_keyed_child
-    parent_nodes = self._hierarchy_keyed_child[cvs_row_splitted[2]]
-    for parent_node in parent_nodes:
-      cvs_row_splitted[2] = parent_node
-      result.append(','.join(cvs_row_splitted))
-    return result
-
-  def expand_labels_from_csv(self, csv_row):
-    """Expands a row containing bounding boxes from CSV file.
-
-    Args:
-      csv_row: a single row of Open Images released groundtruth file.
-
-    Returns:
-      a list of strings (including the initial row) corresponding to the ground
-      truth expanded to multiple annotation for evaluation with Open Images
-      Challenge 2018 metric.
-    """
-    # Row header is expected to be exactly:
-    # ImageID,Source,LabelName,Confidence
-    cvs_row_splited = csv_row.split(',')
-    assert len(cvs_row_splited) == 4
-    result = [csv_row]
-    if int(cvs_row_splited[3]) == 1:
-      assert cvs_row_splited[2] in self._hierarchy_keyed_child
-      parent_nodes = self._hierarchy_keyed_child[cvs_row_splited[2]]
-      for parent_node in parent_nodes:
-        cvs_row_splited[2] = parent_node
-        result.append(','.join(cvs_row_splited))
-    else:
-      assert cvs_row_splited[2] in self._hierarchy_keyed_parent
-      child_nodes = self._hierarchy_keyed_parent[cvs_row_splited[2]]
-      for child_node in child_nodes:
-        cvs_row_splited[2] = child_node
-        result.append(','.join(cvs_row_splited))
-    return result
-
-
-def main(parsed_args):
-
-  with open(parsed_args.json_hierarchy_file) as f:
-    hierarchy = json.load(f)
-  expansion_generator = OIDHierarchicalLabelsExpansion(hierarchy)
-  labels_file = False
-  if parsed_args.annotation_type == 2:
-    labels_file = True
-  elif parsed_args.annotation_type != 1:
-    print '--annotation_type expected value is 1 or 2.'
-    return -1
-  with open(parsed_args.input_annotations, 'r') as source:
-    with open(parsed_args.output_annotations, 'w') as target:
-      header = None
-      for line in source:
-        if not header:
-          header = line
-          target.writelines(header)
-          continue
-        if labels_file:
-          expanded_lines = expansion_generator.expand_labels_from_csv(line)
-        else:
-          expanded_lines = expansion_generator.expand_boxes_from_csv(line)
-        target.writelines(expanded_lines)
-
-
-if __name__ == '__main__':
-
-  parser = argparse.ArgumentParser(
-      description='Hierarchically expand annotations (excluding root node).')
-  parser.add_argument(
-      '--json_hierarchy_file',
-      required=True,
-      help='Path to the file containing label hierarchy in JSON format.')
-  parser.add_argument(
-      '--input_annotations',
-      required=True,
-      help="""Path to Open Images annotations file (either bounding boxes or
-      image-level labels).""")
-  parser.add_argument(
-      '--output_annotations',
-      required=True,
-      help="""Path to the output file.""")
-  parser.add_argument(
-      '--annotation_type',
-      type=int,
-      required=True,
-      help="""Type of the input annotations: 1 - boxes, 2 - image-level
-      labels"""
-  )
-  args = parser.parse_args()
-  main(args)
diff --git a/research/object_detection/dataset_tools/tf_record_creation_util.py b/research/object_detection/dataset_tools/tf_record_creation_util.py
index e8da2291..9a4e5152 100644
--- a/research/object_detection/dataset_tools/tf_record_creation_util.py
+++ b/research/object_detection/dataset_tools/tf_record_creation_util.py
@@ -39,7 +39,7 @@ def open_sharded_output_tfrecords(exit_stack, base_path, num_shards):
   ]
 
   tfrecords = [
-      exit_stack.enter_context(tf.python_io.TFRecordWriter(file_name))
+      exit_stack.enter_context(tf.io.TFRecordWriter(file_name))
       for file_name in tf_record_output_filenames
   ]
 
diff --git a/research/object_detection/dataset_tools/tf_record_creation_util_test.py b/research/object_detection/dataset_tools/tf_record_creation_util_test.py
index f1231f8b..537f2408 100644
--- a/research/object_detection/dataset_tools/tf_record_creation_util_test.py
+++ b/research/object_detection/dataset_tools/tf_record_creation_util_test.py
@@ -27,14 +27,14 @@ class OpenOutputTfrecordsTests(tf.test.TestCase):
     with contextlib2.ExitStack() as tf_record_close_stack:
       output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
           tf_record_close_stack,
-          os.path.join(tf.test.get_temp_dir(), 'test.tfrec'), 10)
+          os.path.join(tf.compat.v1.test.get_temp_dir(), 'test.tfrec'), 10)
       for idx in range(10):
         output_tfrecords[idx].write('test_{}'.format(idx))
 
     for idx in range(10):
       tf_record_path = '{}-{:05d}-of-00010'.format(
-          os.path.join(tf.test.get_temp_dir(), 'test.tfrec'), idx)
-      records = list(tf.python_io.tf_record_iterator(tf_record_path))
+          os.path.join(tf.compat.v1.test.get_temp_dir(), 'test.tfrec'), idx)
+      records = list(tf.compat.v1.python_io.tf_record_iterator(tf_record_path))
       self.assertAllEqual(records, ['test_{}'.format(idx)])
 
 
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index 2664fc66..fca89259 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -30,7 +30,7 @@ from object_detection.utils import label_map_util
 from object_detection.utils import ops
 from object_detection.utils import visualization_utils as vis_utils
 
-slim = tf.contrib.slim
+# slim = tf.contrib.slim
 
 # A dictionary of metric names to classes that implement the metric. The classes
 # in the dictionary must implement
@@ -54,10 +54,10 @@ def write_metrics(metrics, global_step, summary_dir):
     summary_dir: Directory to write tensorflow summaries to.
   """
   logging.info('Writing metrics to tf summary.')
-  summary_writer = tf.summary.FileWriterCache.get(summary_dir)
+  summary_writer = tf.compat.v1.summary.FileWriterCache.get(summary_dir)
   for key in sorted(metrics):
-    summary = tf.Summary(value=[
-        tf.Summary.Value(tag=key, simple_value=metrics[key]),
+    summary = tf.compat.v1.Summary(value=[
+        tf.compat.v1.Summary.Value(tag=key, simple_value=metrics[key]),
     ])
     summary_writer.add_summary(summary, global_step)
     logging.info('%s: %f', key, metrics[key])
@@ -195,14 +195,14 @@ def visualize_detection_results(result_dict,
       export_path = os.path.join(export_dir, 'export-{}.png'.format(tag))
     vis_utils.save_image_array_as_png(image, export_path)
 
-  summary = tf.Summary(value=[
-      tf.Summary.Value(
+  summary = tf.compat.v1.Summary(value=[
+      tf.compat.v1.Summary.Value(
           tag=tag,
-          image=tf.Summary.Image(
+          image=tf.compat.v1.Summary.Image(
               encoded_image_string=vis_utils.encode_image_array_as_png_str(
                   image)))
   ])
-  summary_writer = tf.summary.FileWriterCache.get(summary_dir)
+  summary_writer = tf.compat.v1.summary.FileWriterCache.get(summary_dir)
   summary_writer.add_summary(summary, global_step)
 
   logging.info('Detection visualizations written to summary with tag %s.', tag)
@@ -270,21 +270,21 @@ def _run_checkpoint_once(tensor_dict,
   """
   if save_graph and not save_graph_dir:
     raise ValueError('`save_graph_dir` must be defined.')
-  sess = tf.Session(master, graph=tf.get_default_graph())
-  sess.run(tf.global_variables_initializer())
-  sess.run(tf.local_variables_initializer())
-  sess.run(tf.tables_initializer())
+  sess = tf.compat.v1.Session(master, graph=tf.compat.v1.get_default_graph())
+  sess.run(tf.compat.v1.global_variables_initializer())
+  sess.run(tf.compat.v1.local_variables_initializer())
+  sess.run(tf.compat.v1.tables_initializer())
   if restore_fn:
     restore_fn(sess)
   else:
     if not checkpoint_dirs:
       raise ValueError('`checkpoint_dirs` must have at least one entry.')
     checkpoint_file = tf.train.latest_checkpoint(checkpoint_dirs[0])
-    saver = tf.train.Saver(variables_to_restore)
+    saver = tf.compat.v1.train.Saver(variables_to_restore)
     saver.restore(sess, checkpoint_file)
 
   if save_graph:
-    tf.train.write_graph(sess.graph_def, save_graph_dir, 'eval.pbtxt')
+    tf.io.write_graph(sess.graph_def, save_graph_dir, 'eval.pbtxt')
 
   counters = {'skipped': 0, 'success': 0}
   aggregate_result_losses_dict = collections.defaultdict(list)
@@ -334,7 +334,7 @@ def _run_checkpoint_once(tensor_dict,
         if any(key in all_evaluator_metrics for key in metrics):
           raise ValueError('Metric names between evaluators must not collide.')
         all_evaluator_metrics.update(metrics)
-      global_step = tf.train.global_step(sess, tf.train.get_global_step())
+      global_step = tf.compat.v1.train.global_step(sess, tf.compat.v1.train.get_global_step())
 
       for key, value in iter(aggregate_result_losses_dict.items()):
         all_evaluator_metrics['Losses/' + key] = np.mean(value)
@@ -518,17 +518,17 @@ def result_dict_for_single_example(image,
 
   detection_fields = fields.DetectionResultFields
   detection_boxes = detections[detection_fields.detection_boxes][0]
-  image_shape = tf.shape(image)
+  image_shape = tf.shape(input=image)
   detection_scores = detections[detection_fields.detection_scores][0]
 
   if class_agnostic:
     detection_classes = tf.ones_like(detection_scores, dtype=tf.int64)
   else:
     detection_classes = (
-        tf.to_int64(detections[detection_fields.detection_classes][0]) +
+        tf.cast(detections[detection_fields.detection_classes][0], dtype=tf.int64) +
         label_id_offset)
 
-  num_detections = tf.to_int32(detections[detection_fields.num_detections][0])
+  num_detections = tf.cast(detections[detection_fields.num_detections][0], dtype=tf.int32)
   detection_boxes = tf.slice(
       detection_boxes, begin=[0, 0], size=[num_detections, -1])
   detection_classes = tf.slice(
@@ -570,11 +570,10 @@ def result_dict_for_single_example(image,
     if input_data_fields.groundtruth_instance_masks in groundtruth:
       masks = groundtruth[input_data_fields.groundtruth_instance_masks]
       masks = tf.expand_dims(masks, 3)
-      masks = tf.image.resize_images(
+      masks = tf.image.resize(
           masks,
           image_shape[1:3],
-          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
-          align_corners=True)
+          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
       masks = tf.squeeze(masks, 3)
       groundtruth[input_data_fields.groundtruth_instance_masks] = tf.cast(
           masks, tf.uint8)
diff --git a/research/object_detection/export_inference_graph.py b/research/object_detection/export_inference_graph.py
index 6b5257be..62ceb18a 100644
--- a/research/object_detection/export_inference_graph.py
+++ b/research/object_detection/export_inference_graph.py
@@ -130,7 +130,7 @@ FLAGS = flags.FLAGS
 
 def main(_):
   pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
-  with tf.gfile.GFile(FLAGS.pipeline_config_path, 'r') as f:
+  with tf.io.gfile.GFile(FLAGS.pipeline_config_path, 'r') as f:
     text_format.Merge(f.read(), pipeline_config)
   text_format.Merge(FLAGS.config_override, pipeline_config)
   if FLAGS.input_shape:
@@ -147,4 +147,4 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/export_tflite_ssd_graph.py b/research/object_detection/export_tflite_ssd_graph.py
index 66ae51a0..db654e00 100644
--- a/research/object_detection/export_tflite_ssd_graph.py
+++ b/research/object_detection/export_tflite_ssd_graph.py
@@ -124,7 +124,7 @@ def main(argv):
 
   pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
 
-  with tf.gfile.GFile(FLAGS.pipeline_config_path, 'r') as f:
+  with tf.io.gfile.GFile(FLAGS.pipeline_config_path, 'r') as f:
     text_format.Merge(f.read(), pipeline_config)
   text_format.Merge(FLAGS.config_override, pipeline_config)
   export_tflite_ssd_graph_lib.export_tflite_graph(
@@ -134,4 +134,4 @@ def main(argv):
 
 
 if __name__ == '__main__':
-  tf.app.run(main)
+  tf.compat.v1.app.run(main)
diff --git a/research/object_detection/export_tflite_ssd_graph_lib.py b/research/object_detection/export_tflite_ssd_graph_lib.py
index 576505d6..41bafd09 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib.py
@@ -49,7 +49,7 @@ def get_const_center_size_encoded_anchors(anchors):
   y, x, h, w = anchor_boxlist.get_center_coordinates_and_sizes()
   num_anchors = y.get_shape().as_list()
 
-  with tf.Session() as sess:
+  with tf.compat.v1.Session() as sess:
     y_out, x_out, h_out, w_out = sess.run([y, x, h, w])
   encoded_anchors = tf.constant(
       np.transpose(np.stack((y_out, x_out, h_out, w_out))),
@@ -157,7 +157,7 @@ def export_tflite_graph(pipeline_config, trained_checkpoint_prefix, output_dir,
     ValueError: if the pipeline config contains models other than ssd or uses an
       fixed_shape_resizer and provides a shape as well.
   """
-  tf.gfile.MakeDirs(output_dir)
+  tf.io.gfile.makedirs(output_dir)
   if pipeline_config.model.WhichOneof('model') != 'ssd':
     raise ValueError('Only ssd models are supported in tflite. '
                      'Found {} in config'.format(
@@ -201,7 +201,7 @@ def export_tflite_graph(pipeline_config, trained_checkpoint_prefix, output_dir,
         'is supported with tflite. Found {}'.format(
             image_resizer_config.WhichOneof('image_resizer_oneof')))
 
-  image = tf.placeholder(
+  image = tf.compat.v1.placeholder(
       tf.float32, shape=shape, name='normalized_input_image_tensor')
 
   detection_model = model_builder.build(
@@ -213,7 +213,7 @@ def export_tflite_graph(pipeline_config, trained_checkpoint_prefix, output_dir,
   class_predictions = score_conversion_fn(
       predicted_tensors['class_predictions_with_background'])
 
-  with tf.name_scope('raw_outputs'):
+  with tf.compat.v1.name_scope('raw_outputs'):
     # 'raw_outputs/box_encodings': a float32 tensor of shape [1, num_anchors, 4]
     #  containing the encoded box predictions. Note that these are raw
     #  predictions and no Non-Max suppression is applied on them and
@@ -231,7 +231,7 @@ def export_tflite_graph(pipeline_config, trained_checkpoint_prefix, output_dir,
 
   # Add global step to the graph, so we know the training step number when we
   # evaluate the model.
-  tf.train.get_or_create_global_step()
+  tf.compat.v1.train.get_or_create_global_step()
 
   # graph rewriter
   if pipeline_config.HasField('graph_rewriter'):
@@ -246,16 +246,16 @@ def export_tflite_graph(pipeline_config, trained_checkpoint_prefix, output_dir,
     saver_kwargs['write_version'] = saver_pb2.SaverDef.V1
     moving_average_checkpoint = tempfile.NamedTemporaryFile()
     exporter.replace_variable_values_with_moving_averages(
-        tf.get_default_graph(), trained_checkpoint_prefix,
+        tf.compat.v1.get_default_graph(), trained_checkpoint_prefix,
         moving_average_checkpoint.name)
     checkpoint_to_use = moving_average_checkpoint.name
   else:
     checkpoint_to_use = trained_checkpoint_prefix
 
-  saver = tf.train.Saver(**saver_kwargs)
+  saver = tf.compat.v1.train.Saver(**saver_kwargs)
   input_saver_def = saver.as_saver_def()
   frozen_graph_def = exporter.freeze_graph_with_def_protos(
-      input_graph_def=tf.get_default_graph().as_graph_def(),
+      input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),
       input_saver_def=input_saver_def,
       input_checkpoint=checkpoint_to_use,
       output_node_names=','.join([
@@ -278,8 +278,8 @@ def export_tflite_graph(pipeline_config, trained_checkpoint_prefix, output_dir,
     transformed_graph_def = frozen_graph_def
 
   binary_graph = os.path.join(output_dir, 'tflite_graph.pb')
-  with tf.gfile.GFile(binary_graph, 'wb') as f:
+  with tf.io.gfile.GFile(binary_graph, 'wb') as f:
     f.write(transformed_graph_def.SerializeToString())
   txt_graph = os.path.join(output_dir, 'tflite_graph.pbtxt')
-  with tf.gfile.GFile(txt_graph, 'w') as f:
+  with tf.io.gfile.GFile(txt_graph, 'w') as f:
     f.write(str(transformed_graph_def))
diff --git a/research/object_detection/export_tflite_ssd_graph_lib_test.py b/research/object_detection/export_tflite_ssd_graph_lib_test.py
index 3d79a5bc..11626c0b 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib_test.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib_test.py
@@ -56,7 +56,7 @@ class FakeModel(model.DetectionModel):
               tf.constant([[[0.7, 0.6], [0.9, 0.0]]], tf.float32),
       }
     with tf.control_dependencies(
-        [tf.convert_to_tensor(features.get_shape().as_list()[1:3])]):
+        [tf.convert_to_tensor(value=features.get_shape().as_list()[1:3])]):
       prediction_tensors['anchors'] = tf.constant(
           [[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 1.0, 1.0]], tf.float32)
     return prediction_tensors
@@ -81,11 +81,11 @@ class ExportTfliteGraphTest(tf.test.TestCase):
     g = tf.Graph()
     with g.as_default():
       mock_model = FakeModel()
-      inputs = tf.placeholder(tf.float32, shape=[1, 10, 10, num_channels])
+      inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 10, 10, num_channels])
       mock_model.predict(inputs, true_image_shapes=None)
       if use_moving_averages:
         tf.train.ExponentialMovingAverage(0.0).apply()
-      tf.train.get_or_create_global_step()
+      tf.compat.v1.train.get_or_create_global_step()
       if quantize:
         graph_rewriter_config = graph_rewriter_pb2.GraphRewriter()
         graph_rewriter_config.quantization.delay = 500000
@@ -93,14 +93,14 @@ class ExportTfliteGraphTest(tf.test.TestCase):
             graph_rewriter_config, is_training=False)
         graph_rewriter_fn()
 
-      saver = tf.train.Saver()
-      init = tf.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver()
+      init = tf.compat.v1.global_variables_initializer()
       with self.test_session() as sess:
         sess.run(init)
         saver.save(sess, checkpoint_path)
 
   def _assert_quant_vars_exists(self, tflite_graph_file):
-    with tf.gfile.Open(tflite_graph_file) as f:
+    with tf.io.gfile.GFile(tflite_graph_file) as f:
       graph_string = f.read()
       print(graph_string)
       self.assertTrue('quant' in graph_string)
@@ -109,8 +109,8 @@ class ExportTfliteGraphTest(tf.test.TestCase):
     """Imports a tflite graph, runs single inference and returns outputs."""
     graph = tf.Graph()
     with graph.as_default():
-      graph_def = tf.GraphDef()
-      with tf.gfile.Open(tflite_graph_file) as f:
+      graph_def = tf.compat.v1.GraphDef()
+      with tf.io.gfile.GFile(tflite_graph_file) as f:
         graph_def.ParseFromString(f.read())
       tf.import_graph_def(graph_def, name='')
       input_tensor = graph.get_tensor_by_name('normalized_input_image_tensor:0')
@@ -314,8 +314,8 @@ class ExportTfliteGraphTest(tf.test.TestCase):
     self.assertTrue(os.path.exists(tflite_graph_file))
     graph = tf.Graph()
     with graph.as_default():
-      graph_def = tf.GraphDef()
-      with tf.gfile.Open(tflite_graph_file) as f:
+      graph_def = tf.compat.v1.GraphDef()
+      with tf.io.gfile.GFile(tflite_graph_file) as f:
         graph_def.ParseFromString(f.read())
       all_op_names = [node.name for node in graph_def.node]
       self.assertTrue('TFLite_Detection_PostProcess' in all_op_names)
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index ed62fac2..d46331a0 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -52,10 +52,10 @@ def replace_variable_values_with_moving_averages(graph,
   with graph.as_default():
     variable_averages = tf.train.ExponentialMovingAverage(0.0)
     ema_variables_to_restore = variable_averages.variables_to_restore()
-    with tf.Session() as sess:
-      read_saver = tf.train.Saver(ema_variables_to_restore)
+    with tf.compat.v1.Session() as sess:
+      read_saver = tf.compat.v1.train.Saver(ema_variables_to_restore)
       read_saver.restore(sess, current_checkpoint_file)
-      write_saver = tf.train.Saver()
+      write_saver = tf.compat.v1.train.Saver()
       write_saver.save(sess, new_checkpoint_file)
 
 
@@ -63,7 +63,7 @@ def _image_tensor_input_placeholder(input_shape=None):
   """Returns input placeholder and a 4-D uint8 image tensor."""
   if input_shape is None:
     input_shape = (None, None, None, 3)
-  input_tensor = tf.placeholder(
+  input_tensor = tf.compat.v1.placeholder(
       dtype=tf.uint8, shape=input_shape, name='image_tensor')
   return input_tensor, input_tensor
 
@@ -74,7 +74,7 @@ def _tf_example_input_placeholder():
   Returns:
     a tuple of input placeholder and the output decoded images.
   """
-  batch_tf_example_placeholder = tf.placeholder(
+  batch_tf_example_placeholder = tf.compat.v1.placeholder(
       tf.string, shape=[None], name='tf_example')
   def decode(tf_example_string_tensor):
     tensor_dict = tf_example_decoder.TfExampleDecoder().decode(
@@ -95,7 +95,7 @@ def _encoded_image_string_tensor_input_placeholder():
   Returns:
     a tuple of input placeholder and the output decoded images.
   """
-  batch_image_str_placeholder = tf.placeholder(
+  batch_image_str_placeholder = tf.compat.v1.placeholder(
       dtype=tf.string,
       shape=[None],
       name='encoded_image_string_tensor')
@@ -178,7 +178,7 @@ def _add_output_tensor_nodes(postprocessed_tensors,
     outputs[detection_fields.detection_masks] = tf.identity(
         masks, name=detection_fields.detection_masks)
   for output_key in outputs:
-    tf.add_to_collection(output_collection_name, outputs[output_key])
+    tf.compat.v1.add_to_collection(output_collection_name, outputs[output_key])
 
   return outputs
 
@@ -206,22 +206,22 @@ def write_saved_model(saved_model_path,
 
       tf.import_graph_def(frozen_graph_def, name='')
 
-      builder = tf.saved_model.builder.SavedModelBuilder(saved_model_path)
+      builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(saved_model_path)
 
       tensor_info_inputs = {
-          'inputs': tf.saved_model.utils.build_tensor_info(inputs)}
+          'inputs': tf.compat.v1.saved_model.utils.build_tensor_info(inputs)}
       tensor_info_outputs = {}
       for k, v in outputs.items():
-        tensor_info_outputs[k] = tf.saved_model.utils.build_tensor_info(v)
+        tensor_info_outputs[k] = tf.compat.v1.saved_model.utils.build_tensor_info(v)
 
       detection_signature = (
-          tf.saved_model.signature_def_utils.build_signature_def(
+          tf.compat.v1.saved_model.signature_def_utils.build_signature_def(
               inputs=tensor_info_inputs,
               outputs=tensor_info_outputs,
               method_name=signature_constants.PREDICT_METHOD_NAME))
 
       builder.add_meta_graph_and_variables(
-          sess, [tf.saved_model.tag_constants.SERVING],
+          sess, [tf.saved_model.SERVING],
           signature_def_map={
               signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
                   detection_signature,
@@ -248,7 +248,7 @@ def write_graph_and_checkpoint(inference_graph_def,
 
 def _get_outputs_from_inputs(input_tensors, detection_model,
                              output_collection_name):
-  inputs = tf.to_float(input_tensors)
+  inputs = tf.cast(input_tensors, dtype=tf.float32)
   preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
   output_tensors = detection_model.predict(
       preprocessed_inputs, true_image_shapes)
@@ -295,7 +295,7 @@ def _export_inference_graph(input_type,
                             graph_hook_fn=None,
                             write_inference_graph=False):
   """Export helper."""
-  tf.gfile.MakeDirs(output_directory)
+  tf.io.gfile.makedirs(output_directory)
   frozen_graph_path = os.path.join(output_directory,
                                    'frozen_inference_graph.pb')
   saved_model_path = os.path.join(output_directory, 'saved_model')
@@ -308,7 +308,7 @@ def _export_inference_graph(input_type,
       output_collection_name=output_collection_name,
       graph_hook_fn=graph_hook_fn)
 
-  profile_inference_graph(tf.get_default_graph())
+  profile_inference_graph(tf.compat.v1.get_default_graph())
   saver_kwargs = {}
   if use_moving_averages:
     # This check is to be compatible with both version of SaverDef.
@@ -318,22 +318,22 @@ def _export_inference_graph(input_type,
     else:
       temp_checkpoint_prefix = tempfile.mkdtemp()
     replace_variable_values_with_moving_averages(
-        tf.get_default_graph(), trained_checkpoint_prefix,
+        tf.compat.v1.get_default_graph(), trained_checkpoint_prefix,
         temp_checkpoint_prefix)
     checkpoint_to_use = temp_checkpoint_prefix
   else:
     checkpoint_to_use = trained_checkpoint_prefix
 
-  saver = tf.train.Saver(**saver_kwargs)
+  saver = tf.compat.v1.train.Saver(**saver_kwargs)
   input_saver_def = saver.as_saver_def()
 
   write_graph_and_checkpoint(
-      inference_graph_def=tf.get_default_graph().as_graph_def(),
+      inference_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),
       model_path=model_path,
       input_saver_def=input_saver_def,
       trained_checkpoint_prefix=checkpoint_to_use)
   if write_inference_graph:
-    inference_graph_def = tf.get_default_graph().as_graph_def()
+    inference_graph_def = tf.compat.v1.get_default_graph().as_graph_def()
     inference_graph_path = os.path.join(output_directory,
                                         'inference_graph.pbtxt')
     for node in inference_graph_def.node:
@@ -347,7 +347,7 @@ def _export_inference_graph(input_type,
     output_node_names = ','.join(outputs.keys())
 
   frozen_graph_def = freeze_graph.freeze_graph_with_def_protos(
-      input_graph_def=tf.get_default_graph().as_graph_def(),
+      input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),
       input_saver_def=input_saver_def,
       input_checkpoint=checkpoint_to_use,
       output_node_names=output_node_names,
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index d872b561..81487d63 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -45,7 +45,7 @@ class FakeModel(model.DetectionModel):
     return tf.identity(inputs), true_image_shapes
 
   def predict(self, preprocessed_inputs, true_image_shapes):
-    return {'image': tf.layers.conv2d(preprocessed_inputs, 3, 1)}
+    return {'image': tf.compat.v1.layers.conv2d(preprocessed_inputs, 3, 1)}
 
   def postprocess(self, prediction_dict, true_image_shapes):
     with tf.control_dependencies(prediction_dict.values()):
@@ -85,20 +85,20 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     with g.as_default():
       mock_model = FakeModel()
       preprocessed_inputs, true_image_shapes = mock_model.preprocess(
-          tf.placeholder(tf.float32, shape=[None, None, None, 3]))
+          tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, 3]))
       predictions = mock_model.predict(preprocessed_inputs, true_image_shapes)
       mock_model.postprocess(predictions, true_image_shapes)
       if use_moving_averages:
         tf.train.ExponentialMovingAverage(0.0).apply()
-      tf.train.get_or_create_global_step()
+      tf.compat.v1.train.get_or_create_global_step()
       if enable_quantization:
         graph_rewriter_config = graph_rewriter_pb2.GraphRewriter()
         graph_rewriter_config.quantization.delay = 500000
         graph_rewriter_fn = graph_rewriter_builder.build(
             graph_rewriter_config, is_training=False)
         graph_rewriter_fn()
-      saver = tf.train.Saver()
-      init = tf.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver()
+      init = tf.compat.v1.global_variables_initializer()
       with self.test_session() as sess:
         sess.run(init)
         saver.save(sess, checkpoint_path)
@@ -106,8 +106,8 @@ class ExportInferenceGraphTest(tf.test.TestCase):
   def _load_inference_graph(self, inference_graph_path, is_binary=True):
     od_graph = tf.Graph()
     with od_graph.as_default():
-      od_graph_def = tf.GraphDef()
-      with tf.gfile.GFile(inference_graph_path) as fid:
+      od_graph_def = tf.compat.v1.GraphDef()
+      with tf.io.gfile.GFile(inference_graph_path) as fid:
         if is_binary:
           od_graph_def.ParseFromString(fid.read())
         else:
@@ -191,8 +191,8 @@ class ExportInferenceGraphTest(tf.test.TestCase):
 
     with tf.Graph().as_default() as od_graph:
       with self.test_session(graph=od_graph) as sess:
-        meta_graph = tf.saved_model.loader.load(
-            sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)
+        meta_graph = tf.compat.v1.saved_model.loader.load(
+            sess, [tf.saved_model.SERVING], saved_model_path)
         signature = meta_graph.signature_def['serving_default']
         input_tensor_name = signature.inputs['inputs'].name
         image_tensor = od_graph.get_tensor_by_name(input_tensor_name)
@@ -252,7 +252,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     with graph.as_default():
       fake_model = FakeModel()
       preprocessed_inputs, true_image_shapes = fake_model.preprocess(
-          tf.placeholder(dtype=tf.float32, shape=[None, None, None, 3]))
+          tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, None, None, 3]))
       predictions = fake_model.predict(preprocessed_inputs, true_image_shapes)
       fake_model.postprocess(predictions, true_image_shapes)
       exporter.replace_variable_values_with_moving_averages(
@@ -326,7 +326,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
           write_inference_graph=True)
     self._load_inference_graph(inference_graph_path, is_binary=False)
     has_quant_nodes = False
-    for v in tf.global_variables():
+    for v in tf.compat.v1.global_variables():
       if v.op.name.endswith('act_quant/min'):
         has_quant_nodes = True
         break
@@ -594,7 +594,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     output_directory = os.path.join(tmp_dir, 'output')
     inference_graph_path = os.path.join(output_directory,
                                         'frozen_inference_graph.pb')
-    tf.gfile.MakeDirs(output_directory)
+    tf.io.gfile.makedirs(output_directory)
     with mock.patch.object(
         model_builder, 'build', autospec=True) as mock_builder:
       mock_builder.return_value = FakeModel(
@@ -610,10 +610,10 @@ class ExportInferenceGraphTest(tf.test.TestCase):
           output_collection_name='inference_op',
           graph_hook_fn=None)
       output_node_names = ','.join(outputs.keys())
-      saver = tf.train.Saver()
+      saver = tf.compat.v1.train.Saver()
       input_saver_def = saver.as_saver_def()
       exporter.freeze_graph_with_def_protos(
-          input_graph_def=tf.get_default_graph().as_graph_def(),
+          input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),
           input_saver_def=input_saver_def,
           input_checkpoint=trained_checkpoint_prefix,
           output_node_names=output_node_names,
@@ -670,7 +670,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       self.assertTrue(os.path.exists(expected_pipeline_path))
 
       written_pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
-      with tf.gfile.GFile(expected_pipeline_path, 'r') as f:
+      with tf.io.gfile.GFile(expected_pipeline_path, 'r') as f:
         proto_str = f.read()
         text_format.Merge(proto_str, written_pipeline_config)
         self.assertProtoEquals(pipeline_config, written_pipeline_config)
@@ -699,8 +699,8 @@ class ExportInferenceGraphTest(tf.test.TestCase):
         np.ones((4, 4, 3)).astype(np.uint8))] * 2)
     with tf.Graph().as_default() as od_graph:
       with self.test_session(graph=od_graph) as sess:
-        meta_graph = tf.saved_model.loader.load(
-            sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)
+        meta_graph = tf.compat.v1.saved_model.loader.load(
+            sess, [tf.saved_model.SERVING], saved_model_path)
 
         signature = meta_graph.signature_def['serving_default']
         input_tensor_name = signature.inputs['inputs'].name
@@ -742,7 +742,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
                                           use_moving_averages=False)
     output_directory = os.path.join(tmp_dir, 'output')
     saved_model_path = os.path.join(output_directory, 'saved_model')
-    tf.gfile.MakeDirs(output_directory)
+    tf.io.gfile.makedirs(output_directory)
     with mock.patch.object(
         model_builder, 'build', autospec=True) as mock_builder:
       mock_builder.return_value = FakeModel(
@@ -758,10 +758,10 @@ class ExportInferenceGraphTest(tf.test.TestCase):
           output_collection_name='inference_op',
           graph_hook_fn=None)
       output_node_names = ','.join(outputs.keys())
-      saver = tf.train.Saver()
+      saver = tf.compat.v1.train.Saver()
       input_saver_def = saver.as_saver_def()
       frozen_graph_def = exporter.freeze_graph_with_def_protos(
-          input_graph_def=tf.get_default_graph().as_graph_def(),
+          input_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),
           input_saver_def=input_saver_def,
           input_checkpoint=trained_checkpoint_prefix,
           output_node_names=output_node_names,
@@ -780,8 +780,8 @@ class ExportInferenceGraphTest(tf.test.TestCase):
         np.ones((4, 4, 3)).astype(np.uint8))] * 2)
     with tf.Graph().as_default() as od_graph:
       with self.test_session(graph=od_graph) as sess:
-        meta_graph = tf.saved_model.loader.load(
-            sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)
+        meta_graph = tf.compat.v1.saved_model.loader.load(
+            sess, [tf.saved_model.SERVING], saved_model_path)
 
         signature = meta_graph.signature_def['serving_default']
         input_tensor_name = signature.inputs['inputs'].name
@@ -841,7 +841,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
         np.ones((4, 4, 3)).astype(np.uint8))] * 2)
     with tf.Graph().as_default() as od_graph:
       with self.test_session(graph=od_graph) as sess:
-        new_saver = tf.train.import_meta_graph(meta_graph_path)
+        new_saver = tf.compat.v1.train.import_meta_graph(meta_graph_path)
         new_saver.restore(sess, model_path)
 
         tf_example = od_graph.get_tensor_by_name('tf_example:0')
@@ -875,7 +875,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     output_directory = os.path.join(tmp_dir, 'output')
     model_path = os.path.join(output_directory, 'model.ckpt')
     meta_graph_path = model_path + '.meta'
-    tf.gfile.MakeDirs(output_directory)
+    tf.io.gfile.makedirs(output_directory)
     with mock.patch.object(
         model_builder, 'build', autospec=True) as mock_builder:
       mock_builder.return_value = FakeModel(
@@ -890,10 +890,10 @@ class ExportInferenceGraphTest(tf.test.TestCase):
           input_shape=None,
           output_collection_name='inference_op',
           graph_hook_fn=None)
-      saver = tf.train.Saver()
+      saver = tf.compat.v1.train.Saver()
       input_saver_def = saver.as_saver_def()
       exporter.write_graph_and_checkpoint(
-          inference_graph_def=tf.get_default_graph().as_graph_def(),
+          inference_graph_def=tf.compat.v1.get_default_graph().as_graph_def(),
           model_path=model_path,
           input_saver_def=input_saver_def,
           trained_checkpoint_prefix=trained_checkpoint_prefix)
@@ -902,7 +902,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
         np.ones((4, 4, 3)).astype(np.uint8))] * 2)
     with tf.Graph().as_default() as od_graph:
       with self.test_session(graph=od_graph) as sess:
-        new_saver = tf.train.import_meta_graph(meta_graph_path)
+        new_saver = tf.compat.v1.train.import_meta_graph(meta_graph_path)
         new_saver.restore(sess, model_path)
 
         tf_example = od_graph.get_tensor_by_name('tf_example:0')
diff --git a/research/object_detection/inference/detection_inference.py b/research/object_detection/inference/detection_inference.py
index dc66686f..d21de5b3 100644
--- a/research/object_detection/inference/detection_inference.py
+++ b/research/object_detection/inference/detection_inference.py
@@ -31,16 +31,16 @@ def build_input(tfrecord_paths):
     image_tensor: The decoded image of the example. Uint8 tensor,
         shape=[1, None, None,3]
   """
-  filename_queue = tf.train.string_input_producer(
+  filename_queue = tf.compat.v1.train.string_input_producer(
       tfrecord_paths, shuffle=False, num_epochs=1)
 
-  tf_record_reader = tf.TFRecordReader()
+  tf_record_reader = tf.compat.v1.TFRecordReader()
   _, serialized_example_tensor = tf_record_reader.read(filename_queue)
-  features = tf.parse_single_example(
-      serialized_example_tensor,
+  features = tf.io.parse_single_example(
+      serialized=serialized_example_tensor,
       features={
           standard_fields.TfExampleFields.image_encoded:
-              tf.FixedLenFeature([], tf.string),
+              tf.io.FixedLenFeature([], tf.string),
       })
   encoded_image = features[standard_fields.TfExampleFields.image_encoded]
   image_tensor = tf.image.decode_image(encoded_image, channels=3)
@@ -65,15 +65,15 @@ def build_inference_graph(image_tensor, inference_graph_path):
     detected_labels_tensor: Detected labels. Int64 tensor,
         shape=[num_detections]
   """
-  with tf.gfile.Open(inference_graph_path, 'r') as graph_def_file:
+  with tf.io.gfile.GFile(inference_graph_path, 'r') as graph_def_file:
     graph_content = graph_def_file.read()
-  graph_def = tf.GraphDef()
+  graph_def = tf.compat.v1.GraphDef()
   graph_def.MergeFromString(graph_content)
 
   tf.import_graph_def(
       graph_def, name='', input_map={'image_tensor': image_tensor})
 
-  g = tf.get_default_graph()
+  g = tf.compat.v1.get_default_graph()
 
   num_detections_tensor = tf.squeeze(
       g.get_tensor_by_name('num_detections:0'), 0)
@@ -114,7 +114,7 @@ def infer_detections_and_add_to_example(
   """
   tf_example = tf.train.Example()
   (serialized_example, detected_boxes, detected_scores,
-   detected_classes) = tf.get_default_session().run([
+   detected_classes) = tf.compat.v1.get_default_session().run([
        serialized_example_tensor, detected_boxes_tensor, detected_scores_tensor,
        detected_labels_tensor
    ])
diff --git a/research/object_detection/inference/detection_inference_test.py b/research/object_detection/inference/detection_inference_test.py
index eabb6b47..6126e0fd 100644
--- a/research/object_detection/inference/detection_inference_test.py
+++ b/research/object_detection/inference/detection_inference_test.py
@@ -27,7 +27,7 @@ from object_detection.utils import dataset_util
 
 
 def get_mock_tfrecord_path():
-  return os.path.join(tf.test.get_temp_dir(), 'mock.tfrec')
+  return os.path.join(tf.compat.v1.test.get_temp_dir(), 'mock.tfrec')
 
 
 def create_mock_tfrecord():
@@ -44,18 +44,18 @@ def create_mock_tfrecord():
   }
 
   tf_example = tf.train.Example(features=tf.train.Features(feature=feature_map))
-  with tf.python_io.TFRecordWriter(get_mock_tfrecord_path()) as writer:
+  with tf.io.TFRecordWriter(get_mock_tfrecord_path()) as writer:
     writer.write(tf_example.SerializeToString())
 
 
 def get_mock_graph_path():
-  return os.path.join(tf.test.get_temp_dir(), 'mock_graph.pb')
+  return os.path.join(tf.compat.v1.test.get_temp_dir(), 'mock_graph.pb')
 
 
 def create_mock_graph():
   g = tf.Graph()
   with g.as_default():
-    in_image_tensor = tf.placeholder(
+    in_image_tensor = tf.compat.v1.placeholder(
         tf.uint8, shape=[1, None, None, 3], name='image_tensor')
     tf.constant([2.0], name='num_detections')
     tf.constant(
@@ -64,11 +64,11 @@ def create_mock_graph():
     tf.constant([[0.1, 0.2, 0.3]], name='detection_scores')
     tf.identity(
         tf.constant([[1.0, 2.0, 3.0]]) *
-        tf.reduce_sum(tf.cast(in_image_tensor, dtype=tf.float32)),
+        tf.reduce_sum(input_tensor=tf.cast(in_image_tensor, dtype=tf.float32)),
         name='detection_classes')
     graph_def = g.as_graph_def()
 
-  with tf.gfile.Open(get_mock_graph_path(), 'w') as fl:
+  with tf.io.gfile.GFile(get_mock_graph_path(), 'w') as fl:
     fl.write(graph_def.SerializeToString())
 
 
@@ -87,9 +87,9 @@ class InferDetectionsTests(tf.test.TestCase):
          image_tensor, get_mock_graph_path())
 
     with self.test_session(use_gpu=False) as sess:
-      sess.run(tf.global_variables_initializer())
-      sess.run(tf.local_variables_initializer())
-      tf.train.start_queue_runners()
+      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.compat.v1.local_variables_initializer())
+      tf.compat.v1.train.start_queue_runners()
 
       tf_example = detection_inference.infer_detections_and_add_to_example(
           serialized_example_tensor, detected_boxes_tensor,
@@ -138,9 +138,9 @@ class InferDetectionsTests(tf.test.TestCase):
          image_tensor, get_mock_graph_path())
 
     with self.test_session(use_gpu=False) as sess:
-      sess.run(tf.global_variables_initializer())
-      sess.run(tf.local_variables_initializer())
-      tf.train.start_queue_runners()
+      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.compat.v1.local_variables_initializer())
+      tf.compat.v1.train.start_queue_runners()
 
       tf_example = detection_inference.infer_detections_and_add_to_example(
           serialized_example_tensor, detected_boxes_tensor,
diff --git a/research/object_detection/inference/infer_detections.py b/research/object_detection/inference/infer_detections.py
index a251009e..2b12f83d 100644
--- a/research/object_detection/inference/infer_detections.py
+++ b/research/object_detection/inference/infer_detections.py
@@ -34,63 +34,63 @@ subsequent processing steps that don't require the images (e.g. computing
 metrics).
 """
 
+import argparse
 import itertools
 import tensorflow as tf
 from object_detection.inference import detection_inference
 
-tf.flags.DEFINE_string('input_tfrecord_paths', None,
-                       'A comma separated list of paths to input TFRecords.')
-tf.flags.DEFINE_string('output_tfrecord_path', None,
-                       'Path to the output TFRecord.')
-tf.flags.DEFINE_string('inference_graph', None,
-                       'Path to the inference graph with embedded weights.')
-tf.flags.DEFINE_boolean('discard_image_pixels', False,
-                        'Discards the images in the output TFExamples. This'
-                        ' significantly reduces the output size and is useful'
-                        ' if the subsequent tools don\'t need access to the'
-                        ' images (e.g. when computing evaluation measures).')
+parser = argparse.ArgumentParser()
+parser.add_argument('--input_tfrecord_paths', type=str, default=None,
+                    help='A comma separated list of paths to input TFRecords.')
+parser.add_argument('--output_tfrecord_path', type=str, default=None, help='Path to the output TFRecord.')
+parser.add_argument('--inference_graph', type=str, default=None,
+                    help='Path to the inference graph with embedded weights.')
+parser.add_argument('--discard_image_pixels', type=bool, default=False,
+                    help='Discards the images in the output TFExamples. This significantly reduces the output size '
+                         'and is useful if the subsequent tools don\'t need access to the images '
+                         '(e.g. when computing evaluation measures).')
 
-FLAGS = tf.flags.FLAGS
+args = parser.parse_args()
 
 
 def main(_):
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
   required_flags = ['input_tfrecord_paths', 'output_tfrecord_path',
                     'inference_graph']
   for flag_name in required_flags:
-    if not getattr(FLAGS, flag_name):
+    if not getattr(args, flag_name):
       raise ValueError('Flag --{} is required'.format(flag_name))
 
-  with tf.Session() as sess:
+  with tf.compat.v1.Session() as sess:
     input_tfrecord_paths = [
-        v for v in FLAGS.input_tfrecord_paths.split(',') if v]
-    tf.logging.info('Reading input from %d files', len(input_tfrecord_paths))
+        v for v in args.input_tfrecord_paths.split(',') if v]
+    tf.compat.v1.logging.info('Reading input from %d files', len(input_tfrecord_paths))
     serialized_example_tensor, image_tensor = detection_inference.build_input(
         input_tfrecord_paths)
-    tf.logging.info('Reading graph and building model...')
+    tf.compat.v1.logging.info('Reading graph and building model...')
     (detected_boxes_tensor, detected_scores_tensor,
      detected_labels_tensor) = detection_inference.build_inference_graph(
-         image_tensor, FLAGS.inference_graph)
+         image_tensor, args.inference_graph)
 
-    tf.logging.info('Running inference and writing output to {}'.format(
-        FLAGS.output_tfrecord_path))
-    sess.run(tf.local_variables_initializer())
-    tf.train.start_queue_runners()
-    with tf.python_io.TFRecordWriter(
-        FLAGS.output_tfrecord_path) as tf_record_writer:
+    tf.compat.v1.logging.info('Running inference and writing output to {}'.format(
+        args.output_tfrecord_path))
+    sess.run(tf.compat.v1.local_variables_initializer())
+    tf.compat.v1.train.start_queue_runners()
+    with tf.io.TFRecordWriter(
+        args.output_tfrecord_path) as tf_record_writer:
       try:
         for counter in itertools.count():
-          tf.logging.log_every_n(tf.logging.INFO, 'Processed %d images...', 10,
+          tf.compat.v1.logging.log_every_n(tf.compat.v1.logging.INFO, 'Processed %d images...', 10,
                                  counter)
           tf_example = detection_inference.infer_detections_and_add_to_example(
               serialized_example_tensor, detected_boxes_tensor,
               detected_scores_tensor, detected_labels_tensor,
-              FLAGS.discard_image_pixels)
+              args.discard_image_pixels)
           tf_record_writer.write(tf_example.SerializeToString())
       except tf.errors.OutOfRangeError:
-        tf.logging.info('Finished processing records')
+        tf.compat.v1.logging.info('Finished processing records')
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 8c0718a1..8a146fd8 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -110,7 +110,7 @@ def transform_input_data(tensor_dict,
   # Apply model preprocessing ops and resize instance masks.
   image = tensor_dict[fields.InputDataFields.image]
   preprocessed_resized_image, true_image_shape = model_preprocess_fn(
-      tf.expand_dims(tf.to_float(image), axis=0))
+      tf.expand_dims(tf.cast(image, dtype=tf.float32), axis=0))
   tensor_dict[fields.InputDataFields.image] = tf.squeeze(
       preprocessed_resized_image, axis=0)
   tensor_dict[fields.InputDataFields.true_image_shape] = tf.squeeze(
@@ -246,7 +246,7 @@ def augment_input_data(tensor_dict, data_augmentation_options):
     input tensor dictionary.
   """
   tensor_dict[fields.InputDataFields.image] = tf.expand_dims(
-      tf.to_float(tensor_dict[fields.InputDataFields.image]), 0)
+      tf.cast(tensor_dict[fields.InputDataFields.image], dtype=tf.float32), 0)
 
   include_instance_masks = (fields.InputDataFields.groundtruth_instance_masks
                             in tensor_dict)
@@ -293,7 +293,7 @@ def _get_labels_dict(input_dict):
 
 def _get_features_dict(input_dict):
   """Extracts features dict from input dict."""
-  hash_from_source_id = tf.string_to_hash_bucket_fast(
+  hash_from_source_id = tf.strings.to_hash_bucket_fast(
       input_dict[fields.InputDataFields.source_id], HASH_BINS)
   features = {
       fields.InputDataFields.image:
@@ -525,7 +525,7 @@ def create_predict_input_fn(model_config, predict_input_config):
       `ServingInputReceiver`.
     """
     del params
-    example = tf.placeholder(dtype=tf.string, shape=[], name='tf_example')
+    example = tf.compat.v1.placeholder(dtype=tf.string, shape=[], name='tf_example')
 
     num_classes = config_util.get_number_of_classes(model_config)
     model = model_builder.build(model_config, is_training=False)
@@ -542,7 +542,7 @@ def create_predict_input_fn(model_config, predict_input_config):
         load_instance_masks=False,
         num_additional_channels=predict_input_config.num_additional_channels)
     input_dict = transform_fn(decoder.decode(example))
-    images = tf.to_float(input_dict[fields.InputDataFields.image])
+    images = tf.cast(input_dict[fields.InputDataFields.image], dtype=tf.float32)
     images = tf.expand_dims(images, axis=0)
     true_image_shape = tf.expand_dims(
         input_dict[fields.InputDataFields.true_image_shape], axis=0)
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index abb05b4f..6f96d185 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -34,11 +34,11 @@ FLAGS = tf.flags.FLAGS
 
 def _get_configs_for_model(model_name):
   """Returns configurations for model."""
-  fname = os.path.join(tf.resource_loader.get_data_files_path(),
+  fname = os.path.join(tf.compat.v1.resource_loader.get_data_files_path(),
                        'samples/configs/' + model_name + '.config')
-  label_map_path = os.path.join(tf.resource_loader.get_data_files_path(),
+  label_map_path = os.path.join(tf.compat.v1.resource_loader.get_data_files_path(),
                                 'data/pet_label_map.pbtxt')
-  data_path = os.path.join(tf.resource_loader.get_data_files_path(),
+  data_path = os.path.join(tf.compat.v1.resource_loader.get_data_files_path(),
                            'test_data/pets_examples.record')
   configs = config_util.get_configs_from_pipeline_file(fname)
   return config_util.merge_external_params_with_configs(
@@ -57,8 +57,8 @@ def _make_initializable_iterator(dataset):
   Returns:
     A `tf.data.Iterator`.
   """
-  iterator = dataset.make_initializable_iterator()
-  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
+  iterator = tf.compat.v1.data.make_initializable_iterator(dataset)
+  tf.compat.v1.add_to_collection(tf.compat.v1.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
   return iterator
 
 
@@ -418,11 +418,11 @@ class DataAugmentationFnTest(tf.test.TestCase):
 
 
 def _fake_model_preprocessor_fn(image):
-  return (image, tf.expand_dims(tf.shape(image)[1:], axis=0))
+  return (image, tf.expand_dims(tf.shape(input=image)[1:], axis=0))
 
 
 def _fake_image_resizer_fn(image, mask):
-  return (image, mask, tf.shape(image))
+  return (image, mask, tf.shape(input=image))
 
 
 class DataTransformationFnTest(tf.test.TestCase):
@@ -515,14 +515,14 @@ class DataTransformationFnTest(tf.test.TestCase):
             tf.constant(np.array([3, 1], np.int32))
     }
     def fake_image_resizer_fn(image, masks=None):
-      resized_image = tf.image.resize_images(image, [8, 8])
+      resized_image = tf.image.resize(image, [8, 8])
       results = [resized_image]
       if masks is not None:
         resized_masks = tf.transpose(
-            tf.image.resize_images(tf.transpose(masks, [1, 2, 0]), [8, 8]),
-            [2, 0, 1])
+            a=tf.image.resize(tf.transpose(a=masks, perm=[1, 2, 0]), [8, 8]),
+            perm=[2, 0, 1])
         results.append(resized_masks)
-      results.append(tf.shape(resized_image))
+      results.append(tf.shape(input=resized_image))
       return results
 
     num_classes = 3
@@ -551,7 +551,7 @@ class DataTransformationFnTest(tf.test.TestCase):
             tf.constant(np.array([3, 1], np.int32))
     }
     def fake_model_preprocessor_fn(image):
-      return (image / 255., tf.expand_dims(tf.shape(image)[1:], axis=0))
+      return (image / 255., tf.expand_dims(tf.shape(input=image)[1:], axis=0))
 
     num_classes = 3
     input_transformation_fn = functools.partial(
@@ -606,7 +606,7 @@ class DataTransformationFnTest(tf.test.TestCase):
             tf.constant(np.array([3, 1], np.int32))
     }
     def mul_two_model_preprocessor_fn(image):
-      return (image * 2, tf.expand_dims(tf.shape(image)[1:], axis=0))
+      return (image * 2, tf.expand_dims(tf.shape(input=image)[1:], axis=0))
     def add_five_to_image_data_augmentation_fn(tensor_dict):
       tensor_dict[fields.InputDataFields.image] += 5
       return tensor_dict
@@ -631,12 +631,12 @@ class PadInputDataToStaticShapesFnTest(tf.test.TestCase):
   def test_pad_images_boxes_and_classes(self):
     input_tensor_dict = {
         fields.InputDataFields.image:
-            tf.placeholder(tf.float32, [None, None, 3]),
+            tf.compat.v1.placeholder(tf.float32, [None, None, 3]),
         fields.InputDataFields.groundtruth_boxes:
-            tf.placeholder(tf.float32, [None, 4]),
+            tf.compat.v1.placeholder(tf.float32, [None, 4]),
         fields.InputDataFields.groundtruth_classes:
-            tf.placeholder(tf.int32, [None, 3]),
-        fields.InputDataFields.true_image_shape: tf.placeholder(tf.int32, [3]),
+            tf.compat.v1.placeholder(tf.int32, [None, 3]),
+        fields.InputDataFields.true_image_shape: tf.compat.v1.placeholder(tf.int32, [3]),
     }
     padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
         tensor_dict=input_tensor_dict,
@@ -660,11 +660,11 @@ class PadInputDataToStaticShapesFnTest(tf.test.TestCase):
   def test_clip_boxes_and_classes(self):
     input_tensor_dict = {
         fields.InputDataFields.groundtruth_boxes:
-            tf.placeholder(tf.float32, [None, 4]),
+            tf.compat.v1.placeholder(tf.float32, [None, 4]),
         fields.InputDataFields.groundtruth_classes:
-            tf.placeholder(tf.int32, [None, 3]),
+            tf.compat.v1.placeholder(tf.int32, [None, 3]),
         fields.InputDataFields.num_groundtruth_boxes:
-            tf.placeholder(tf.int32, [])
+            tf.compat.v1.placeholder(tf.int32, [])
     }
     padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
         tensor_dict=input_tensor_dict,
@@ -703,7 +703,7 @@ class PadInputDataToStaticShapesFnTest(tf.test.TestCase):
   def test_do_not_pad_dynamic_images(self):
     input_tensor_dict = {
         fields.InputDataFields.image:
-            tf.placeholder(tf.float32, [None, None, 3]),
+            tf.compat.v1.placeholder(tf.float32, [None, None, 3]),
     }
     padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
         tensor_dict=input_tensor_dict,
@@ -718,9 +718,9 @@ class PadInputDataToStaticShapesFnTest(tf.test.TestCase):
   def test_images_and_additional_channels(self):
     input_tensor_dict = {
         fields.InputDataFields.image:
-            tf.placeholder(tf.float32, [None, None, 3]),
+            tf.compat.v1.placeholder(tf.float32, [None, None, 3]),
         fields.InputDataFields.image_additional_channels:
-            tf.placeholder(tf.float32, [None, None, 2]),
+            tf.compat.v1.placeholder(tf.float32, [None, None, 2]),
     }
     padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
         tensor_dict=input_tensor_dict,
@@ -738,9 +738,9 @@ class PadInputDataToStaticShapesFnTest(tf.test.TestCase):
   def test_keypoints(self):
     input_tensor_dict = {
         fields.InputDataFields.groundtruth_keypoints:
-            tf.placeholder(tf.float32, [None, 16, 4]),
+            tf.compat.v1.placeholder(tf.float32, [None, 16, 4]),
         fields.InputDataFields.groundtruth_keypoint_visibilities:
-            tf.placeholder(tf.bool, [None, 16]),
+            tf.compat.v1.placeholder(tf.bool, [None, 16]),
     }
     padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
         tensor_dict=input_tensor_dict,
diff --git a/research/object_detection/legacy/eval.py b/research/object_detection/legacy/eval.py
index 13e4cf09..ffd87339 100644
--- a/research/object_detection/legacy/eval.py
+++ b/research/object_detection/legacy/eval.py
@@ -55,7 +55,7 @@ from object_detection.utils import config_util
 from object_detection.utils import label_map_util
 
 
-tf.logging.set_verbosity(tf.logging.INFO)
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
 flags = tf.app.flags
 flags.DEFINE_boolean('eval_training_data', False,
@@ -84,11 +84,11 @@ FLAGS = flags.FLAGS
 def main(unused_argv):
   assert FLAGS.checkpoint_dir, '`checkpoint_dir` is missing.'
   assert FLAGS.eval_dir, '`eval_dir` is missing.'
-  tf.gfile.MakeDirs(FLAGS.eval_dir)
+  tf.io.gfile.makedirs(FLAGS.eval_dir)
   if FLAGS.pipeline_config_path:
     configs = config_util.get_configs_from_pipeline_file(
         FLAGS.pipeline_config_path)
-    tf.gfile.Copy(FLAGS.pipeline_config_path,
+    tf.io.gfile.copy(FLAGS.pipeline_config_path,
                   os.path.join(FLAGS.eval_dir, 'pipeline.config'),
                   overwrite=True)
   else:
@@ -99,7 +99,7 @@ def main(unused_argv):
     for name, config in [('model.config', FLAGS.model_config_path),
                          ('eval.config', FLAGS.eval_config_path),
                          ('input.config', FLAGS.input_config_path)]:
-      tf.gfile.Copy(config,
+      tf.io.gfile.copy(config,
                     os.path.join(FLAGS.eval_dir, name),
                     overwrite=True)
 
@@ -115,8 +115,8 @@ def main(unused_argv):
       is_training=False)
 
   def get_next(config):
-    return dataset_builder.make_initializable_iterator(
-        dataset_builder.build(config)).get_next()
+    return tf.compat.v1.data.make_initializable_iterator(
+        dataset_builder, dataset_builder.build(config)).get_next()
 
   create_input_dict_fn = functools.partial(get_next, input_config)
 
@@ -144,4 +144,4 @@ def main(unused_argv):
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/legacy/evaluator.py b/research/object_detection/legacy/evaluator.py
index 4506f672..f75c43c2 100644
--- a/research/object_detection/legacy/evaluator.py
+++ b/research/object_detection/legacy/evaluator.py
@@ -74,7 +74,7 @@ def _extract_predictions_and_losses(model,
   input_dict = prefetch_queue.dequeue()
   original_image = tf.expand_dims(input_dict[fields.InputDataFields.image], 0)
   preprocessed_image, true_image_shapes = model.preprocess(
-      tf.to_float(original_image))
+      tf.cast(original_image, dtype=tf.float32))
   prediction_dict = model.predict(preprocessed_image, true_image_shapes)
   detections = model.postprocess(prediction_dict, true_image_shapes)
 
@@ -218,7 +218,7 @@ def evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,
       logging.info('Skipping image')
       counters['skipped'] += 1
       return {}, {}
-    global_step = tf.train.global_step(sess, tf.train.get_global_step())
+    global_step = tf.compat.v1.train.global_step(sess, tf.compat.v1.train.get_global_step())
     if batch_index < eval_config.num_visualizations:
       tag = 'image-{}'.format(batch_index)
       eval_util.visualize_detection_results(
@@ -241,14 +241,14 @@ def evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,
 
   if graph_hook_fn: graph_hook_fn()
 
-  variables_to_restore = tf.global_variables()
-  global_step = tf.train.get_or_create_global_step()
+  variables_to_restore = tf.compat.v1.global_variables()
+  global_step = tf.compat.v1.train.get_or_create_global_step()
   variables_to_restore.append(global_step)
 
   if eval_config.use_moving_averages:
     variable_averages = tf.train.ExponentialMovingAverage(0.0)
     variables_to_restore = variable_averages.variables_to_restore()
-  saver = tf.train.Saver(variables_to_restore)
+  saver = tf.compat.v1.train.Saver(variables_to_restore)
 
   def _restore_latest_checkpoint(sess):
     latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)
diff --git a/research/object_detection/legacy/train.py b/research/object_detection/legacy/train.py
index 33b9a9cf..54259284 100644
--- a/research/object_detection/legacy/train.py
+++ b/research/object_detection/legacy/train.py
@@ -52,7 +52,7 @@ from object_detection.builders import model_builder
 from object_detection.legacy import trainer
 from object_detection.utils import config_util
 
-tf.logging.set_verbosity(tf.logging.INFO)
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
 flags = tf.app.flags
 flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')
@@ -87,12 +87,12 @@ FLAGS = flags.FLAGS
 @tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')
 def main(_):
   assert FLAGS.train_dir, '`train_dir` is missing.'
-  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)
+  if FLAGS.task == 0: tf.io.gfile.makedirs(FLAGS.train_dir)
   if FLAGS.pipeline_config_path:
     configs = config_util.get_configs_from_pipeline_file(
         FLAGS.pipeline_config_path)
     if FLAGS.task == 0:
-      tf.gfile.Copy(FLAGS.pipeline_config_path,
+      tf.io.gfile.copy(FLAGS.pipeline_config_path,
                     os.path.join(FLAGS.train_dir, 'pipeline.config'),
                     overwrite=True)
   else:
@@ -104,7 +104,7 @@ def main(_):
       for name, config in [('model.config', FLAGS.model_config_path),
                            ('train.config', FLAGS.train_config_path),
                            ('input.config', FLAGS.input_config_path)]:
-        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),
+        tf.io.gfile.copy(config, os.path.join(FLAGS.train_dir, name),
                       overwrite=True)
 
   model_config = configs['model']
@@ -117,8 +117,8 @@ def main(_):
       is_training=True)
 
   def get_next(config):
-    return dataset_builder.make_initializable_iterator(
-        dataset_builder.build(config)).get_next()
+    return tf.compat.v1.data.make_initializable_iterator(
+        dataset_builder, dataset_builder.build(config)).get_next()
 
   create_input_dict_fn = functools.partial(get_next, input_config)
 
@@ -147,7 +147,7 @@ def main(_):
 
   if worker_replicas >= 1 and ps_tasks > 0:
     # Set up distributed training.
-    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',
+    server = tf.distribute.Server(tf.train.ClusterSpec(cluster), protocol='grpc',
                              job_name=task_info.type,
                              task_index=task_info.index)
     if task_info.type == 'ps':
@@ -181,4 +181,4 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/legacy/trainer.py b/research/object_detection/legacy/trainer.py
index 89635728..15af2720 100644
--- a/research/object_detection/legacy/trainer.py
+++ b/research/object_detection/legacy/trainer.py
@@ -62,7 +62,7 @@ def create_input_queue(batch_size_per_clone, create_tensor_dict_fn,
       tensor_dict[fields.InputDataFields.image], 0)
 
   images = tensor_dict[fields.InputDataFields.image]
-  float_images = tf.to_float(images)
+  float_images = tf.cast(images, dtype=tf.float32)
   tensor_dict[fields.InputDataFields.image] = float_images
 
   include_instance_masks = (fields.InputDataFields.groundtruth_instance_masks
@@ -204,7 +204,7 @@ def _create_losses(input_queue, create_model_fn, train_config):
 
   losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
   for loss_tensor in losses_dict.values():
-    tf.losses.add_loss(loss_tensor)
+    tf.compat.v1.losses.add_loss(loss_tensor)
 
 
 def train(create_tensor_dict_fn,
@@ -281,7 +281,7 @@ def train(create_tensor_dict_fn,
     # Gather initial summaries.
     # TODO(rathodv): See if summaries can be added/extracted from global tf
     # collections so that they don't have to be passed around.
-    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))
+    summaries = set(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.SUMMARIES))
     global_summaries = set([])
 
     model_fn = functools.partial(_create_losses,
@@ -296,17 +296,17 @@ def train(create_tensor_dict_fn,
 
     # Gather update_ops from the first clone. These contain, for example,
     # the updates for the batch_norm variables created by model_fn.
-    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)
+    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS, first_clone_scope)
 
     with tf.device(deploy_config.optimizer_device()):
       training_optimizer, optimizer_summary_vars = optimizer_builder.build(
           train_config.optimizer)
       for var in optimizer_summary_vars:
-        tf.summary.scalar(var.op.name, var, family='LearningRate')
+        tf.compat.v1.summary.scalar(var.op.name, var, family='LearningRate')
 
     sync_optimizer = None
     if train_config.sync_replicas:
-      training_optimizer = tf.train.SyncReplicasOptimizer(
+      training_optimizer = tf.compat.v1.train.SyncReplicasOptimizer(
           training_optimizer,
           replicas_to_aggregate=train_config.replicas_to_aggregate,
           total_num_replicas=worker_replicas)
@@ -318,7 +318,7 @@ def train(create_tensor_dict_fn,
       total_loss, grads_and_vars = model_deploy.optimize_clones(
           clones, training_optimizer,
           regularization_losses=regularization_losses)
-      total_loss = tf.check_numerics(total_loss, 'LossTensor is inf or nan.')
+      total_loss = tf.debugging.check_numerics(total_loss, 'LossTensor is inf or nan.')
 
       # Optionally multiply bias gradients by train_config.bias_grad_multiplier.
       if train_config.bias_grad_multiplier:
@@ -335,7 +335,7 @@ def train(create_tensor_dict_fn,
 
       # Optionally clip gradients
       if train_config.gradient_clipping_by_norm > 0:
-        with tf.name_scope('clip_grads'):
+        with tf.compat.v1.name_scope('clip_grads'):
           grads_and_vars = slim.learning.clip_gradient_norms(
               grads_and_vars, train_config.gradient_clipping_by_norm)
 
@@ -349,30 +349,30 @@ def train(create_tensor_dict_fn,
 
     # Add summaries.
     for model_var in slim.get_model_variables():
-      global_summaries.add(tf.summary.histogram('ModelVars/' +
+      global_summaries.add(tf.compat.v1.summary.histogram('ModelVars/' +
                                                 model_var.op.name, model_var))
-    for loss_tensor in tf.losses.get_losses():
-      global_summaries.add(tf.summary.scalar('Losses/' + loss_tensor.op.name,
+    for loss_tensor in tf.compat.v1.losses.get_losses():
+      global_summaries.add(tf.compat.v1.summary.scalar('Losses/' + loss_tensor.op.name,
                                              loss_tensor))
     global_summaries.add(
-        tf.summary.scalar('Losses/TotalLoss', tf.losses.get_total_loss()))
+        tf.compat.v1.summary.scalar('Losses/TotalLoss', tf.compat.v1.losses.get_total_loss()))
 
     # Add the summaries from the first clone. These contain the summaries
     # created by model_fn and either optimize_clones() or _gather_clone_loss().
-    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,
+    summaries |= set(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.SUMMARIES,
                                        first_clone_scope))
     summaries |= global_summaries
 
     # Merge all summaries together.
-    summary_op = tf.summary.merge(list(summaries), name='summary_op')
+    summary_op = tf.compat.v1.summary.merge(list(summaries), name='summary_op')
 
     # Soft placement allows placing on CPU ops without GPU implementation.
-    session_config = tf.ConfigProto(allow_soft_placement=True,
+    session_config = tf.compat.v1.ConfigProto(allow_soft_placement=True,
                                     log_device_placement=False)
 
     # Save checkpoints regularly.
     keep_checkpoint_every_n_hours = train_config.keep_checkpoint_every_n_hours
-    saver = tf.train.Saver(
+    saver = tf.compat.v1.train.Saver(
         keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
 
     # Create ops required to initialize the model from a given checkpoint.
@@ -394,7 +394,7 @@ def train(create_tensor_dict_fn,
                            get_variables_available_in_checkpoint(
                                var_map, train_config.fine_tune_checkpoint,
                                include_global_step=False))
-      init_saver = tf.train.Saver(available_var_map)
+      init_saver = tf.compat.v1.train.Saver(available_var_map)
       def initializer_fn(sess):
         init_saver.restore(sess, train_config.fine_tune_checkpoint)
       init_fn = initializer_fn
diff --git a/research/object_detection/legacy/trainer_test.py b/research/object_detection/legacy/trainer_test.py
index 82e77274..6551383f 100644
--- a/research/object_detection/legacy/trainer_test.py
+++ b/research/object_detection/legacy/trainer_test.py
@@ -31,13 +31,13 @@ NUMBER_OF_CLASSES = 2
 
 def get_input_function():
   """A function to get test inputs. Returns an image with one box."""
-  image = tf.random_uniform([32, 32, 3], dtype=tf.float32)
+  image = tf.random.uniform([32, 32, 3], dtype=tf.float32)
   key = tf.constant('image_000000')
-  class_label = tf.random_uniform(
+  class_label = tf.random.uniform(
       [1], minval=0, maxval=NUMBER_OF_CLASSES, dtype=tf.int32)
-  box_label = tf.random_uniform(
+  box_label = tf.random.uniform(
       [1, 4], minval=0.4, maxval=0.6, dtype=tf.float32)
-  multiclass_scores = tf.random_uniform(
+  multiclass_scores = tf.random.uniform(
       [1, NUMBER_OF_CLASSES], minval=0.4, maxval=0.6, dtype=tf.float32)
 
   return {
@@ -73,7 +73,7 @@ class FakeDetectionModel(model.DetectionModel):
     """
     true_image_shapes = [inputs.shape[:-1].as_list()
                          for _ in range(inputs.shape[-1])]
-    return tf.image.resize_images(inputs, [28, 28]), true_image_shapes
+    return tf.image.resize(inputs, [28, 28]), true_image_shapes
 
   def predict(self, preprocessed_inputs, true_image_shapes):
     """Prediction tensors from inputs tensor.
@@ -155,8 +155,8 @@ class FakeDetectionModel(model.DetectionModel):
         weights=weights)
 
     loss_dict = {
-        'localization_loss': tf.reduce_sum(location_losses),
-        'classification_loss': tf.reduce_sum(cls_losses),
+        'localization_loss': tf.reduce_sum(input_tensor=location_losses),
+        'classification_loss': tf.reduce_sum(input_tensor=cls_losses),
     }
     return loss_dict
 
@@ -172,7 +172,7 @@ class FakeDetectionModel(model.DetectionModel):
     Returns:
       A dict mapping variable names to variables.
     """
-    return {var.op.name: var for var in tf.global_variables()}
+    return {var.op.name: var for var in tf.compat.v1.global_variables()}
 
 
 class TrainerTest(tf.test.TestCase):
diff --git a/research/object_detection/matchers/argmax_matcher.py b/research/object_detection/matchers/argmax_matcher.py
index d397ff41..3f6a41a9 100644
--- a/research/object_detection/matchers/argmax_matcher.py
+++ b/research/object_detection/matchers/argmax_matcher.py
@@ -135,12 +135,12 @@ class ArgMaxMatcher(matcher.Matcher):
         matches:  int32 tensor indicating the row each column matches to.
       """
       # Matches for each column
-      matches = tf.argmax(similarity_matrix, 0, output_type=tf.int32)
+      matches = tf.argmax(input=similarity_matrix, axis=0, output_type=tf.int32)
 
       # Deal with matched and unmatched threshold
       if self._matched_threshold is not None:
         # Get logical indices of ignored and unmatched columns as tf.int64
-        matched_vals = tf.reduce_max(similarity_matrix, 0)
+        matched_vals = tf.reduce_max(input_tensor=similarity_matrix, axis=0)
         below_unmatched_threshold = tf.greater(self._unmatched_threshold,
                                                matched_vals)
         between_thresholds = tf.logical_and(
@@ -165,15 +165,15 @@ class ArgMaxMatcher(matcher.Matcher):
       if self._force_match_for_each_row:
         similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(
             similarity_matrix)
-        force_match_column_ids = tf.argmax(similarity_matrix, 1,
+        force_match_column_ids = tf.argmax(input=similarity_matrix, axis=1,
                                            output_type=tf.int32)
         force_match_column_indicators = tf.one_hot(
             force_match_column_ids, depth=similarity_matrix_shape[1])
-        force_match_row_ids = tf.argmax(force_match_column_indicators, 0,
+        force_match_row_ids = tf.argmax(input=force_match_column_indicators, axis=0,
                                         output_type=tf.int32)
         force_match_column_mask = tf.cast(
-            tf.reduce_max(force_match_column_indicators, 0), tf.bool)
-        final_matches = tf.where(force_match_column_mask,
+            tf.reduce_max(input_tensor=force_match_column_indicators, axis=0), tf.bool)
+        final_matches = tf.compat.v1.where(force_match_column_mask,
                                  force_match_row_ids, matches)
         return final_matches
       else:
@@ -186,8 +186,8 @@ class ArgMaxMatcher(matcher.Matcher):
         return _match_when_rows_are_non_empty()
     else:
       return tf.cond(
-          tf.greater(tf.shape(similarity_matrix)[0], 0),
-          _match_when_rows_are_non_empty, _match_when_rows_are_empty)
+          pred=tf.greater(tf.shape(input=similarity_matrix)[0], 0),
+          true_fn=_match_when_rows_are_non_empty, false_fn=_match_when_rows_are_empty)
 
   def _set_values_using_indicator(self, x, indicator, val):
     """Set the indicated fields of x to val.
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index 9d60c541..74a234d3 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -160,7 +160,7 @@ class FasterRCNNFeatureExtractor(object):
       rpn_feature_map: A tensor with shape [batch, height, width, depth]
       activations: A dictionary mapping activation tensor names to tensors.
     """
-    with tf.variable_scope(scope, values=[preprocessed_inputs]):
+    with tf.compat.v1.variable_scope(scope, values=[preprocessed_inputs]):
       return self._extract_proposal_features(preprocessed_inputs, scope)
 
   @abstractmethod
@@ -182,8 +182,8 @@ class FasterRCNNFeatureExtractor(object):
         [batch_size * self.max_num_proposals, height, width, depth]
         representing box classifier features for each proposal.
     """
-    with tf.variable_scope(
-        scope, values=[proposal_feature_maps], reuse=tf.AUTO_REUSE):
+    with tf.compat.v1.variable_scope(
+        scope, values=[proposal_feature_maps], reuse=tf.compat.v1.AUTO_REUSE):
       return self._extract_box_classifier_features(proposal_feature_maps, scope)
 
   @abstractmethod
@@ -208,7 +208,7 @@ class FasterRCNNFeatureExtractor(object):
       the model graph.
     """
     variables_to_restore = {}
-    for variable in tf.global_variables():
+    for variable in tf.compat.v1.global_variables():
       for scope_name in [first_stage_feature_extractor_scope,
                          second_stage_feature_extractor_scope]:
         if variable.op.name.startswith(scope_name):
@@ -532,7 +532,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     """
     if inputs.dtype is not tf.float32:
       raise ValueError('`preprocess` expects a tf.float32 tensor')
-    with tf.name_scope('Preprocessor'):
+    with tf.compat.v1.name_scope('Preprocessor'):
       outputs = shape_utils.static_or_dynamic_map_fn(
           self._image_resizer_fn,
           elems=inputs,
@@ -559,9 +559,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
     """
     clip_heights = image_shapes[:, 0]
     clip_widths = image_shapes[:, 1]
-    clip_window = tf.to_float(tf.stack([tf.zeros_like(clip_heights),
+    clip_window = tf.cast(tf.stack([tf.zeros_like(clip_heights),
                                         tf.zeros_like(clip_heights),
-                                        clip_heights, clip_widths], axis=1))
+                                        clip_heights, clip_widths], axis=1), dtype=tf.float32)
     return clip_window
 
   def predict(self, preprocessed_inputs, true_image_shapes):
@@ -647,7 +647,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     # The Faster R-CNN paper recommends pruning anchors that venture outside
     # the image window at training time and clipping at inference time.
-    clip_window = tf.to_float(tf.stack([0, 0, image_shape[1], image_shape[2]]))
+    clip_window = tf.cast(tf.stack([0, 0, image_shape[1], image_shape[2]]), dtype=tf.float32)
     if self._is_training:
       if self.clip_anchors_to_image:
         anchors_boxlist = box_list_ops.clip_to_window(
@@ -878,8 +878,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
       detection_classes = detections_dict[
           fields.DetectionResultFields.detection_classes]
       rpn_features_to_crop = prediction_dict['rpn_features_to_crop']
-      batch_size = tf.shape(detection_boxes)[0]
-      max_detection = tf.shape(detection_boxes)[1]
+      batch_size = tf.shape(input=detection_boxes)[0]
+      max_detection = tf.shape(input=detection_boxes)[1]
       flattened_detected_feature_maps = (
           self._compute_second_stage_input_feature_maps(
               rpn_features_to_crop, detection_boxes))
@@ -927,9 +927,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
       masks: a 3-D float32 tensor with shape [K, mask_height, mask_width].
     """
     _, num_classes, height, width = instance_masks.get_shape().as_list()
-    k = tf.shape(instance_masks)[0]
+    k = tf.shape(input=instance_masks)[0]
     instance_masks = tf.reshape(instance_masks, [-1, height, width])
-    classes = tf.to_int32(tf.reshape(classes, [-1]))
+    classes = tf.cast(tf.reshape(classes, [-1]), dtype=tf.int32)
     gather_idx = tf.range(k) * num_classes + classes
     return tf.gather(instance_masks, gather_idx)
 
@@ -955,11 +955,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
         absolute coordinates.
       image_shape: A 1-D tensor representing the input image shape.
     """
-    image_shape = tf.shape(preprocessed_inputs)
+    image_shape = tf.shape(input=preprocessed_inputs)
     rpn_features_to_crop, _ = self._feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope=self.first_stage_feature_extractor_scope)
 
-    feature_map_shape = tf.shape(rpn_features_to_crop)
+    feature_map_shape = tf.shape(input=rpn_features_to_crop)
     anchors = box_list_ops.concatenate(
         self._first_stage_anchor_generator.generate([(feature_map_shape[1],
                                                       feature_map_shape[2])]))
@@ -1125,7 +1125,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
       ValueError: If `predict` is called before `preprocess`.
     """
 
-    with tf.name_scope('FirstStagePostprocessor'):
+    with tf.compat.v1.name_scope('FirstStagePostprocessor'):
       if self._number_of_stages == 1:
         proposal_boxes, proposal_scores, num_proposals = self._postprocess_rpn(
             prediction_dict['rpn_box_encodings'],
@@ -1137,11 +1137,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
             fields.DetectionResultFields.detection_boxes: proposal_boxes,
             fields.DetectionResultFields.detection_scores: proposal_scores,
             fields.DetectionResultFields.num_detections:
-                tf.to_float(num_proposals),
+                tf.cast(num_proposals, dtype=tf.float32),
         }
 
     # TODO(jrru): Remove mask_predictions from _post_process_box_classifier.
-    with tf.name_scope('SecondStagePostprocessor'):
+    with tf.compat.v1.name_scope('SecondStagePostprocessor'):
       if (self._number_of_stages == 2 or
           (self._number_of_stages == 3 and self._is_training)):
         mask_predictions = prediction_dict.get(box_predictor.MASK_PREDICTIONS)
@@ -1370,8 +1370,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
             self.groundtruth_lists(fields.BoxListFields.boxes))
     ]
     groundtruth_classes_with_background_list = [
-        tf.to_float(
-            tf.pad(one_hot_encoding, [[0, 0], [1, 0]], mode='CONSTANT'))
+        tf.cast(
+            tf.pad(tensor=one_hot_encoding, paddings=[[0, 0], [1, 0]], mode='CONSTANT'), dtype=tf.float32)
         for one_hot_encoding in self.groundtruth_lists(
             fields.BoxListFields.classes)]
 
@@ -1386,7 +1386,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
             # image_height, image_width]. Below we create a dummy image of the
             # the shape [image_height, image_width, 1] to use with
             # `image_resizer_fn`.
-            image=tf.zeros(tf.stack([tf.shape(mask)[1], tf.shape(mask)[2], 1])),
+            image=tf.zeros(tf.stack([tf.shape(input=mask)[1], tf.shape(input=mask)[2], 1])),
             masks=mask)
         resized_masks_list.append(resized_mask)
 
@@ -1398,7 +1398,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
       # Set weights for all batch elements equally to 1.0
       groundtruth_weights_list = []
       for groundtruth_classes in groundtruth_classes_with_background_list:
-        num_gt = tf.shape(groundtruth_classes)[0]
+        num_gt = tf.shape(input=groundtruth_classes)[0]
         groundtruth_weights = tf.ones(num_gt)
         groundtruth_weights_list.append(groundtruth_weights)
 
@@ -1438,7 +1438,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     # to cls_weights. This could happen as boxes within certain IOU ranges
     # are ignored. If triggered, the selected boxes will still be ignored
     # during loss computation.
-    positive_indicator = tf.greater(tf.argmax(cls_targets, axis=1), 0)
+    positive_indicator = tf.greater(tf.argmax(input=cls_targets, axis=1), 0)
     valid_indicator = tf.logical_and(
         tf.range(proposal_boxlist.num_boxes()) < num_valid_proposals,
         cls_weights > 0
@@ -1470,7 +1470,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     def get_box_inds(proposals):
       proposals_shape = proposals.get_shape().as_list()
       if any(dim is None for dim in proposals_shape):
-        proposals_shape = tf.shape(proposals)
+        proposals_shape = tf.shape(input=proposals)
       ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)
       multiplier = tf.expand_dims(
           tf.range(start=0, limit=proposals_shape[0]), 1)
@@ -1582,7 +1582,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         fields.DetectionResultFields.detection_boxes: nmsed_boxes,
         fields.DetectionResultFields.detection_scores: nmsed_scores,
         fields.DetectionResultFields.detection_classes: nmsed_classes,
-        fields.DetectionResultFields.num_detections: tf.to_float(num_detections)
+        fields.DetectionResultFields.num_detections: tf.cast(num_detections, dtype=tf.float32)
     }
     if nmsed_masks is not None:
       detections[fields.DetectionResultFields.detection_masks] = nmsed_masks
@@ -1650,7 +1650,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         'second_stage_classification_loss') to scalar tensors representing
         corresponding loss values.
     """
-    with tf.name_scope(scope, 'Loss', prediction_dict.values()):
+    with tf.compat.v1.name_scope(scope, 'Loss', prediction_dict.values()):
       (groundtruth_boxlists, groundtruth_classes_with_background_list,
        groundtruth_masks_list, groundtruth_weights_list
       ) = self._format_groundtruth_data(true_image_shapes)
@@ -1709,7 +1709,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         `first_stage_objectness_loss`) to scalar tensors representing
         corresponding loss values.
     """
-    with tf.name_scope('RPNLoss'):
+    with tf.compat.v1.name_scope('RPNLoss'):
       (batch_cls_targets, batch_cls_weights, batch_reg_targets,
        batch_reg_weights, _) = target_assigner.batch_assign_targets(
            target_assigner=self._proposal_target_assigner,
@@ -1724,17 +1724,17 @@ class FasterRCNNMetaArch(model.DetectionModel):
         return self._first_stage_sampler.subsample(
             tf.cast(cls_weights, tf.bool),
             self._first_stage_minibatch_size, tf.cast(cls_targets, tf.bool))
-      batch_sampled_indices = tf.to_float(shape_utils.static_or_dynamic_map_fn(
+      batch_sampled_indices = tf.cast(shape_utils.static_or_dynamic_map_fn(
           _minibatch_subsample_fn,
           [batch_cls_targets, batch_cls_weights],
           dtype=tf.bool,
           parallel_iterations=self._parallel_iterations,
-          back_prop=True))
+          back_prop=True), dtype=tf.float32)
 
       # Normalize by number of examples in sampled minibatch
-      normalizer = tf.reduce_sum(batch_sampled_indices, axis=1)
+      normalizer = tf.reduce_sum(input_tensor=batch_sampled_indices, axis=1)
       batch_one_hot_targets = tf.one_hot(
-          tf.to_int32(batch_cls_targets), depth=2)
+          tf.cast(batch_cls_targets, dtype=tf.int32), depth=2)
       sampled_reg_indices = tf.multiply(batch_sampled_indices,
                                         batch_reg_weights)
 
@@ -1744,9 +1744,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
           rpn_objectness_predictions_with_background,
           batch_one_hot_targets, weights=batch_sampled_indices)
       localization_loss = tf.reduce_mean(
-          tf.reduce_sum(localization_losses, axis=1) / normalizer)
+          input_tensor=tf.reduce_sum(input_tensor=localization_losses, axis=1) / normalizer)
       objectness_loss = tf.reduce_mean(
-          tf.reduce_sum(objectness_losses, axis=1) / normalizer)
+          input_tensor=tf.reduce_sum(input_tensor=objectness_losses, axis=1) / normalizer)
 
       localization_loss = tf.multiply(self._first_stage_loc_loss_weight,
                                       localization_loss,
@@ -1821,7 +1821,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         second_stage_mask_rcnn_box_predictor is True and
         `groundtruth_masks_list` is not provided.
     """
-    with tf.name_scope('BoxClassifierLoss'):
+    with tf.compat.v1.name_scope('BoxClassifierLoss'):
       paddings_indicator = self._padded_batched_proposals_indicator(
           num_proposals, self.max_num_proposals)
       proposal_boxlists = [
@@ -1829,8 +1829,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
           for proposal_boxes_single_image in tf.unstack(proposal_boxes)]
       batch_size = len(proposal_boxlists)
 
-      num_proposals_or_one = tf.to_float(tf.expand_dims(
-          tf.maximum(num_proposals, tf.ones_like(num_proposals)), 1))
+      num_proposals_or_one = tf.cast(tf.expand_dims(
+          tf.maximum(num_proposals, tf.ones_like(num_proposals)), 1), dtype=tf.float32)
       normalizer = tf.tile(num_proposals_or_one,
                            [1, self.max_num_proposals]) * batch_size
 
@@ -1852,7 +1852,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
           batch_cls_targets_with_background,
           [batch_size * self.max_num_proposals, -1])
       one_hot_flat_cls_targets_with_background = tf.argmax(
-          flat_cls_targets_with_background, axis=1)
+          input=flat_cls_targets_with_background, axis=1)
       one_hot_flat_cls_targets_with_background = tf.one_hot(
           one_hot_flat_cls_targets_with_background,
           flat_cls_targets_with_background.get_shape()[1])
@@ -1870,10 +1870,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
         # classes, but we now pad it to make it compatible with the class
         # predictions
         refined_box_encodings_with_background = tf.pad(
-            refined_box_encodings, [[0, 0], [1, 0], [0, 0]])
+            tensor=refined_box_encodings, paddings=[[0, 0], [1, 0], [0, 0]])
         refined_box_encodings_masked_by_class_targets = tf.boolean_mask(
-            refined_box_encodings_with_background,
-            tf.greater(one_hot_flat_cls_targets_with_background, 0))
+            tensor=refined_box_encodings_with_background,
+            mask=tf.greater(one_hot_flat_cls_targets_with_background, 0))
         reshaped_refined_box_encodings = tf.reshape(
             refined_box_encodings_masked_by_class_targets,
             [batch_size, self.max_num_proposals, self._box_coder.code_size])
@@ -1889,9 +1889,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
           ndims=2) / normalizer
 
       second_stage_loc_loss = tf.reduce_sum(
-          tf.boolean_mask(second_stage_loc_losses, paddings_indicator))
+          input_tensor=tf.boolean_mask(tensor=second_stage_loc_losses, mask=paddings_indicator))
       second_stage_cls_loss = tf.reduce_sum(
-          tf.boolean_mask(second_stage_cls_losses, paddings_indicator))
+          input_tensor=tf.boolean_mask(tensor=second_stage_cls_losses, mask=paddings_indicator))
 
       if self._hard_example_miner:
         (second_stage_loc_loss, second_stage_cls_loss
@@ -1933,10 +1933,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
           prediction_masks_masked_by_class_targets = prediction_masks
         else:
           prediction_masks_with_background = tf.pad(
-              prediction_masks, [[0, 0], [1, 0], [0, 0], [0, 0]])
+              tensor=prediction_masks, paddings=[[0, 0], [1, 0], [0, 0], [0, 0]])
           prediction_masks_masked_by_class_targets = tf.boolean_mask(
-              prediction_masks_with_background,
-              tf.greater(one_hot_flat_cls_targets_with_background, 0))
+              tensor=prediction_masks_with_background,
+              mask=tf.greater(one_hot_flat_cls_targets_with_background, 0))
 
         mask_height = prediction_masks.shape[2].value
         mask_width = prediction_masks.shape[3].value
@@ -1944,7 +1944,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
             prediction_masks_masked_by_class_targets,
             [batch_size, -1, mask_height * mask_width])
 
-        batch_mask_targets_shape = tf.shape(batch_mask_targets)
+        batch_mask_targets_shape = tf.shape(input=batch_mask_targets)
         flat_gt_masks = tf.reshape(batch_mask_targets,
                                    [-1, batch_mask_targets_shape[2],
                                     batch_mask_targets_shape[3]])
@@ -1972,10 +1972,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
             ndims=2) / (
                 mask_height * mask_width * tf.maximum(
                     tf.reduce_sum(
-                        batch_mask_target_weights, axis=1, keep_dims=True
+                        input_tensor=batch_mask_target_weights, axis=1, keepdims=True
                     ), tf.ones((batch_size, 1))))
         second_stage_mask_loss = tf.reduce_sum(
-            tf.boolean_mask(second_stage_mask_losses, paddings_indicator))
+            input_tensor=tf.boolean_mask(tensor=second_stage_mask_losses, mask=paddings_indicator))
 
       if second_stage_mask_loss is not None:
         mask_loss = tf.multiply(self._second_stage_mask_loss_weight,
@@ -1995,7 +1995,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     Returns:
       A Tensor of type tf.bool with shape [batch_size, max_num_proposals].
     """
-    batch_size = tf.size(num_proposals)
+    batch_size = tf.size(input=num_proposals)
     tiled_num_proposals = tf.tile(
         tf.expand_dims(num_proposals, 1), [1, max_num_proposals])
     tiled_proposal_index = tf.tile(
@@ -2078,7 +2078,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
           self.first_stage_feature_extractor_scope,
           self.second_stage_feature_extractor_scope)
 
-    variables_to_restore = tf.global_variables()
+    variables_to_restore = tf.compat.v1.global_variables()
     variables_to_restore.append(slim.get_or_create_global_step())
     # Only load feature extractor variables to be consistent with loading from
     # a classification checkpoint.
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
index 094e312b..873a733e 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
@@ -174,11 +174,11 @@ class FasterRCNNMetaArchTest(
             second_stage_batch_size=2,
             predict_masks=True,
             masks_are_class_agnostic=masks_are_class_agnostic)
-        preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)
+        preprocessed_inputs = tf.compat.v1.placeholder(tf.float32, shape=input_shape)
         _, true_image_shapes = model.preprocess(preprocessed_inputs)
         result_tensor_dict = model.predict(preprocessed_inputs,
                                            true_image_shapes)
-        init_op = tf.global_variables_initializer()
+        init_op = tf.compat.v1.global_variables_initializer()
       with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
         tensor_dict_out = sess.run(result_tensor_dict, feed_dict={
@@ -257,7 +257,7 @@ class FasterRCNNMetaArchTest(
           'mask_predictions': (2 * max_num_proposals, mask_shape_1, 14, 14)
       }
 
-      init_op = tf.global_variables_initializer()
+      init_op = tf.compat.v1.global_variables_initializer()
       with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
         tensor_dict_out = sess.run(result_tensor_dict)
@@ -315,15 +315,15 @@ class FasterRCNNMetaArchTest(
         class_predictions_with_background = np.ones(
             [total_num_padded_proposals, model.num_classes+1])
 
-        num_proposals_placeholder = tf.placeholder(tf.int32,
+        num_proposals_placeholder = tf.compat.v1.placeholder(tf.int32,
                                                    shape=num_proposals_shape)
-        refined_box_encodings_placeholder = tf.placeholder(
+        refined_box_encodings_placeholder = tf.compat.v1.placeholder(
             tf.float32, shape=refined_box_encoding_shape)
-        class_predictions_with_background_placeholder = tf.placeholder(
+        class_predictions_with_background_placeholder = tf.compat.v1.placeholder(
             tf.float32, shape=class_predictions_with_background_shape)
-        proposal_boxes_placeholder = tf.placeholder(
+        proposal_boxes_placeholder = tf.compat.v1.placeholder(
             tf.float32, shape=proposal_boxes_shape)
-        image_shape_placeholder = tf.placeholder(tf.int32, shape=(4))
+        image_shape_placeholder = tf.compat.v1.placeholder(tf.int32, shape=(4))
         _, true_image_shapes = model.preprocess(
             tf.zeros(image_shape_placeholder))
         detections = model.postprocess({
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index 299bc147..db045384 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -49,13 +49,13 @@ class FakeFasterRCNNFeatureExtractor(
     return tf.identity(resized_inputs)
 
   def _extract_proposal_features(self, preprocessed_inputs, scope):
-    with tf.variable_scope('mock_model'):
+    with tf.compat.v1.variable_scope('mock_model'):
       proposal_features = 0 * slim.conv2d(
           preprocessed_inputs, num_outputs=3, kernel_size=1, scope='layer1')
       return proposal_features, {}
 
   def _extract_box_classifier_features(self, proposal_feature_maps, scope):
-    with tf.variable_scope('mock_model'):
+    with tf.compat.v1.variable_scope('mock_model'):
       return 0 * slim.conv2d(proposal_feature_maps,
                              num_outputs=3, kernel_size=1, scope='layer2')
 
@@ -171,14 +171,14 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       if masks is not None:
         resized_masks = tf.identity(masks)
         if pad_to_max_dimension is not None:
-          resized_masks = tf.image.pad_to_bounding_box(tf.transpose(masks,
-                                                                    [1, 2, 0]),
+          resized_masks = tf.image.pad_to_bounding_box(tf.transpose(a=masks,
+                                                                    perm=[1, 2, 0]),
                                                        0, 0,
                                                        pad_to_max_dimension,
                                                        pad_to_max_dimension)
-          resized_masks = tf.transpose(resized_masks, [2, 0, 1])
+          resized_masks = tf.transpose(a=resized_masks, perm=[2, 0, 1])
         resized_inputs.append(resized_masks)
-      resized_inputs.append(tf.shape(image))
+      resized_inputs.append(tf.shape(input=image))
       return resized_inputs
 
     # anchors in this test are designed so that a subset of anchors are inside
@@ -327,7 +327,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       input_image_shape = (batch_size, height, width, 3)
 
       _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
-      preprocessed_inputs = tf.placeholder(
+      preprocessed_inputs = tf.compat.v1.placeholder(
           dtype=tf.float32, shape=(batch_size, None, None, 3))
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
 
@@ -349,7 +349,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
           'anchors': (expected_num_anchors, 4)
       }
 
-      init_op = tf.global_variables_initializer()
+      init_op = tf.compat.v1.global_variables_initializer()
       with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
         prediction_out = sess.run(prediction_dict,
@@ -382,7 +382,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       width = 12
       input_image_shape = (batch_size, height, width, 3)
       _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
-      preprocessed_inputs = tf.placeholder(
+      preprocessed_inputs = tf.compat.v1.placeholder(
           dtype=tf.float32, shape=(batch_size, None, None, 3))
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
 
@@ -395,7 +395,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       # a strict upper bound on the number of anchors.
       num_anchors_strict_upper_bound = height * width * 3 * 3
 
-      init_op = tf.global_variables_initializer()
+      init_op = tf.compat.v1.global_variables_initializer()
       with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
         prediction_out = sess.run(prediction_dict,
@@ -469,11 +469,11 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
             number_of_stages=2,
             second_stage_batch_size=2,
             predict_masks=False)
-        preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)
+        preprocessed_inputs = tf.compat.v1.placeholder(tf.float32, shape=input_shape)
         _, true_image_shapes = model.preprocess(preprocessed_inputs)
         result_tensor_dict = model.predict(
             preprocessed_inputs, true_image_shapes)
-        init_op = tf.global_variables_initializer()
+        init_op = tf.compat.v1.global_variables_initializer()
       with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
         tensor_dict_out = sess.run(result_tensor_dict, feed_dict={
@@ -540,7 +540,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
                                                   3)
       }
 
-      init_op = tf.global_variables_initializer()
+      init_op = tf.compat.v1.global_variables_initializer()
       with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
         tensor_dict_out = sess.run(result_tensor_dict)
@@ -746,15 +746,15 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         class_predictions_with_background = np.ones(
             [total_num_padded_proposals, model.num_classes+1])
 
-        num_proposals_placeholder = tf.placeholder(tf.int32,
+        num_proposals_placeholder = tf.compat.v1.placeholder(tf.int32,
                                                    shape=num_proposals_shape)
-        refined_box_encodings_placeholder = tf.placeholder(
+        refined_box_encodings_placeholder = tf.compat.v1.placeholder(
             tf.float32, shape=refined_box_encoding_shape)
-        class_predictions_with_background_placeholder = tf.placeholder(
+        class_predictions_with_background_placeholder = tf.compat.v1.placeholder(
             tf.float32, shape=class_predictions_with_background_shape)
-        proposal_boxes_placeholder = tf.placeholder(
+        proposal_boxes_placeholder = tf.compat.v1.placeholder(
             tf.float32, shape=proposal_boxes_shape)
-        image_shape_placeholder = tf.placeholder(tf.int32, shape=(4))
+        image_shape_placeholder = tf.compat.v1.placeholder(tf.int32, shape=(4))
 
         detections = model.postprocess({
             'refined_box_encodings': refined_box_encodings_placeholder,
@@ -795,7 +795,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     for image_shape in image_shapes:
       model = self._build_model(
           is_training=False, number_of_stages=2, second_stage_batch_size=6)
-      image_placeholder = tf.placeholder(tf.float32, shape=image_shape)
+      image_placeholder = tf.compat.v1.placeholder(tf.float32, shape=image_shape)
       preprocessed_inputs, _ = model.preprocess(image_placeholder)
       self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)
 
@@ -930,9 +930,9 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     # proposal. Thus, if mask_predictions_logits element values are all greater
     # than 20, the loss should be zero.
     groundtruth_masks_list = [
-        tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32),
-        tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32),
-        tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32)
+        tf.convert_to_tensor(value=np.ones((2, 32, 32)), dtype=tf.float32),
+        tf.convert_to_tensor(value=np.ones((2, 32, 32)), dtype=tf.float32),
+        tf.convert_to_tensor(value=np.ones((2, 32, 32)), dtype=tf.float32)
     ]
     groundtruth_weights_list = [
         tf.constant([1, 1], dtype=tf.float32),
@@ -1028,7 +1028,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     # crops of the groundtruth masks should return a mask that covers the entire
     # proposal. Thus, if mask_predictions_logits element values are all greater
     # than 20, the loss should be zero.
-    groundtruth_masks_list = [tf.convert_to_tensor(np.ones((1, 32, 32)),
+    groundtruth_masks_list = [tf.convert_to_tensor(value=np.ones((1, 32, 32)),
                                                    dtype=tf.float32)]
 
     prediction_dict = {
@@ -1126,7 +1126,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     # crops of the groundtruth masks should return a mask that covers the entire
     # proposal. Thus, if mask_predictions_logits element values are all greater
     # than 20, the loss should be zero.
-    groundtruth_masks_list = [tf.convert_to_tensor(np.ones((1, 32, 32)),
+    groundtruth_masks_list = [tf.convert_to_tensor(value=np.ones((1, 32, 32)),
                                                    dtype=tf.float32)]
 
     prediction_dict = {
@@ -1414,9 +1414,9 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     # crops of the groundtruth masks should return a mask that covers the entire
     # proposal. Thus, if mask_predictions_logits element values are all greater
     # than 20, the loss should be zero.
-    groundtruth_masks_list = [tf.convert_to_tensor(np.ones((2, 32, 32)),
+    groundtruth_masks_list = [tf.convert_to_tensor(value=np.ones((2, 32, 32)),
                                                    dtype=tf.float32),
-                              tf.convert_to_tensor(np.ones((2, 32, 32)),
+                              tf.convert_to_tensor(value=np.ones((2, 32, 32)),
                                                    dtype=tf.float32)]
     prediction_dict = {
         'rpn_box_encodings': rpn_box_encodings,
@@ -1450,13 +1450,13 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     # Define mock tensorflow classification graph and save variables.
     test_graph_classification = tf.Graph()
     with test_graph_classification.as_default():
-      image = tf.placeholder(dtype=tf.float32, shape=[1, 20, 20, 3])
-      with tf.variable_scope('mock_model'):
+      image = tf.compat.v1.placeholder(dtype=tf.float32, shape=[1, 20, 20, 3])
+      with tf.compat.v1.variable_scope('mock_model'):
         net = slim.conv2d(image, num_outputs=3, kernel_size=1, scope='layer1')
         slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
 
-      init_op = tf.global_variables_initializer()
-      saver = tf.train.Saver()
+      init_op = tf.compat.v1.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver()
       save_path = self.get_temp_dir()
       with self.test_session(graph=test_graph_classification) as sess:
         sess.run(init_op)
@@ -1470,17 +1470,17 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
           is_training=False, number_of_stages=2, second_stage_batch_size=6)
 
       inputs_shape = (2, 20, 20, 3)
-      inputs = tf.to_float(tf.random_uniform(
-          inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      inputs = tf.cast(tf.random.uniform(
+          inputs_shape, minval=0, maxval=255, dtype=tf.int32), dtype=tf.float32)
       preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       model.postprocess(prediction_dict, true_image_shapes)
       var_map = model.restore_map(fine_tune_checkpoint_type='classification')
       self.assertIsInstance(var_map, dict)
-      saver = tf.train.Saver(var_map)
+      saver = tf.compat.v1.train.Saver(var_map)
       with self.test_session(graph=test_graph_classification) as sess:
         saver.restore(sess, saved_model_path)
-        for var in sess.run(tf.report_uninitialized_variables()):
+        for var in sess.run(tf.compat.v1.report_uninitialized_variables()):
           self.assertNotIn(model.first_stage_feature_extractor_scope, var)
           self.assertNotIn(model.second_stage_feature_extractor_scope, var)
 
@@ -1491,14 +1491,14 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       model = self._build_model(
           is_training=False, number_of_stages=2, second_stage_batch_size=6)
       inputs_shape = (2, 20, 20, 3)
-      inputs = tf.to_float(tf.random_uniform(
-          inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      inputs = tf.cast(tf.random.uniform(
+          inputs_shape, minval=0, maxval=255, dtype=tf.int32), dtype=tf.float32)
       preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       model.postprocess(prediction_dict, true_image_shapes)
       another_variable = tf.Variable([17.0], name='another_variable')  # pylint: disable=unused-variable
-      init_op = tf.global_variables_initializer()
-      saver = tf.train.Saver()
+      init_op = tf.compat.v1.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver()
       save_path = self.get_temp_dir()
       with self.test_session(graph=test_graph_detection1) as sess:
         sess.run(init_op)
@@ -1511,18 +1511,18 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
                                  second_stage_batch_size=6, num_classes=42)
 
       inputs_shape2 = (2, 20, 20, 3)
-      inputs2 = tf.to_float(tf.random_uniform(
-          inputs_shape2, minval=0, maxval=255, dtype=tf.int32))
+      inputs2 = tf.cast(tf.random.uniform(
+          inputs_shape2, minval=0, maxval=255, dtype=tf.int32), dtype=tf.float32)
       preprocessed_inputs2, true_image_shapes = model2.preprocess(inputs2)
       prediction_dict2 = model2.predict(preprocessed_inputs2, true_image_shapes)
       model2.postprocess(prediction_dict2, true_image_shapes)
       another_variable = tf.Variable([17.0], name='another_variable')  # pylint: disable=unused-variable
       var_map = model2.restore_map(fine_tune_checkpoint_type='detection')
       self.assertIsInstance(var_map, dict)
-      saver = tf.train.Saver(var_map)
+      saver = tf.compat.v1.train.Saver(var_map)
       with self.test_session(graph=test_graph_detection2) as sess:
         saver.restore(sess, saved_model_path)
-        uninitialized_vars_list = sess.run(tf.report_uninitialized_variables())
+        uninitialized_vars_list = sess.run(tf.compat.v1.report_uninitialized_variables())
         self.assertIn('another_variable', uninitialized_vars_list)
         for var in uninitialized_vars_list:
           self.assertNotIn(model2.first_stage_feature_extractor_scope, var)
@@ -1538,8 +1538,8 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
           num_classes=42)
 
       inputs_shape = (2, 20, 20, 3)
-      inputs = tf.to_float(
-          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      inputs = tf.cast(
+          tf.random.uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32), dtype=tf.float32)
       preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       model.postprocess(prediction_dict, true_image_shapes)
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index e2f60aab..7211c040 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -415,7 +415,7 @@ class SSDMetaArch(model.DetectionModel):
     """
     if inputs.dtype is not tf.float32:
       raise ValueError('`preprocess` expects a tf.float32 tensor')
-    with tf.name_scope('Preprocessor'):
+    with tf.compat.v1.name_scope('Preprocessor'):
       # TODO(jonathanhuang): revisit whether to always use batch size as
       # the number of parallel iterations vs allow for dynamic batching.
       outputs = shape_utils.static_or_dynamic_map_fn(
@@ -456,9 +456,9 @@ class SSDMetaArch(model.DetectionModel):
     resized_inputs_shape = shape_utils.combined_static_and_dynamic_shape(
         preprocessed_images)
     true_heights, true_widths, _ = tf.unstack(
-        tf.to_float(true_image_shapes), axis=1)
-    padded_height = tf.to_float(resized_inputs_shape[1])
-    padded_width = tf.to_float(resized_inputs_shape[2])
+        tf.cast(true_image_shapes, dtype=tf.float32), axis=1)
+    padded_height = tf.cast(resized_inputs_shape[1], dtype=tf.float32)
+    padded_width = tf.cast(resized_inputs_shape[2], dtype=tf.float32)
     return tf.stack(
         [
             tf.zeros_like(true_heights),
@@ -500,7 +500,7 @@ class SSDMetaArch(model.DetectionModel):
           the generated anchors in normalized coordinates.
     """
     batchnorm_updates_collections = (None if self._inplace_batchnorm_update
-                                     else tf.GraphKeys.UPDATE_OPS)
+                                     else tf.compat.v1.GraphKeys.UPDATE_OPS)
     if self._feature_extractor.is_keras_model:
       feature_maps = self._feature_extractor(preprocessed_inputs)
     else:
@@ -508,7 +508,7 @@ class SSDMetaArch(model.DetectionModel):
                           is_training=(self._is_training and
                                        not self._freeze_batchnorm),
                           updates_collections=batchnorm_updates_collections):
-        with tf.variable_scope(None, self._extract_features_scope,
+        with tf.compat.v1.variable_scope(None, self._extract_features_scope,
                                [preprocessed_inputs]):
           feature_maps = self._feature_extractor.extract_features(
               preprocessed_inputs)
@@ -607,7 +607,7 @@ class SSDMetaArch(model.DetectionModel):
     if ('box_encodings' not in prediction_dict or
         'class_predictions_with_background' not in prediction_dict):
       raise ValueError('prediction_dict does not contain expected entries.')
-    with tf.name_scope('Postprocessor'):
+    with tf.compat.v1.name_scope('Postprocessor'):
       preprocessed_images = prediction_dict['preprocessed_inputs']
       box_encodings = prediction_dict['box_encodings']
       box_encodings = tf.identity(box_encodings, 'raw_box_encodings')
@@ -639,7 +639,7 @@ class SSDMetaArch(model.DetectionModel):
           fields.DetectionResultFields.detection_scores: nmsed_scores,
           fields.DetectionResultFields.detection_classes: nmsed_classes,
           fields.DetectionResultFields.num_detections:
-              tf.to_float(num_detections)
+              tf.cast(num_detections, dtype=tf.float32)
       }
       if (nmsed_additional_fields is not None and
           fields.BoxListFields.keypoints in nmsed_additional_fields):
@@ -672,7 +672,7 @@ class SSDMetaArch(model.DetectionModel):
         `classification_loss`) to scalar tensors representing corresponding loss
         values.
     """
-    with tf.name_scope(scope, 'Loss', prediction_dict.values()):
+    with tf.compat.v1.name_scope(scope, 'Loss', prediction_dict.values()):
       keypoints = None
       if self.groundtruth_has_field(fields.BoxListFields.keypoints):
         keypoints = self.groundtruth_lists(fields.BoxListFields.keypoints)
@@ -689,13 +689,13 @@ class SSDMetaArch(model.DetectionModel):
             self.groundtruth_lists(fields.BoxListFields.boxes), match_list)
 
       if self._random_example_sampler:
-        batch_sampled_indicator = tf.to_float(
+        batch_sampled_indicator = tf.cast(
             shape_utils.static_or_dynamic_map_fn(
                 self._minibatch_subsample_fn,
                 [batch_cls_targets, batch_cls_weights],
                 dtype=tf.bool,
                 parallel_iterations=self._parallel_iterations,
-                back_prop=True))
+                back_prop=True), dtype=tf.float32)
         batch_reg_weights = tf.multiply(batch_sampled_indicator,
                                         batch_reg_weights)
         batch_cls_weights = tf.multiply(batch_sampled_indicator,
@@ -724,8 +724,8 @@ class SSDMetaArch(model.DetectionModel):
         cls_losses = self._expected_classification_loss_under_sampling(
             batch_cls_targets, cls_losses)
 
-        classification_loss = tf.reduce_sum(cls_losses)
-        localization_loss = tf.reduce_sum(location_losses)
+        classification_loss = tf.reduce_sum(input_tensor=cls_losses)
+        localization_loss = tf.reduce_sum(input_tensor=location_losses)
       elif self._hard_example_miner:
         cls_losses = ops.reduce_sum_trailing_dimensions(cls_losses, ndims=2)
         (localization_loss, classification_loss) = self._apply_hard_mining(
@@ -735,18 +735,18 @@ class SSDMetaArch(model.DetectionModel):
       else:
         cls_losses = ops.reduce_sum_trailing_dimensions(cls_losses, ndims=2)
         if self._add_summaries:
-          class_ids = tf.argmax(batch_cls_targets, axis=2)
+          class_ids = tf.argmax(input=batch_cls_targets, axis=2)
           flattened_class_ids = tf.reshape(class_ids, [-1])
           flattened_classification_losses = tf.reshape(cls_losses, [-1])
           self._summarize_anchor_classification_loss(
               flattened_class_ids, flattened_classification_losses)
-        localization_loss = tf.reduce_sum(location_losses)
-        classification_loss = tf.reduce_sum(cls_losses)
+        localization_loss = tf.reduce_sum(input_tensor=location_losses)
+        classification_loss = tf.reduce_sum(input_tensor=cls_losses)
 
       # Optionally normalize by number of positive matches
       normalizer = tf.constant(1.0, dtype=tf.float32)
       if self._normalize_loss_by_num_matches:
-        normalizer = tf.maximum(tf.to_float(tf.reduce_sum(batch_reg_weights)),
+        normalizer = tf.maximum(tf.cast(tf.reduce_sum(input_tensor=batch_reg_weights), dtype=tf.float32),
                                 1.0)
 
       localization_loss_normalizer = normalizer
@@ -786,19 +786,19 @@ class SSDMetaArch(model.DetectionModel):
       background_class = tf.zeros_like(tf.slice(cls_targets, [0, 0], [-1, 1]))
       regular_class = tf.slice(cls_targets, [0, 1], [-1, -1])
       cls_targets = tf.concat([background_class, regular_class], 1)
-    positives_indicator = tf.reduce_sum(cls_targets, axis=1)
+    positives_indicator = tf.reduce_sum(input_tensor=cls_targets, axis=1)
     return self._random_example_sampler.subsample(
         tf.cast(cls_weights, tf.bool),
         batch_size=None,
         labels=tf.cast(positives_indicator, tf.bool))
 
   def _summarize_anchor_classification_loss(self, class_ids, cls_losses):
-    positive_indices = tf.where(tf.greater(class_ids, 0))
+    positive_indices = tf.compat.v1.where(tf.greater(class_ids, 0))
     positive_anchor_cls_loss = tf.squeeze(
         tf.gather(cls_losses, positive_indices), axis=1)
     visualization_utils.add_cdf_image_summary(positive_anchor_cls_loss,
                                               'PositiveAnchorLossCDF')
-    negative_indices = tf.where(tf.equal(class_ids, 0))
+    negative_indices = tf.compat.v1.where(tf.equal(class_ids, 0))
     negative_anchor_cls_loss = tf.squeeze(
         tf.gather(cls_losses, negative_indices), axis=1)
     visualization_utils.add_cdf_image_summary(negative_anchor_cls_loss,
@@ -843,7 +843,7 @@ class SSDMetaArch(model.DetectionModel):
     ]
     if self._add_background_class:
       groundtruth_classes_with_background_list = [
-          tf.pad(one_hot_encoding, [[0, 0], [1, 0]], mode='CONSTANT')
+          tf.pad(tensor=one_hot_encoding, paddings=[[0, 0], [1, 0]], mode='CONSTANT')
           for one_hot_encoding in groundtruth_classes_list
       ]
     else:
@@ -875,24 +875,24 @@ class SSDMetaArch(model.DetectionModel):
         and columns corresponding to anchors.
     """
     num_boxes_per_image = tf.stack(
-        [tf.shape(x)[0] for x in groundtruth_boxes_list])
+        [tf.shape(input=x)[0] for x in groundtruth_boxes_list])
     pos_anchors_per_image = tf.stack(
         [match.num_matched_columns() for match in match_list])
     neg_anchors_per_image = tf.stack(
         [match.num_unmatched_columns() for match in match_list])
     ignored_anchors_per_image = tf.stack(
         [match.num_ignored_columns() for match in match_list])
-    tf.summary.scalar('AvgNumGroundtruthBoxesPerImage',
-                      tf.reduce_mean(tf.to_float(num_boxes_per_image)),
+    tf.compat.v1.summary.scalar('AvgNumGroundtruthBoxesPerImage',
+                      tf.reduce_mean(input_tensor=tf.cast(num_boxes_per_image, dtype=tf.float32)),
                       family='TargetAssignment')
-    tf.summary.scalar('AvgNumPositiveAnchorsPerImage',
-                      tf.reduce_mean(tf.to_float(pos_anchors_per_image)),
+    tf.compat.v1.summary.scalar('AvgNumPositiveAnchorsPerImage',
+                      tf.reduce_mean(input_tensor=tf.cast(pos_anchors_per_image, dtype=tf.float32)),
                       family='TargetAssignment')
-    tf.summary.scalar('AvgNumNegativeAnchorsPerImage',
-                      tf.reduce_mean(tf.to_float(neg_anchors_per_image)),
+    tf.compat.v1.summary.scalar('AvgNumNegativeAnchorsPerImage',
+                      tf.reduce_mean(input_tensor=tf.cast(neg_anchors_per_image, dtype=tf.float32)),
                       family='TargetAssignment')
-    tf.summary.scalar('AvgNumIgnoredAnchorsPerImage',
-                      tf.reduce_mean(tf.to_float(ignored_anchors_per_image)),
+    tf.compat.v1.summary.scalar('AvgNumIgnoredAnchorsPerImage',
+                      tf.reduce_mean(input_tensor=tf.cast(ignored_anchors_per_image, dtype=tf.float32)),
                       family='TargetAssignment')
 
   def _apply_hard_mining(self, location_losses, cls_losses, prediction_dict,
@@ -1004,7 +1004,7 @@ class SSDMetaArch(model.DetectionModel):
       raise ValueError('Not supported fine_tune_checkpoint_type: {}'.format(
           fine_tune_checkpoint_type))
     variables_to_restore = {}
-    for variable in tf.global_variables():
+    for variable in tf.compat.v1.global_variables():
       var_name = variable.op.name
       if (fine_tune_checkpoint_type == 'detection' and
           load_all_detection_checkpoint_vars):
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index baf5c336..e69de29b 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -1,723 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-
-"""Tests for object_detection.meta_architectures.ssd_meta_arch."""
-import functools
-from absl.testing import parameterized
-
-import numpy as np
-import tensorflow as tf
-
-from object_detection.core import anchor_generator
-from object_detection.core import balanced_positive_negative_sampler as sampler
-from object_detection.core import box_list
-from object_detection.core import losses
-from object_detection.core import post_processing
-from object_detection.core import region_similarity_calculator as sim_calc
-from object_detection.core import target_assigner
-from object_detection.meta_architectures import ssd_meta_arch
-from object_detection.utils import ops
-from object_detection.utils import test_case
-from object_detection.utils import test_utils
-
-slim = tf.contrib.slim
-keras = tf.keras.layers
-
-
-class FakeSSDFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
-
-  def __init__(self):
-    super(FakeSSDFeatureExtractor, self).__init__(
-        is_training=True,
-        depth_multiplier=0,
-        min_depth=0,
-        pad_to_multiple=1,
-        conv_hyperparams_fn=None)
-
-  def preprocess(self, resized_inputs):
-    return tf.identity(resized_inputs)
-
-  def extract_features(self, preprocessed_inputs):
-    with tf.variable_scope('mock_model'):
-      features = slim.conv2d(inputs=preprocessed_inputs, num_outputs=32,
-                             kernel_size=1, scope='layer1')
-      return [features]
-
-
-class FakeSSDKerasFeatureExtractor(ssd_meta_arch.SSDKerasFeatureExtractor):
-
-  def __init__(self):
-    with tf.name_scope('mock_model'):
-      super(FakeSSDKerasFeatureExtractor, self).__init__(
-          is_training=True,
-          depth_multiplier=0,
-          min_depth=0,
-          pad_to_multiple=1,
-          conv_hyperparams_config=None,
-          freeze_batchnorm=False,
-          inplace_batchnorm_update=False,
-      )
-
-      self._conv = keras.Conv2D(filters=32, kernel_size=1, name='layer1')
-
-  def preprocess(self, resized_inputs):
-    return tf.identity(resized_inputs)
-
-  def _extract_features(self, preprocessed_inputs, **kwargs):
-    with tf.name_scope('mock_model'):
-      return [self._conv(preprocessed_inputs)]
-
-
-class MockAnchorGenerator2x2(anchor_generator.AnchorGenerator):
-  """Sets up a simple 2x2 anchor grid on the unit square."""
-
-  def name_scope(self):
-    return 'MockAnchorGenerator'
-
-  def num_anchors_per_location(self):
-    return [1]
-
-  def _generate(self, feature_map_shape_list, im_height, im_width):
-    return [box_list.BoxList(
-        tf.constant([[0, 0, .5, .5],
-                     [0, .5, .5, 1],
-                     [.5, 0, 1, .5],
-                     [1., 1., 1.5, 1.5]  # Anchor that is outside clip_window.
-                    ], tf.float32))]
-
-  def num_anchors(self):
-    return 4
-
-
-def _get_value_for_matching_key(dictionary, suffix):
-  for key in dictionary.keys():
-    if key.endswith(suffix):
-      return dictionary[key]
-  raise ValueError('key not found {}'.format(suffix))
-
-
-@parameterized.parameters(
-    {'use_keras': False},
-    {'use_keras': True},
-)
-class SsdMetaArchTest(test_case.TestCase, parameterized.TestCase):
-
-  def _create_model(self,
-                    apply_hard_mining=True,
-                    normalize_loc_loss_by_codesize=False,
-                    add_background_class=True,
-                    random_example_sampling=False,
-                    weight_regression_loss_by_score=False,
-                    use_expected_classification_loss_under_sampling=False,
-                    minimum_negative_sampling=1,
-                    desired_negative_sampling_ratio=3,
-                    use_keras=False):
-    is_training = False
-    num_classes = 1
-    mock_anchor_generator = MockAnchorGenerator2x2()
-    if use_keras:
-      mock_box_predictor = test_utils.MockKerasBoxPredictor(
-          is_training, num_classes)
-    else:
-      mock_box_predictor = test_utils.MockBoxPredictor(
-          is_training, num_classes)
-    mock_box_coder = test_utils.MockBoxCoder()
-    if use_keras:
-      fake_feature_extractor = FakeSSDKerasFeatureExtractor()
-    else:
-      fake_feature_extractor = FakeSSDFeatureExtractor()
-    mock_matcher = test_utils.MockMatcher()
-    region_similarity_calculator = sim_calc.IouSimilarity()
-    encode_background_as_zeros = False
-    def image_resizer_fn(image):
-      return [tf.identity(image), tf.shape(image)]
-
-    classification_loss = losses.WeightedSigmoidClassificationLoss()
-    localization_loss = losses.WeightedSmoothL1LocalizationLoss()
-    non_max_suppression_fn = functools.partial(
-        post_processing.batch_multiclass_non_max_suppression,
-        score_thresh=-20.0,
-        iou_thresh=1.0,
-        max_size_per_class=5,
-        max_total_size=5)
-    classification_loss_weight = 1.0
-    localization_loss_weight = 1.0
-    negative_class_weight = 1.0
-    normalize_loss_by_num_matches = False
-
-    hard_example_miner = None
-    if apply_hard_mining:
-      # This hard example miner is expected to be a no-op.
-      hard_example_miner = losses.HardExampleMiner(
-          num_hard_examples=None,
-          iou_threshold=1.0)
-
-    random_example_sampler = None
-    if random_example_sampling:
-      random_example_sampler = sampler.BalancedPositiveNegativeSampler(
-          positive_fraction=0.5)
-
-    target_assigner_instance = target_assigner.TargetAssigner(
-        region_similarity_calculator,
-        mock_matcher,
-        mock_box_coder,
-        negative_class_weight=negative_class_weight,
-        weight_regression_loss_by_score=weight_regression_loss_by_score)
-
-    expected_classification_loss_under_sampling = None
-    if use_expected_classification_loss_under_sampling:
-      expected_classification_loss_under_sampling = functools.partial(
-          ops.expected_classification_loss_under_sampling,
-          minimum_negative_sampling=minimum_negative_sampling,
-          desired_negative_sampling_ratio=desired_negative_sampling_ratio)
-
-    code_size = 4
-    model = ssd_meta_arch.SSDMetaArch(
-        is_training,
-        mock_anchor_generator,
-        mock_box_predictor,
-        mock_box_coder,
-        fake_feature_extractor,
-        mock_matcher,
-        region_similarity_calculator,
-        encode_background_as_zeros,
-        negative_class_weight,
-        image_resizer_fn,
-        non_max_suppression_fn,
-        tf.identity,
-        classification_loss,
-        localization_loss,
-        classification_loss_weight,
-        localization_loss_weight,
-        normalize_loss_by_num_matches,
-        hard_example_miner,
-        target_assigner_instance=target_assigner_instance,
-        add_summaries=False,
-        normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize,
-        freeze_batchnorm=False,
-        inplace_batchnorm_update=False,
-        add_background_class=add_background_class,
-        random_example_sampler=random_example_sampler,
-        expected_classification_loss_under_sampling=
-        expected_classification_loss_under_sampling)
-    return model, num_classes, mock_anchor_generator.num_anchors(), code_size
-
-  def test_preprocess_preserves_shapes_with_dynamic_input_image(
-      self, use_keras):
-    image_shapes = [(3, None, None, 3),
-                    (None, 10, 10, 3),
-                    (None, None, None, 3)]
-    model, _, _, _ = self._create_model(use_keras=use_keras)
-    for image_shape in image_shapes:
-      image_placeholder = tf.placeholder(tf.float32, shape=image_shape)
-      preprocessed_inputs, _ = model.preprocess(image_placeholder)
-      self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)
-
-  def test_preprocess_preserves_shape_with_static_input_image(self, use_keras):
-    def graph_fn(input_image):
-      model, _, _, _ = self._create_model(use_keras=use_keras)
-      return model.preprocess(input_image)
-    input_image = np.random.rand(2, 3, 3, 3).astype(np.float32)
-    preprocessed_inputs, _ = self.execute(graph_fn, [input_image])
-    self.assertAllEqual(preprocessed_inputs.shape, [2, 3, 3, 3])
-
-  def test_predict_result_shapes_on_image_with_dynamic_shape(self, use_keras):
-    batch_size = 3
-    image_size = 2
-    input_shapes = [(None, image_size, image_size, 3),
-                    (batch_size, None, None, 3),
-                    (None, None, None, 3)]
-
-    for input_shape in input_shapes:
-      tf_graph = tf.Graph()
-      with tf_graph.as_default():
-        model, num_classes, num_anchors, code_size = self._create_model(
-            use_keras=use_keras)
-        preprocessed_input_placeholder = tf.placeholder(tf.float32,
-                                                        shape=input_shape)
-        prediction_dict = model.predict(
-            preprocessed_input_placeholder, true_image_shapes=None)
-
-        self.assertIn('box_encodings', prediction_dict)
-        self.assertIn('class_predictions_with_background', prediction_dict)
-        self.assertIn('feature_maps', prediction_dict)
-        self.assertIn('anchors', prediction_dict)
-
-        init_op = tf.global_variables_initializer()
-      with self.test_session(graph=tf_graph) as sess:
-        sess.run(init_op)
-        prediction_out = sess.run(prediction_dict,
-                                  feed_dict={
-                                      preprocessed_input_placeholder:
-                                      np.random.uniform(
-                                          size=(batch_size, 2, 2, 3))})
-      expected_box_encodings_shape_out = (batch_size, num_anchors, code_size)
-      expected_class_predictions_with_background_shape_out = (batch_size,
-                                                              num_anchors,
-                                                              num_classes + 1)
-
-      self.assertAllEqual(prediction_out['box_encodings'].shape,
-                          expected_box_encodings_shape_out)
-      self.assertAllEqual(
-          prediction_out['class_predictions_with_background'].shape,
-          expected_class_predictions_with_background_shape_out)
-
-  def test_predict_result_shapes_on_image_with_static_shape(self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, code_size = self._create_model(
-          use_keras=use_keras)
-
-    def graph_fn(input_image):
-      model, _, _, _ = self._create_model()
-      predictions = model.predict(input_image, true_image_shapes=None)
-      return (predictions['box_encodings'],
-              predictions['class_predictions_with_background'],
-              predictions['feature_maps'],
-              predictions['anchors'])
-    batch_size = 3
-    image_size = 2
-    channels = 3
-    input_image = np.random.rand(batch_size, image_size, image_size,
-                                 channels).astype(np.float32)
-    expected_box_encodings_shape = (batch_size, num_anchors, code_size)
-    expected_class_predictions_shape = (batch_size, num_anchors, num_classes+1)
-    (box_encodings, class_predictions, _, _) = self.execute(graph_fn,
-                                                            [input_image])
-    self.assertAllEqual(box_encodings.shape, expected_box_encodings_shape)
-    self.assertAllEqual(class_predictions.shape,
-                        expected_class_predictions_shape)
-
-  def test_postprocess_results_are_correct(self, use_keras):
-    batch_size = 2
-    image_size = 2
-    input_shapes = [(batch_size, image_size, image_size, 3),
-                    (None, image_size, image_size, 3),
-                    (batch_size, None, None, 3),
-                    (None, None, None, 3)]
-
-    expected_boxes = [
-        [
-            [0, 0, .5, .5],
-            [0, .5, .5, 1],
-            [.5, 0, 1, .5],
-            [0, 0, 0, 0],  # pruned prediction
-            [0, 0, 0, 0]
-        ],  # padding
-        [
-            [0, 0, .5, .5],
-            [0, .5, .5, 1],
-            [.5, 0, 1, .5],
-            [0, 0, 0, 0],  # pruned prediction
-            [0, 0, 0, 0]
-        ]
-    ]  # padding
-    expected_scores = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
-    expected_classes = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
-    expected_num_detections = np.array([3, 3])
-
-    for input_shape in input_shapes:
-      tf_graph = tf.Graph()
-      with tf_graph.as_default():
-        model, _, _, _ = self._create_model(use_keras=use_keras)
-        input_placeholder = tf.placeholder(tf.float32, shape=input_shape)
-        preprocessed_inputs, true_image_shapes = model.preprocess(
-            input_placeholder)
-        prediction_dict = model.predict(preprocessed_inputs,
-                                        true_image_shapes)
-        detections = model.postprocess(prediction_dict, true_image_shapes)
-        self.assertIn('detection_boxes', detections)
-        self.assertIn('detection_scores', detections)
-        self.assertIn('detection_classes', detections)
-        self.assertIn('num_detections', detections)
-        init_op = tf.global_variables_initializer()
-      with self.test_session(graph=tf_graph) as sess:
-        sess.run(init_op)
-        detections_out = sess.run(detections,
-                                  feed_dict={
-                                      input_placeholder:
-                                      np.random.uniform(
-                                          size=(batch_size, 2, 2, 3))})
-      for image_idx in range(batch_size):
-        self.assertTrue(
-            test_utils.first_rows_close_as_set(
-                detections_out['detection_boxes'][image_idx].tolist(),
-                expected_boxes[image_idx]))
-      self.assertAllClose(detections_out['detection_scores'], expected_scores)
-      self.assertAllClose(detections_out['detection_classes'], expected_classes)
-      self.assertAllClose(detections_out['num_detections'],
-                          expected_num_detections)
-
-  def test_loss_results_are_correct(self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(use_keras=use_keras)
-    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
-                 groundtruth_classes1, groundtruth_classes2):
-      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
-      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(apply_hard_mining=False)
-      model.provide_groundtruth(groundtruth_boxes_list,
-                                groundtruth_classes_list)
-      prediction_dict = model.predict(preprocessed_tensor,
-                                      true_image_shapes=None)
-      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
-      return (
-          _get_value_for_matching_key(loss_dict, 'Loss/localization_loss'),
-          _get_value_for_matching_key(loss_dict, 'Loss/classification_loss'))
-
-    batch_size = 2
-    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
-    groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_classes1 = np.array([[1]], dtype=np.float32)
-    groundtruth_classes2 = np.array([[1]], dtype=np.float32)
-    expected_localization_loss = 0.0
-    expected_classification_loss = (batch_size * num_anchors
-                                    * (num_classes+1) * np.log(2.0))
-    (localization_loss,
-     classification_loss) = self.execute(graph_fn, [preprocessed_input,
-                                                    groundtruth_boxes1,
-                                                    groundtruth_boxes2,
-                                                    groundtruth_classes1,
-                                                    groundtruth_classes2])
-    self.assertAllClose(localization_loss, expected_localization_loss)
-    self.assertAllClose(classification_loss, expected_classification_loss)
-
-  def test_loss_results_are_correct_with_normalize_by_codesize_true(
-      self, use_keras):
-
-    with tf.Graph().as_default():
-      _, _, _, _ = self._create_model(use_keras=use_keras)
-    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
-                 groundtruth_classes1, groundtruth_classes2):
-      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
-      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(apply_hard_mining=False,
-                                          normalize_loc_loss_by_codesize=True,
-                                          use_keras=use_keras)
-      model.provide_groundtruth(groundtruth_boxes_list,
-                                groundtruth_classes_list)
-      prediction_dict = model.predict(preprocessed_tensor,
-                                      true_image_shapes=None)
-      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
-      return (_get_value_for_matching_key(loss_dict, 'Loss/localization_loss'),)
-
-    batch_size = 2
-    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
-    groundtruth_boxes1 = np.array([[0, 0, 1, 1]], dtype=np.float32)
-    groundtruth_boxes2 = np.array([[0, 0, 1, 1]], dtype=np.float32)
-    groundtruth_classes1 = np.array([[1]], dtype=np.float32)
-    groundtruth_classes2 = np.array([[1]], dtype=np.float32)
-    expected_localization_loss = 0.5 / 4
-    localization_loss = self.execute(graph_fn, [preprocessed_input,
-                                                groundtruth_boxes1,
-                                                groundtruth_boxes2,
-                                                groundtruth_classes1,
-                                                groundtruth_classes2])
-    self.assertAllClose(localization_loss, expected_localization_loss)
-
-  def test_loss_results_are_correct_with_hard_example_mining(self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(use_keras=use_keras)
-    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
-                 groundtruth_classes1, groundtruth_classes2):
-      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
-      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model()
-      model.provide_groundtruth(groundtruth_boxes_list,
-                                groundtruth_classes_list)
-      prediction_dict = model.predict(preprocessed_tensor,
-                                      true_image_shapes=None)
-      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
-      return (
-          _get_value_for_matching_key(loss_dict, 'Loss/localization_loss'),
-          _get_value_for_matching_key(loss_dict, 'Loss/classification_loss'))
-
-    batch_size = 2
-    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
-    groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_classes1 = np.array([[1]], dtype=np.float32)
-    groundtruth_classes2 = np.array([[1]], dtype=np.float32)
-    expected_localization_loss = 0.0
-    expected_classification_loss = (batch_size * num_anchors
-                                    * (num_classes+1) * np.log(2.0))
-    (localization_loss, classification_loss) = self.execute_cpu(
-        graph_fn, [
-            preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
-            groundtruth_classes1, groundtruth_classes2
-        ])
-    self.assertAllClose(localization_loss, expected_localization_loss)
-    self.assertAllClose(classification_loss, expected_classification_loss)
-
-  def test_loss_results_are_correct_without_add_background_class(
-      self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(
-          add_background_class=False, use_keras=use_keras)
-
-    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
-                 groundtruth_classes1, groundtruth_classes2):
-      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
-      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(
-          apply_hard_mining=False, add_background_class=False,
-          use_keras=use_keras)
-      model.provide_groundtruth(groundtruth_boxes_list,
-                                groundtruth_classes_list)
-      prediction_dict = model.predict(
-          preprocessed_tensor, true_image_shapes=None)
-      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
-      return (loss_dict['Loss/localization_loss'],
-              loss_dict['Loss/classification_loss'])
-
-    batch_size = 2
-    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
-    groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_classes1 = np.array([[0, 1]], dtype=np.float32)
-    groundtruth_classes2 = np.array([[0, 1]], dtype=np.float32)
-    expected_localization_loss = 0.0
-    expected_classification_loss = (
-        batch_size * num_anchors * (num_classes + 1) * np.log(2.0))
-    (localization_loss, classification_loss) = self.execute(
-        graph_fn, [
-            preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
-            groundtruth_classes1, groundtruth_classes2
-        ])
-
-    self.assertAllClose(localization_loss, expected_localization_loss)
-    self.assertAllClose(classification_loss, expected_classification_loss)
-
-  def test_loss_with_expected_classification_loss(self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(use_keras=use_keras)
-
-    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
-                 groundtruth_classes1, groundtruth_classes2):
-      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
-      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(
-          apply_hard_mining=False,
-          add_background_class=True,
-          use_expected_classification_loss_under_sampling=True,
-          minimum_negative_sampling=1,
-          desired_negative_sampling_ratio=desired_negative_sampling_ratio)
-      model.provide_groundtruth(groundtruth_boxes_list,
-                                groundtruth_classes_list)
-      prediction_dict = model.predict(
-          preprocessed_tensor, true_image_shapes=None)
-      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
-      return (loss_dict['Loss/localization_loss'],
-              loss_dict['Loss/classification_loss'])
-
-    batch_size = 2
-    desired_negative_sampling_ratio = 4
-    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
-    groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_classes1 = np.array([[1]], dtype=np.float32)
-    groundtruth_classes2 = np.array([[1]], dtype=np.float32)
-    expected_localization_loss = 0.0
-
-    expected_classification_loss = (
-        batch_size * (desired_negative_sampling_ratio * num_anchors +
-                      num_classes * num_anchors) * np.log(2.0))
-    (localization_loss, classification_loss) = self.execute(
-        graph_fn, [
-            preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
-            groundtruth_classes1, groundtruth_classes2
-        ])
-
-    self.assertAllClose(localization_loss, expected_localization_loss)
-    self.assertAllClose(classification_loss, expected_classification_loss)
-
-  def test_loss_results_are_correct_with_weight_regression_loss_by_score(
-      self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(
-          use_keras=use_keras,
-          add_background_class=False,
-          weight_regression_loss_by_score=True)
-
-    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
-                 groundtruth_classes1, groundtruth_classes2):
-      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
-      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(
-          use_keras=use_keras,
-          apply_hard_mining=False,
-          add_background_class=False,
-          weight_regression_loss_by_score=True)
-      model.provide_groundtruth(groundtruth_boxes_list,
-                                groundtruth_classes_list)
-      prediction_dict = model.predict(
-          preprocessed_tensor, true_image_shapes=None)
-      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
-      return (loss_dict['Loss/localization_loss'],
-              loss_dict['Loss/classification_loss'])
-
-    batch_size = 2
-    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
-    groundtruth_boxes1 = np.array([[0, 0, 1, 1]], dtype=np.float32)
-    groundtruth_boxes2 = np.array([[0, 0, 1, 1]], dtype=np.float32)
-    groundtruth_classes1 = np.array([[0, 1]], dtype=np.float32)
-    groundtruth_classes2 = np.array([[1, 0]], dtype=np.float32)
-    expected_localization_loss = 0.25
-    expected_classification_loss = (
-        batch_size * num_anchors * (num_classes + 1) * np.log(2.0))
-    (localization_loss, classification_loss) = self.execute(
-        graph_fn, [
-            preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
-            groundtruth_classes1, groundtruth_classes2
-        ])
-    self.assertAllClose(localization_loss, expected_localization_loss)
-    self.assertAllClose(classification_loss, expected_classification_loss)
-
-  def test_restore_map_for_detection_ckpt(self, use_keras):
-    model, _, _, _ = self._create_model(use_keras=use_keras)
-    model.predict(tf.constant(np.array([[[[0, 0], [1, 1]], [[1, 0], [0, 1]]]],
-                                       dtype=np.float32)),
-                  true_image_shapes=None)
-    init_op = tf.global_variables_initializer()
-    saver = tf.train.Saver()
-    save_path = self.get_temp_dir()
-    with self.test_session() as sess:
-      sess.run(init_op)
-      saved_model_path = saver.save(sess, save_path)
-      var_map = model.restore_map(
-          fine_tune_checkpoint_type='detection',
-          load_all_detection_checkpoint_vars=False)
-      self.assertIsInstance(var_map, dict)
-      saver = tf.train.Saver(var_map)
-      saver.restore(sess, saved_model_path)
-      for var in sess.run(tf.report_uninitialized_variables()):
-        self.assertNotIn('FeatureExtractor', var)
-
-  def test_restore_map_for_classification_ckpt(self, use_keras):
-    # Define mock tensorflow classification graph and save variables.
-    test_graph_classification = tf.Graph()
-    with test_graph_classification.as_default():
-      image = tf.placeholder(dtype=tf.float32, shape=[1, 20, 20, 3])
-      if use_keras:
-        with tf.name_scope('mock_model'):
-          layer_one = keras.Conv2D(32, kernel_size=1, name='layer1')
-          net = layer_one(image)
-          layer_two = keras.Conv2D(3, kernel_size=1, name='layer2')
-          layer_two(net)
-      else:
-        with tf.variable_scope('mock_model'):
-          net = slim.conv2d(image, num_outputs=32, kernel_size=1,
-                            scope='layer1')
-          slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
-
-      init_op = tf.global_variables_initializer()
-      saver = tf.train.Saver()
-      save_path = self.get_temp_dir()
-      with self.test_session(graph=test_graph_classification) as sess:
-        sess.run(init_op)
-        saved_model_path = saver.save(sess, save_path)
-
-    # Create tensorflow detection graph and load variables from
-    # classification checkpoint.
-    test_graph_detection = tf.Graph()
-    with test_graph_detection.as_default():
-      model, _, _, _ = self._create_model(use_keras=use_keras)
-      inputs_shape = [2, 2, 2, 3]
-      inputs = tf.to_float(tf.random_uniform(
-          inputs_shape, minval=0, maxval=255, dtype=tf.int32))
-      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
-      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
-      model.postprocess(prediction_dict, true_image_shapes)
-      another_variable = tf.Variable([17.0], name='another_variable')  # pylint: disable=unused-variable
-      var_map = model.restore_map(fine_tune_checkpoint_type='classification')
-      self.assertNotIn('another_variable', var_map)
-      self.assertIsInstance(var_map, dict)
-      saver = tf.train.Saver(var_map)
-      with self.test_session(graph=test_graph_detection) as sess:
-        saver.restore(sess, saved_model_path)
-        for var in sess.run(tf.report_uninitialized_variables()):
-          self.assertNotIn('FeatureExtractor', var)
-
-  def test_load_all_det_checkpoint_vars(self, use_keras):
-    test_graph_detection = tf.Graph()
-    with test_graph_detection.as_default():
-      model, _, _, _ = self._create_model(use_keras=use_keras)
-      inputs_shape = [2, 2, 2, 3]
-      inputs = tf.to_float(
-          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32))
-      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
-      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
-      model.postprocess(prediction_dict, true_image_shapes)
-      another_variable = tf.Variable([17.0], name='another_variable')  # pylint: disable=unused-variable
-      var_map = model.restore_map(
-          fine_tune_checkpoint_type='detection',
-          load_all_detection_checkpoint_vars=True)
-      self.assertIsInstance(var_map, dict)
-      self.assertIn('another_variable', var_map)
-
-  def test_loss_results_are_correct_with_random_example_sampling(
-      self,
-      use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(
-          random_example_sampling=True,
-          use_keras=use_keras)
-    print num_classes, num_anchors
-
-    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
-                 groundtruth_classes1, groundtruth_classes2):
-      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
-      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(random_example_sampling=True,
-                                          use_keras=use_keras)
-      model.provide_groundtruth(groundtruth_boxes_list,
-                                groundtruth_classes_list)
-      prediction_dict = model.predict(
-          preprocessed_tensor, true_image_shapes=None)
-      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
-      return (_get_value_for_matching_key(loss_dict, 'Loss/localization_loss'),
-              _get_value_for_matching_key(loss_dict,
-                                          'Loss/classification_loss'))
-
-    batch_size = 2
-    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
-    groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)
-    groundtruth_classes1 = np.array([[1]], dtype=np.float32)
-    groundtruth_classes2 = np.array([[1]], dtype=np.float32)
-    expected_localization_loss = 0.0
-    # Among 4 anchors (1 positive, 3 negative) in this test, only 2 anchors are
-    # selected (1 positive, 1 negative) since random sampler will adjust number
-    # of negative examples to make sure positive example fraction in the batch
-    # is 0.5.
-    expected_classification_loss = (
-        batch_size * 2 * (num_classes + 1) * np.log(2.0))
-    (localization_loss, classification_loss) = self.execute_cpu(
-        graph_fn, [
-            preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
-            groundtruth_classes1, groundtruth_classes2
-        ])
-    self.assertAllClose(localization_loss, expected_localization_loss)
-    self.assertAllClose(classification_loss, expected_classification_loss)
-
-if __name__ == '__main__':
-  tf.test.main()
diff --git a/research/object_detection/metrics/coco_evaluation.py b/research/object_detection/metrics/coco_evaluation.py
index eb204592..a44ff370 100644
--- a/research/object_detection/metrics/coco_evaluation.py
+++ b/research/object_detection/metrics/coco_evaluation.py
@@ -78,7 +78,7 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
           shape [num_boxes] containing iscrowd flag for groundtruth boxes.
     """
     if image_id in self._image_ids:
-      tf.logging.warning('Ignoring ground truth with image id %s since it was '
+      tf.compat.v1.logging.warning('Ignoring ground truth with image id %s since it was '
                          'previously added', image_id)
       return
 
@@ -129,7 +129,7 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
       raise ValueError('Missing groundtruth for image id: {}'.format(image_id))
 
     if self._image_ids[image_id]:
-      tf.logging.warning('Ignoring detection with image id %s since it was '
+      tf.compat.v1.logging.warning('Ignoring detection with image id %s since it was '
                          'previously added', image_id)
       return
 
@@ -284,25 +284,25 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
       detection_classes = tf.expand_dims(detection_classes, 0)
 
       if num_gt_boxes_per_image is None:
-        num_gt_boxes_per_image = tf.shape(groundtruth_boxes)[1:2]
+        num_gt_boxes_per_image = tf.shape(input=groundtruth_boxes)[1:2]
       else:
         num_gt_boxes_per_image = tf.expand_dims(num_gt_boxes_per_image, 0)
 
       if num_det_boxes_per_image is None:
-        num_det_boxes_per_image = tf.shape(detection_boxes)[1:2]
+        num_det_boxes_per_image = tf.shape(input=detection_boxes)[1:2]
       else:
         num_det_boxes_per_image = tf.expand_dims(num_det_boxes_per_image, 0)
     else:
       if num_gt_boxes_per_image is None:
         num_gt_boxes_per_image = tf.tile(
-            tf.shape(groundtruth_boxes)[1:2],
-            multiples=tf.shape(groundtruth_boxes)[0:1])
+            tf.shape(input=groundtruth_boxes)[1:2],
+            multiples=tf.shape(input=groundtruth_boxes)[0:1])
       if num_det_boxes_per_image is None:
         num_det_boxes_per_image = tf.tile(
-            tf.shape(detection_boxes)[1:2],
-            multiples=tf.shape(detection_boxes)[0:1])
+            tf.shape(input=detection_boxes)[1:2],
+            multiples=tf.shape(input=detection_boxes)[0:1])
 
-    update_op = tf.py_func(update_op, [image_id,
+    update_op = tf.compat.v1.py_func(update_op, [image_id,
                                        groundtruth_boxes,
                                        groundtruth_classes,
                                        groundtruth_is_crowd,
@@ -339,11 +339,11 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
       return value_func
 
     # Ensure that the metrics are only evaluated once.
-    first_value_op = tf.py_func(first_value_func, [], tf.float32)
+    first_value_op = tf.compat.v1.py_func(first_value_func, [], tf.float32)
     eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}
     with tf.control_dependencies([first_value_op]):
       for metric_name in metric_names[1:]:
-        eval_metric_ops[metric_name] = (tf.py_func(
+        eval_metric_ops[metric_name] = (tf.compat.v1.py_func(
             value_func_factory(metric_name), [], np.float32), update_op)
     return eval_metric_ops
 
@@ -408,7 +408,7 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
           {0, 1}.
     """
     if image_id in self._image_id_to_mask_shape_map:
-      tf.logging.warning('Ignoring ground truth with image id %s since it was '
+      tf.compat.v1.logging.warning('Ignoring ground truth with image id %s since it was '
                          'previously added', image_id)
       return
 
@@ -463,7 +463,7 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
       raise ValueError('Missing groundtruth for image id: {}'.format(image_id))
 
     if image_id in self._image_ids_with_detections:
-      tf.logging.warning('Ignoring detection with image id %s since it was '
+      tf.compat.v1.logging.warning('Ignoring detection with image id %s since it was '
                          'previously added', image_id)
       return
 
@@ -601,7 +601,7 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
 
     if groundtruth_is_crowd is None:
       groundtruth_is_crowd = tf.zeros_like(groundtruth_classes, dtype=tf.bool)
-    update_op = tf.py_func(update_op, [image_id,
+    update_op = tf.compat.v1.py_func(update_op, [image_id,
                                        groundtruth_boxes,
                                        groundtruth_classes,
                                        groundtruth_instance_masks,
@@ -637,10 +637,10 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
       return value_func
 
     # Ensure that the metrics are only evaluated once.
-    first_value_op = tf.py_func(first_value_func, [], tf.float32)
+    first_value_op = tf.compat.v1.py_func(first_value_func, [], tf.float32)
     eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}
     with tf.control_dependencies([first_value_op]):
       for metric_name in metric_names[1:]:
-        eval_metric_ops[metric_name] = (tf.py_func(
+        eval_metric_ops[metric_name] = (tf.compat.v1.py_func(
             value_func_factory(metric_name), [], np.float32), update_op)
     return eval_metric_ops
diff --git a/research/object_detection/metrics/coco_evaluation_test.py b/research/object_detection/metrics/coco_evaluation_test.py
index 50da9552..7ceb92e6 100644
--- a/research/object_detection/metrics/coco_evaluation_test.py
+++ b/research/object_detection/metrics/coco_evaluation_test.py
@@ -251,12 +251,12 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                      {'id': 1, 'name': 'cat'},
                      {'id': 2, 'name': 'dog'}]
     coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)
-    image_id = tf.placeholder(tf.string, shape=())
-    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))
-    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    detection_scores = tf.placeholder(tf.float32, shape=(None))
-    detection_classes = tf.placeholder(tf.float32, shape=(None))
+    image_id = tf.compat.v1.placeholder(tf.string, shape=())
+    groundtruth_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    groundtruth_classes = tf.compat.v1.placeholder(tf.float32, shape=(None))
+    detection_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    detection_scores = tf.compat.v1.placeholder(tf.float32, shape=(None))
+    detection_classes = tf.compat.v1.placeholder(tf.float32, shape=(None))
 
     input_data_fields = standard_fields.InputDataFields
     detection_fields = standard_fields.DetectionResultFields
@@ -335,12 +335,12 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
         'name': 'dog'
     }]
     coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)
-    image_id = tf.placeholder(tf.string, shape=())
-    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))
-    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    detection_scores = tf.placeholder(tf.float32, shape=(None))
-    detection_classes = tf.placeholder(tf.float32, shape=(None))
+    image_id = tf.compat.v1.placeholder(tf.string, shape=())
+    groundtruth_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    groundtruth_classes = tf.compat.v1.placeholder(tf.float32, shape=(None))
+    detection_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    detection_scores = tf.compat.v1.placeholder(tf.float32, shape=(None))
+    detection_classes = tf.compat.v1.placeholder(tf.float32, shape=(None))
 
     input_data_fields = standard_fields.InputDataFields
     detection_fields = standard_fields.DetectionResultFields
@@ -434,12 +434,12 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                      {'id': 2, 'name': 'dog'}]
     coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)
     batch_size = 3
-    image_id = tf.placeholder(tf.string, shape=(batch_size))
-    groundtruth_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))
-    groundtruth_classes = tf.placeholder(tf.float32, shape=(batch_size, None))
-    detection_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))
-    detection_scores = tf.placeholder(tf.float32, shape=(batch_size, None))
-    detection_classes = tf.placeholder(tf.float32, shape=(batch_size, None))
+    image_id = tf.compat.v1.placeholder(tf.string, shape=(batch_size))
+    groundtruth_boxes = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None, 4))
+    groundtruth_classes = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None))
+    detection_boxes = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None, 4))
+    detection_scores = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None))
+    detection_classes = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None))
 
     input_data_fields = standard_fields.InputDataFields
     detection_fields = standard_fields.DetectionResultFields
@@ -498,14 +498,14 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                      {'id': 2, 'name': 'dog'}]
     coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)
     batch_size = 3
-    image_id = tf.placeholder(tf.string, shape=(batch_size))
-    groundtruth_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))
-    groundtruth_classes = tf.placeholder(tf.float32, shape=(batch_size, None))
-    num_gt_boxes_per_image = tf.placeholder(tf.int32, shape=(None))
-    detection_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))
-    detection_scores = tf.placeholder(tf.float32, shape=(batch_size, None))
-    detection_classes = tf.placeholder(tf.float32, shape=(batch_size, None))
-    num_det_boxes_per_image = tf.placeholder(tf.int32, shape=(None))
+    image_id = tf.compat.v1.placeholder(tf.string, shape=(batch_size))
+    groundtruth_boxes = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None, 4))
+    groundtruth_classes = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None))
+    num_gt_boxes_per_image = tf.compat.v1.placeholder(tf.int32, shape=(None))
+    detection_boxes = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None, 4))
+    detection_scores = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None))
+    detection_classes = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, None))
+    num_det_boxes_per_image = tf.compat.v1.placeholder(tf.int32, shape=(None))
 
     input_data_fields = standard_fields.InputDataFields
     detection_fields = standard_fields.DetectionResultFields
@@ -661,13 +661,13 @@ class CocoMaskEvaluationPyFuncTest(tf.test.TestCase):
                      {'id': 1, 'name': 'cat'},
                      {'id': 2, 'name': 'dog'}]
     coco_evaluator = coco_evaluation.CocoMaskEvaluator(category_list)
-    image_id = tf.placeholder(tf.string, shape=())
-    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))
-    groundtruth_masks = tf.placeholder(tf.uint8, shape=(None, None, None))
-    detection_scores = tf.placeholder(tf.float32, shape=(None))
-    detection_classes = tf.placeholder(tf.float32, shape=(None))
-    detection_masks = tf.placeholder(tf.uint8, shape=(None, None, None))
+    image_id = tf.compat.v1.placeholder(tf.string, shape=())
+    groundtruth_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    groundtruth_classes = tf.compat.v1.placeholder(tf.float32, shape=(None))
+    groundtruth_masks = tf.compat.v1.placeholder(tf.uint8, shape=(None, None, None))
+    detection_scores = tf.compat.v1.placeholder(tf.float32, shape=(None))
+    detection_classes = tf.compat.v1.placeholder(tf.float32, shape=(None))
+    detection_masks = tf.compat.v1.placeholder(tf.uint8, shape=(None, None, None))
 
     input_data_fields = standard_fields.InputDataFields
     detection_fields = standard_fields.DetectionResultFields
diff --git a/research/object_detection/metrics/coco_tools.py b/research/object_detection/metrics/coco_tools.py
index 71b747bc..145e93ba 100644
--- a/research/object_detection/metrics/coco_tools.py
+++ b/research/object_detection/metrics/coco_tools.py
@@ -106,7 +106,7 @@ class COCOWrapper(coco.COCO):
     results = coco.COCO()
     results.dataset['images'] = [img for img in self.dataset['images']]
 
-    tf.logging.info('Loading and preparing annotation results...')
+    tf.compat.v1.logging.info('Loading and preparing annotation results...')
     tic = time.time()
 
     if not isinstance(annotations, list):
@@ -128,7 +128,7 @@ class COCOWrapper(coco.COCO):
         ann['bbox'] = mask.toBbox(ann['segmentation'])
         ann['id'] = idx + 1
         ann['iscrowd'] = 0
-    tf.logging.info('DONE (t=%0.2fs)', (time.time() - tic))
+    tf.compat.v1.logging.info('DONE (t=%0.2fs)', (time.time() - tic))
 
     results.dataset['annotations'] = annotations
     results.createIndex()
@@ -479,7 +479,7 @@ def ExportGroundtruthToCOCO(image_ids,
       'categories': categories
   }
   if output_path:
-    with tf.gfile.GFile(output_path, 'w') as fid:
+    with tf.io.gfile.GFile(output_path, 'w') as fid:
       json_utils.Dump(groundtruth_dict, fid, float_digits=4, indent=2)
   return groundtruth_dict
 
@@ -666,7 +666,7 @@ def ExportDetectionsToCOCO(image_ids,
         scores,
         classes))
   if output_path:
-    with tf.gfile.GFile(output_path, 'w') as fid:
+    with tf.io.gfile.GFile(output_path, 'w') as fid:
       json_utils.Dump(detections_export_list, fid, float_digits=4, indent=2)
   return detections_export_list
 
@@ -746,7 +746,7 @@ def ExportSegmentsToCOCO(image_ids,
         image_id, category_id_set, np.squeeze(masks, axis=3), scores, classes))
 
   if output_path:
-    with tf.gfile.GFile(output_path, 'w') as fid:
+    with tf.io.gfile.GFile(output_path, 'w') as fid:
       json_utils.Dump(segment_export_list, fid, float_digits=4, indent=2)
   return segment_export_list
 
@@ -845,6 +845,6 @@ def ExportKeypointsToCOCO(image_ids,
         })
 
   if output_path:
-    with tf.gfile.GFile(output_path, 'w') as fid:
+    with tf.io.gfile.GFile(output_path, 'w') as fid:
       json_utils.Dump(keypoints_export_list, fid, float_digits=4, indent=2)
   return keypoints_export_list
diff --git a/research/object_detection/metrics/coco_tools_test.py b/research/object_detection/metrics/coco_tools_test.py
index cfb73d8c..fd79a4a9 100644
--- a/research/object_detection/metrics/coco_tools_test.py
+++ b/research/object_detection/metrics/coco_tools_test.py
@@ -86,7 +86,7 @@ class CocoToolsTest(tf.test.TestCase):
     categories = [{'id': 0, 'name': 'person'},
                   {'id': 1, 'name': 'cat'},
                   {'id': 2, 'name': 'dog'}]
-    output_path = os.path.join(tf.test.get_temp_dir(), 'groundtruth.json')
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'groundtruth.json')
     result = coco_tools.ExportGroundtruthToCOCO(
         image_ids,
         groundtruth_boxes,
@@ -94,7 +94,7 @@ class CocoToolsTest(tf.test.TestCase):
         categories,
         output_path=output_path)
     self.assertDictEqual(result, self._groundtruth_dict)
-    with tf.gfile.GFile(output_path, 'r') as f:
+    with tf.io.gfile.GFile(output_path, 'r') as f:
       written_result = f.read()
       # The json output should have floats written to 4 digits of precision.
       matcher = re.compile(r'"bbox":\s+\[\n\s+\d+.\d\d\d\d,', re.MULTILINE)
@@ -111,7 +111,7 @@ class CocoToolsTest(tf.test.TestCase):
     categories = [{'id': 0, 'name': 'person'},
                   {'id': 1, 'name': 'cat'},
                   {'id': 2, 'name': 'dog'}]
-    output_path = os.path.join(tf.test.get_temp_dir(), 'detections.json')
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'detections.json')
     result = coco_tools.ExportDetectionsToCOCO(
         image_ids,
         detections_boxes,
@@ -120,7 +120,7 @@ class CocoToolsTest(tf.test.TestCase):
         categories,
         output_path=output_path)
     self.assertListEqual(result, self._detections_list)
-    with tf.gfile.GFile(output_path, 'r') as f:
+    with tf.io.gfile.GFile(output_path, 'r') as f:
       written_result = f.read()
       # The json output should have floats written to 4 digits of precision.
       matcher = re.compile(r'"bbox":\s+\[\n\s+\d+.\d\d\d\d,', re.MULTILINE)
@@ -145,7 +145,7 @@ class CocoToolsTest(tf.test.TestCase):
     categories = [{'id': 0, 'name': 'person'},
                   {'id': 1, 'name': 'cat'},
                   {'id': 2, 'name': 'dog'}]
-    output_path = os.path.join(tf.test.get_temp_dir(), 'segments.json')
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'segments.json')
     result = coco_tools.ExportSegmentsToCOCO(
         image_ids,
         detection_masks,
@@ -153,7 +153,7 @@ class CocoToolsTest(tf.test.TestCase):
         detection_classes,
         categories,
         output_path=output_path)
-    with tf.gfile.GFile(output_path, 'r') as f:
+    with tf.io.gfile.GFile(output_path, 'r') as f:
       written_result = f.read()
       written_result = json.loads(written_result)
       mask_load = mask.decode([written_result[0]['segmentation']])
@@ -178,7 +178,7 @@ class CocoToolsTest(tf.test.TestCase):
                   {'id': 2, 'name': 'cat'},
                   {'id': 3, 'name': 'dog'}]
 
-    output_path = os.path.join(tf.test.get_temp_dir(), 'keypoints.json')
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'keypoints.json')
     result = coco_tools.ExportKeypointsToCOCO(
         image_ids,
         detection_keypoints,
@@ -187,7 +187,7 @@ class CocoToolsTest(tf.test.TestCase):
         categories,
         output_path=output_path)
 
-    with tf.gfile.GFile(output_path, 'r') as f:
+    with tf.io.gfile.GFile(output_path, 'r') as f:
       written_result = f.read()
       written_result = json.loads(written_result)
       self.assertAlmostEqual(result, written_result)
diff --git a/research/object_detection/metrics/offline_eval_map_corloc.py b/research/object_detection/metrics/offline_eval_map_corloc.py
index ff2efbaf..b3cd5c00 100644
--- a/research/object_detection/metrics/offline_eval_map_corloc.py
+++ b/research/object_detection/metrics/offline_eval_map_corloc.py
@@ -34,6 +34,7 @@ Example usage:
 import csv
 import os
 import re
+import argparse
 import tensorflow as tf
 
 from object_detection.core import standard_fields
@@ -42,16 +43,14 @@ from object_detection.metrics import tf_example_parser
 from object_detection.utils import config_util
 from object_detection.utils import label_map_util
 
-flags = tf.app.flags
-tf.logging.set_verbosity(tf.logging.INFO)
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
-flags.DEFINE_string('eval_dir', None, 'Directory to write eval summaries to.')
-flags.DEFINE_string('eval_config_path', None,
-                    'Path to an eval_pb2.EvalConfig config file.')
-flags.DEFINE_string('input_config_path', None,
-                    'Path to an eval_pb2.InputConfig config file.')
+parser = argparse.ArgumentParser()
+parser.add_argument('--eval_dir', type=str, default=None, help='Directory to write eval summaries to.')
+parser.add_argument('--eval_config_path', type=str, default=None, help='Path to an eval_pb2.EvalConfig config file.')
+parser.add_argument('--input_config_path', type=str, default=None, help='Path to an eval_pb2.InputConfig config file.')
 
-FLAGS = flags.FLAGS
+args = parser.parse_args()
 
 
 def _generate_sharded_filenames(filename):
@@ -104,13 +103,13 @@ def read_data_and_evaluate(input_config, eval_config):
     skipped_images = 0
     processed_images = 0
     for input_path in _generate_filenames(input_paths):
-      tf.logging.info('Processing file: {0}'.format(input_path))
+      tf.compat.v1.logging.info('Processing file: {0}'.format(input_path))
 
-      record_iterator = tf.python_io.tf_record_iterator(path=input_path)
+      record_iterator = tf.compat.v1.python_io.tf_record_iterator(path=input_path)
       data_parser = tf_example_parser.TfExampleDetectionAndGTParser()
 
       for string_record in record_iterator:
-        tf.logging.log_every_n(tf.logging.INFO, 'Processed %d images...', 1000,
+        tf.compat.v1.logging.log_every_n(tf.compat.v1.logging.INFO, 'Processed %d images...', 1000,
                                processed_images)
         processed_images += 1
 
@@ -127,7 +126,7 @@ def read_data_and_evaluate(input_config, eval_config):
               decoded_dict)
         else:
           skipped_images += 1
-          tf.logging.info('Skipped images: {0}'.format(skipped_images))
+          tf.compat.v1.logging.info('Skipped images: {0}'.format(skipped_images))
 
     return object_detection_evaluator.evaluate()
 
@@ -141,7 +140,7 @@ def write_metrics(metrics, output_dir):
     metrics: A dictionary containing metric names and values.
     output_dir: Directory to write metrics to.
   """
-  tf.logging.info('Writing metrics.')
+  tf.compat.v1.logging.info('Writing metrics.')
 
   with open(os.path.join(output_dir, 'metrics.csv'), 'w') as csvfile:
     metrics_writer = csv.writer(csvfile, delimiter=',')
@@ -153,12 +152,12 @@ def main(argv):
   del argv
   required_flags = ['input_config_path', 'eval_config_path', 'eval_dir']
   for flag_name in required_flags:
-    if not getattr(FLAGS, flag_name):
+    if not getattr(args, flag_name):
       raise ValueError('Flag --{} is required'.format(flag_name))
 
   configs = config_util.get_configs_from_multiple_files(
-      eval_input_config_path=FLAGS.input_config_path,
-      eval_config_path=FLAGS.eval_config_path)
+      eval_input_config_path=args.input_config_path,
+      eval_config_path=args.eval_config_path)
 
   eval_config = configs['eval_config']
   input_config = configs['eval_input_config']
@@ -166,8 +165,8 @@ def main(argv):
   metrics = read_data_and_evaluate(input_config, eval_config)
 
   # Save metrics
-  write_metrics(metrics, FLAGS.eval_dir)
+  write_metrics(metrics, args.eval_dir)
 
 
 if __name__ == '__main__':
-  tf.app.run(main)
+  tf.compat.v1.app.run(main)
diff --git a/research/object_detection/metrics/tf_example_parser.py b/research/object_detection/metrics/tf_example_parser.py
index 9a5f130f..fa361bf5 100644
--- a/research/object_detection/metrics/tf_example_parser.py
+++ b/research/object_detection/metrics/tf_example_parser.py
@@ -44,7 +44,7 @@ class StringParser(data_parser.DataToNumpyParser):
     self.field_name = field_name
 
   def parse(self, tf_example):
-    return "".join(tf_example.features.feature[self.field_name]
+    return b"".join(tf_example.features.feature[self.field_name]
                    .bytes_list.value) if tf_example.features.feature[
                        self.field_name].HasField("bytes_list") else None
 
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index 175d815f..55635e09 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -74,14 +74,14 @@ def _prepare_groundtruth_for_eval(detection_model, class_agnostic):
   # For class-agnostic models, groundtruth one-hot encodings collapse to all
   # ones.
   if class_agnostic:
-    groundtruth_boxes_shape = tf.shape(groundtruth_boxes)
+    groundtruth_boxes_shape = tf.shape(input=groundtruth_boxes)
     groundtruth_classes_one_hot = tf.ones([groundtruth_boxes_shape[0], 1])
   else:
     groundtruth_classes_one_hot = detection_model.groundtruth_lists(
         fields.BoxListFields.classes)[0]
   label_id_offset = 1  # Applying label id offset (b/63711816)
   groundtruth_classes = (
-      tf.argmax(groundtruth_classes_one_hot, axis=1) + label_id_offset)
+      tf.argmax(input=groundtruth_classes_one_hot, axis=1) + label_id_offset)
   groundtruth = {
       input_data_fields.groundtruth_boxes: groundtruth_boxes,
       input_data_fields.groundtruth_classes: groundtruth_classes
@@ -274,12 +274,12 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
                 include_global_step=False))
         if use_tpu:
           def tpu_scaffold():
-            tf.train.init_from_checkpoint(train_config.fine_tune_checkpoint,
+            tf.compat.v1.train.init_from_checkpoint(train_config.fine_tune_checkpoint,
                                           available_var_map)
-            return tf.train.Scaffold()
+            return tf.compat.v1.train.Scaffold()
           scaffold_fn = tpu_scaffold
         else:
-          tf.train.init_from_checkpoint(train_config.fine_tune_checkpoint,
+          tf.compat.v1.train.init_from_checkpoint(train_config.fine_tune_checkpoint,
                                         available_var_map)
 
     if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
@@ -287,8 +287,8 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
           prediction_dict, features[fields.InputDataFields.true_image_shape])
       losses = [loss_tensor for loss_tensor in losses_dict.values()]
       if train_config.add_regularization_loss:
-        regularization_losses = tf.get_collection(
-            tf.GraphKeys.REGULARIZATION_LOSSES)
+        regularization_losses = tf.compat.v1.get_collection(
+            tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)
         if regularization_losses:
           regularization_loss = tf.add_n(regularization_losses,
                                          name='regularization_loss')
@@ -304,13 +304,13 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
 
       # TODO(rathodv): Stop creating optimizer summary vars in EVAL mode once we
       # can write learning rate summaries on TPU without host calls.
-      global_step = tf.train.get_or_create_global_step()
+      global_step = tf.compat.v1.train.get_or_create_global_step()
       training_optimizer, optimizer_summary_vars = optimizer_builder.build(
           train_config.optimizer)
 
     if mode == tf.estimator.ModeKeys.TRAIN:
       if use_tpu:
-        training_optimizer = tf.contrib.tpu.CrossShardOptimizer(
+        training_optimizer = tf.compat.v1.tpu.CrossShardOptimizer(
             training_optimizer)
 
       # Optionally freeze some layers by setting their gradients to be zero.
@@ -322,7 +322,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
           train_config.freeze_variables
           if train_config.freeze_variables else None)
       trainable_variables = tf.contrib.framework.filter_variables(
-          tf.trainable_variables(),
+          tf.compat.v1.trainable_variables(),
           include_patterns=include_variables,
           exclude_patterns=exclude_variables)
 
@@ -332,7 +332,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
 
       if not use_tpu:
         for var in optimizer_summary_vars:
-          tf.summary.scalar(var.op.name, var)
+          tf.compat.v1.summary.scalar(var.op.name, var)
       summaries = [] if use_tpu else None
       train_op = tf.contrib.layers.optimize_loss(
           loss=total_loss,
@@ -346,7 +346,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
 
     if mode == tf.estimator.ModeKeys.PREDICT:
       export_outputs = {
-          tf.saved_model.signature_constants.PREDICT_METHOD_NAME:
+          tf.saved_model.PREDICT_METHOD_NAME:
               tf.estimator.export.PredictOutput(detections)
       }
 
@@ -381,7 +381,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
                 eval_dict, category_index, max_boxes_to_draw=eval_config.max_num_boxes_to_visualize,
                 min_score_thresh=eval_config.min_score_threshold,
                 use_normalized_coordinates=False))
-        img_summary = tf.summary.image('Detections_Left_Groundtruth_Right',
+        img_summary = tf.compat.v1.summary.image('Detections_Left_Groundtruth_Right',
                                        detection_and_groundtruth)
 
       # Eval metrics on a single example.
@@ -390,7 +390,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
           category_index.values(),
           eval_dict)
       for loss_key, loss_tensor in iter(losses_dict.items()):
-        eval_metric_ops[loss_key] = tf.metrics.mean(loss_tensor)
+        eval_metric_ops[loss_key] = tf.compat.v1.metrics.mean(loss_tensor)
       for var in optimizer_summary_vars:
         eval_metric_ops[var.op.name] = (var, tf.no_op())
       if img_summary is not None:
@@ -403,14 +403,14 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
         variables_to_restore = variable_averages.variables_to_restore()
         keep_checkpoint_every_n_hours = (
             train_config.keep_checkpoint_every_n_hours)
-        saver = tf.train.Saver(
+        saver = tf.compat.v1.train.Saver(
             variables_to_restore,
             keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
-        scaffold = tf.train.Scaffold(saver=saver)
+        scaffold = tf.compat.v1.train.Scaffold(saver=saver)
 
     # EVAL executes on CPU, so use regular non-TPU EstimatorSpec.
     if use_tpu and mode != tf.estimator.ModeKeys.EVAL:
-      return tf.contrib.tpu.TPUEstimatorSpec(
+      return tf.compat.v1.estimator.tpu.TPUEstimatorSpec(
           mode=mode,
           scaffold_fn=scaffold_fn,
           predictions=detections,
@@ -535,10 +535,10 @@ def create_estimator_and_inputs(run_config,
   predict_input_fn = create_predict_input_fn(
       model_config=model_config, predict_input_config=eval_input_config)
 
-  tf.logging.info('create_estimator_and_inputs: use_tpu %s', use_tpu)
+  tf.compat.v1.logging.info('create_estimator_and_inputs: use_tpu %s', use_tpu)
   model_fn = model_fn_creator(detection_model_fn, configs, hparams, use_tpu)
   if use_tpu_estimator:
-    estimator = tf.contrib.tpu.TPUEstimator(
+    estimator = tf.compat.v1.estimator.tpu.TPUEstimator(
         model_fn=model_fn,
         train_batch_size=train_config.batch_size,
         # For each core, only batch size 1 is supported for eval.
@@ -636,31 +636,31 @@ def continuous_eval(estimator, model_dir, input_fn, eval_steps, train_steps,
     name: Namescope for eval summary.
   """
   def terminate_eval():
-    tf.logging.info('Terminating eval after 180 seconds of no checkpoints')
+    tf.compat.v1.logging.info('Terminating eval after 180 seconds of no checkpoints')
     return True
 
-  for ckpt in tf.contrib.training.checkpoints_iterator(
+  for ckpt in tf.train.checkpoints_iterator(
       model_dir, min_interval_secs=180, timeout=None,
       timeout_fn=terminate_eval):
 
-    tf.logging.info('Starting Evaluation.')
+    tf.compat.v1.logging.info('Starting Evaluation.')
     try:
       eval_results = estimator.evaluate(
           input_fn=input_fn,
           steps=eval_steps,
           checkpoint_path=ckpt,
           name=name)
-      tf.logging.info('Eval results: %s' % eval_results)
+      tf.compat.v1.logging.info('Eval results: %s' % eval_results)
 
       # Terminate eval job when final checkpoint is reached
       current_step = int(os.path.basename(ckpt).split('-')[1])
       if current_step >= train_steps:
-        tf.logging.info(
+        tf.compat.v1.logging.info(
             'Evaluation finished after training step %d' % current_step)
         break
 
     except tf.errors.NotFoundError:
-      tf.logging.info(
+      tf.compat.v1.logging.info(
           'Checkpoint %s no longer exists, skipping checkpoint' % ckpt)
 
 
@@ -700,7 +700,7 @@ def populate_experiment(run_config,
     An `Experiment` that defines all aspects of training, evaluation, and
     export.
   """
-  tf.logging.warning('Experiment is being deprecated. Please use '
+  tf.compat.v1.logging.warning('Experiment is being deprecated. Please use '
                      'tf.estimator.train_and_evaluate(). See model_main.py for '
                      'an example.')
   train_and_eval_dict = create_estimator_and_inputs(
diff --git a/research/object_detection/model_lib_test.py b/research/object_detection/model_lib_test.py
index 1bfa9779..a5dc4535 100644
--- a/research/object_detection/model_lib_test.py
+++ b/research/object_detection/model_lib_test.py
@@ -42,19 +42,19 @@ MODEL_NAME_FOR_TEST = 'ssd_inception_v2_pets'
 
 def _get_data_path():
   """Returns an absolute path to TFRecord file."""
-  return os.path.join(tf.resource_loader.get_data_files_path(), 'test_data',
+  return os.path.join(tf.compat.v1.resource_loader.get_data_files_path(), 'test_data',
                       'pets_examples.record')
 
 
 def get_pipeline_config_path(model_name):
   """Returns path to the local pipeline config file."""
-  return os.path.join(tf.resource_loader.get_data_files_path(), 'samples',
+  return os.path.join(tf.compat.v1.resource_loader.get_data_files_path(), 'samples',
                       'configs', model_name + '.config')
 
 
 def _get_labelmap_path():
   """Returns an absolute path to label map file."""
-  return os.path.join(tf.resource_loader.get_data_files_path(), 'data',
+  return os.path.join(tf.compat.v1.resource_loader.get_data_files_path(), 'data',
                       'pet_label_map.pbtxt')
 
 
@@ -81,8 +81,8 @@ def _make_initializable_iterator(dataset):
   Returns:
     A `tf.data.Iterator`.
   """
-  iterator = dataset.make_initializable_iterator()
-  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
+  iterator = tf.compat.v1.data.make_initializable_iterator(dataset)
+  tf.compat.v1.add_to_collection(tf.compat.v1.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
   return iterator
 
 
@@ -90,7 +90,7 @@ class ModelLibTest(tf.test.TestCase):
 
   @classmethod
   def setUpClass(cls):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
 
   def _assert_model_fn_for_train_eval(self, configs, mode,
                                       class_agnostic=False):
@@ -170,7 +170,7 @@ class ModelLibTest(tf.test.TestCase):
       self.assertIsNone(estimator_spec.train_op)
       self.assertIsNotNone(estimator_spec.predictions)
       self.assertIsNotNone(estimator_spec.export_outputs)
-      self.assertIn(tf.saved_model.signature_constants.PREDICT_METHOD_NAME,
+      self.assertIn(tf.saved_model.PREDICT_METHOD_NAME,
                     estimator_spec.export_outputs)
 
   def test_model_fn_in_train_mode(self):
@@ -345,11 +345,11 @@ class ModelLibTest(tf.test.TestCase):
 class UnbatchTensorsTest(tf.test.TestCase):
 
   def test_unbatch_without_unpadding(self):
-    image_placeholder = tf.placeholder(tf.float32, [2, None, None, None])
-    groundtruth_boxes_placeholder = tf.placeholder(tf.float32, [2, None, None])
-    groundtruth_classes_placeholder = tf.placeholder(tf.float32,
+    image_placeholder = tf.compat.v1.placeholder(tf.float32, [2, None, None, None])
+    groundtruth_boxes_placeholder = tf.compat.v1.placeholder(tf.float32, [2, None, None])
+    groundtruth_classes_placeholder = tf.compat.v1.placeholder(tf.float32,
                                                      [2, None, None])
-    groundtruth_weights_placeholder = tf.placeholder(tf.float32, [2, None])
+    groundtruth_weights_placeholder = tf.compat.v1.placeholder(tf.float32, [2, None])
 
     tensor_dict = {
         fields.InputDataFields.image:
@@ -390,11 +390,11 @@ class UnbatchTensorsTest(tf.test.TestCase):
       self.assertAllEqual(groundtruth_weights_out.shape, [5])
 
   def test_unbatch_and_unpad_groundtruth_tensors(self):
-    image_placeholder = tf.placeholder(tf.float32, [2, None, None, None])
-    groundtruth_boxes_placeholder = tf.placeholder(tf.float32, [2, 5, None])
-    groundtruth_classes_placeholder = tf.placeholder(tf.float32, [2, 5, None])
-    groundtruth_weights_placeholder = tf.placeholder(tf.float32, [2, 5])
-    num_groundtruth_placeholder = tf.placeholder(tf.int32, [2])
+    image_placeholder = tf.compat.v1.placeholder(tf.float32, [2, None, None, None])
+    groundtruth_boxes_placeholder = tf.compat.v1.placeholder(tf.float32, [2, 5, None])
+    groundtruth_classes_placeholder = tf.compat.v1.placeholder(tf.float32, [2, 5, None])
+    groundtruth_weights_placeholder = tf.compat.v1.placeholder(tf.float32, [2, 5])
+    num_groundtruth_placeholder = tf.compat.v1.placeholder(tf.int32, [2])
 
     tensor_dict = {
         fields.InputDataFields.image:
diff --git a/research/object_detection/model_main.py b/research/object_detection/model_main.py
index 2082c848..8fca5947 100644
--- a/research/object_detection/model_main.py
+++ b/research/object_detection/model_main.py
@@ -98,4 +98,4 @@ def main(unused_argv):
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/model_tpu_main.py b/research/object_detection/model_tpu_main.py
index cb9f78ce..13509b2c 100644
--- a/research/object_detection/model_tpu_main.py
+++ b/research/object_detection/model_tpu_main.py
@@ -80,17 +80,17 @@ def main(unused_argv):
   flags.mark_flag_as_required('pipeline_config_path')
 
   tpu_cluster_resolver = (
-      tf.contrib.cluster_resolver.TPUClusterResolver(
+      tf.distribute.cluster_resolver.TPUClusterResolver(
           tpu=[FLAGS.tpu_name],
           zone=FLAGS.tpu_zone,
           project=FLAGS.gcp_project))
   tpu_grpc_url = tpu_cluster_resolver.get_master()
 
-  config = tf.contrib.tpu.RunConfig(
+  config = tf.compat.v1.estimator.tpu.RunConfig(
       master=tpu_grpc_url,
       evaluation_master=tpu_grpc_url,
       model_dir=FLAGS.model_dir,
-      tpu_config=tf.contrib.tpu.TPUConfig(
+      tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(
           iterations_per_loop=FLAGS.iterations_per_loop,
           num_shards=FLAGS.num_shards))
 
@@ -131,4 +131,4 @@ def main(unused_argv):
 
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
index f0cad235..bb1fc775 100644
--- a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
@@ -121,8 +121,8 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
 
     if image_height is None or image_width is None:
       shape_assert = tf.Assert(
-          tf.logical_and(tf.equal(tf.shape(preprocessed_inputs)[1], 256),
-                         tf.equal(tf.shape(preprocessed_inputs)[2], 256)),
+          tf.logical_and(tf.equal(tf.shape(input=preprocessed_inputs)[1], 256),
+                         tf.equal(tf.shape(input=preprocessed_inputs)[2], 256)),
           ['image size must be 256 in both height and width.'])
       with tf.control_dependencies([shape_assert]):
         preprocessed_inputs = tf.identity(preprocessed_inputs)
@@ -140,7 +140,7 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         'use_depthwise': self._use_depthwise,
     }
 
-    with tf.variable_scope('MobilenetV1',
+    with tf.compat.v1.variable_scope('MobilenetV1',
                            reuse=self._reuse_weights) as scope:
       with slim.arg_scope(
           mobilenet_v1.mobilenet_v1_arg_scope(is_training=None)):
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py
index 30b3dd4e..fdf6c263 100644
--- a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py
@@ -103,7 +103,7 @@ class FasterRCNNInceptionResnetV2FeatureExtractor(
       # Forces is_training to False to disable batch norm update.
       with slim.arg_scope([slim.batch_norm],
                           is_training=self._train_batch_norm):
-        with tf.variable_scope('InceptionResnetV2',
+        with tf.compat.v1.variable_scope('InceptionResnetV2',
                                reuse=self._reuse_weights) as scope:
           return inception_resnet_v2.inception_resnet_v2_base(
               preprocessed_inputs, final_endpoint='PreAuxLogits',
@@ -127,7 +127,7 @@ class FasterRCNNInceptionResnetV2FeatureExtractor(
         [batch_size * self.max_num_proposals, height, width, depth]
         representing box classifier features for each proposal.
     """
-    with tf.variable_scope('InceptionResnetV2', reuse=self._reuse_weights):
+    with tf.compat.v1.variable_scope('InceptionResnetV2', reuse=self._reuse_weights):
       with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope(
           weight_decay=self._weight_decay)):
         # Forces is_training to False to disable batch norm update.
@@ -135,20 +135,20 @@ class FasterRCNNInceptionResnetV2FeatureExtractor(
                             is_training=self._train_batch_norm):
           with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                               stride=1, padding='SAME'):
-            with tf.variable_scope('Mixed_7a'):
-              with tf.variable_scope('Branch_0'):
+            with tf.compat.v1.variable_scope('Mixed_7a'):
+              with tf.compat.v1.variable_scope('Branch_0'):
                 tower_conv = slim.conv2d(proposal_feature_maps,
                                          256, 1, scope='Conv2d_0a_1x1')
                 tower_conv_1 = slim.conv2d(
                     tower_conv, 384, 3, stride=2,
                     padding='VALID', scope='Conv2d_1a_3x3')
-              with tf.variable_scope('Branch_1'):
+              with tf.compat.v1.variable_scope('Branch_1'):
                 tower_conv1 = slim.conv2d(
                     proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1')
                 tower_conv1_1 = slim.conv2d(
                     tower_conv1, 288, 3, stride=2,
                     padding='VALID', scope='Conv2d_1a_3x3')
-              with tf.variable_scope('Branch_2'):
+              with tf.compat.v1.variable_scope('Branch_2'):
                 tower_conv2 = slim.conv2d(
                     proposal_feature_maps, 256, 1, scope='Conv2d_0a_1x1')
                 tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,
@@ -156,7 +156,7 @@ class FasterRCNNInceptionResnetV2FeatureExtractor(
                 tower_conv2_2 = slim.conv2d(
                     tower_conv2_1, 320, 3, stride=2,
                     padding='VALID', scope='Conv2d_1a_3x3')
-              with tf.variable_scope('Branch_3'):
+              with tf.compat.v1.variable_scope('Branch_3'):
                 tower_pool = slim.max_pool2d(
                     proposal_feature_maps, 3, stride=2, padding='VALID',
                     scope='MaxPool_1a_3x3')
@@ -195,7 +195,7 @@ class FasterRCNNInceptionResnetV2FeatureExtractor(
     """
 
     variables_to_restore = {}
-    for variable in tf.global_variables():
+    for variable in tf.compat.v1.global_variables():
       if variable.op.name.startswith(
           first_stage_feature_extractor_scope):
         var_name = variable.op.name.replace(
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py
index 1d9f088f..e6f45064 100644
--- a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py
@@ -33,13 +33,13 @@ class FasterRcnnInceptionResnetV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 299, 299, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -48,13 +48,13 @@ class FasterRcnnInceptionResnetV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_stride_eight(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=8)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 224, 224, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -63,13 +63,13 @@ class FasterRcnnInceptionResnetV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_half_size_input(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 112, 112, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -82,7 +82,7 @@ class FasterRcnnInceptionResnetV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [224, 224, 3], maxval=255, dtype=tf.float32)
     with self.assertRaises(ValueError):
       feature_extractor.extract_proposal_features(
@@ -91,14 +91,14 @@ class FasterRcnnInceptionResnetV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_box_classifier_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    proposal_feature_maps = tf.random_uniform(
+    proposal_feature_maps = tf.random.uniform(
         [2, 17, 17, 1088], maxval=255, dtype=tf.float32)
     proposal_classifier_features = (
         feature_extractor.extract_box_classifier_features(
             proposal_feature_maps, scope='TestScope'))
-    features_shape = tf.shape(proposal_classifier_features)
+    features_shape = tf.shape(input=proposal_classifier_features)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
diff --git a/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py b/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py
index fe067516..9dc019ac 100644
--- a/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py
@@ -120,12 +120,12 @@ class FasterRCNNInceptionV2FeatureExtractor(
 
     preprocessed_inputs.get_shape().assert_has_rank(4)
     shape_assert = tf.Assert(
-        tf.logical_and(tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),
-                       tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),
+        tf.logical_and(tf.greater_equal(tf.shape(input=preprocessed_inputs)[1], 33),
+                       tf.greater_equal(tf.shape(input=preprocessed_inputs)[2], 33)),
         ['image size must at least be 33 in both height and width.'])
 
     with tf.control_dependencies([shape_assert]):
-      with tf.variable_scope('InceptionV2',
+      with tf.compat.v1.variable_scope('InceptionV2',
                              reuse=self._reuse_weights) as scope:
         with _batch_norm_arg_scope([slim.conv2d, slim.separable_conv2d],
                                    batch_norm_scale=True,
@@ -156,12 +156,12 @@ class FasterRCNNInceptionV2FeatureExtractor(
     net = proposal_feature_maps
 
     depth = lambda d: max(int(d * self._depth_multiplier), self._min_depth)
-    trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
+    trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(0.0, stddev)
 
     data_format = 'NHWC'
     concat_dim = 3 if data_format == 'NHWC' else 1
 
-    with tf.variable_scope('InceptionV2', reuse=self._reuse_weights):
+    with tf.compat.v1.variable_scope('InceptionV2', reuse=self._reuse_weights):
       with slim.arg_scope(
           [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
           stride=1,
@@ -171,15 +171,15 @@ class FasterRCNNInceptionV2FeatureExtractor(
                                    batch_norm_scale=True,
                                    train_batch_norm=self._train_batch_norm):
 
-          with tf.variable_scope('Mixed_5a'):
-            with tf.variable_scope('Branch_0'):
+          with tf.compat.v1.variable_scope('Mixed_5a'):
+            with tf.compat.v1.variable_scope('Branch_0'):
               branch_0 = slim.conv2d(
                   net, depth(128), [1, 1],
                   weights_initializer=trunc_normal(0.09),
                   scope='Conv2d_0a_1x1')
               branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,
                                      scope='Conv2d_1a_3x3')
-            with tf.variable_scope('Branch_1'):
+            with tf.compat.v1.variable_scope('Branch_1'):
               branch_1 = slim.conv2d(
                   net, depth(192), [1, 1],
                   weights_initializer=trunc_normal(0.09),
@@ -188,23 +188,23 @@ class FasterRCNNInceptionV2FeatureExtractor(
                                      scope='Conv2d_0b_3x3')
               branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,
                                      scope='Conv2d_1a_3x3')
-            with tf.variable_scope('Branch_2'):
+            with tf.compat.v1.variable_scope('Branch_2'):
               branch_2 = slim.max_pool2d(net, [3, 3], stride=2,
                                          scope='MaxPool_1a_3x3')
             net = tf.concat([branch_0, branch_1, branch_2], concat_dim)
 
-          with tf.variable_scope('Mixed_5b'):
-            with tf.variable_scope('Branch_0'):
+          with tf.compat.v1.variable_scope('Mixed_5b'):
+            with tf.compat.v1.variable_scope('Branch_0'):
               branch_0 = slim.conv2d(net, depth(352), [1, 1],
                                      scope='Conv2d_0a_1x1')
-            with tf.variable_scope('Branch_1'):
+            with tf.compat.v1.variable_scope('Branch_1'):
               branch_1 = slim.conv2d(
                   net, depth(192), [1, 1],
                   weights_initializer=trunc_normal(0.09),
                   scope='Conv2d_0a_1x1')
               branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],
                                      scope='Conv2d_0b_3x3')
-            with tf.variable_scope('Branch_2'):
+            with tf.compat.v1.variable_scope('Branch_2'):
               branch_2 = slim.conv2d(
                   net, depth(160), [1, 1],
                   weights_initializer=trunc_normal(0.09),
@@ -213,7 +213,7 @@ class FasterRCNNInceptionV2FeatureExtractor(
                                      scope='Conv2d_0b_3x3')
               branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],
                                      scope='Conv2d_0c_3x3')
-            with tf.variable_scope('Branch_3'):
+            with tf.compat.v1.variable_scope('Branch_3'):
               branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
               branch_3 = slim.conv2d(
                   branch_3, depth(128), [1, 1],
@@ -222,18 +222,18 @@ class FasterRCNNInceptionV2FeatureExtractor(
             net = tf.concat([branch_0, branch_1, branch_2, branch_3],
                             concat_dim)
 
-          with tf.variable_scope('Mixed_5c'):
-            with tf.variable_scope('Branch_0'):
+          with tf.compat.v1.variable_scope('Mixed_5c'):
+            with tf.compat.v1.variable_scope('Branch_0'):
               branch_0 = slim.conv2d(net, depth(352), [1, 1],
                                      scope='Conv2d_0a_1x1')
-            with tf.variable_scope('Branch_1'):
+            with tf.compat.v1.variable_scope('Branch_1'):
               branch_1 = slim.conv2d(
                   net, depth(192), [1, 1],
                   weights_initializer=trunc_normal(0.09),
                   scope='Conv2d_0a_1x1')
               branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],
                                      scope='Conv2d_0b_3x3')
-            with tf.variable_scope('Branch_2'):
+            with tf.compat.v1.variable_scope('Branch_2'):
               branch_2 = slim.conv2d(
                   net, depth(192), [1, 1],
                   weights_initializer=trunc_normal(0.09),
@@ -242,7 +242,7 @@ class FasterRCNNInceptionV2FeatureExtractor(
                                      scope='Conv2d_0b_3x3')
               branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],
                                      scope='Conv2d_0c_3x3')
-            with tf.variable_scope('Branch_3'):
+            with tf.compat.v1.variable_scope('Branch_3'):
               branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
               branch_3 = slim.conv2d(
                   branch_3, depth(128), [1, 1],
diff --git a/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py
index 6b5bc2f9..c36ae8ce 100644
--- a/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py
@@ -34,13 +34,13 @@ class FasterRcnnInceptionV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [4, 224, 224, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -49,13 +49,13 @@ class FasterRcnnInceptionV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_stride_eight(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=8)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [4, 224, 224, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -64,13 +64,13 @@ class FasterRcnnInceptionV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_half_size_input(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 112, 112, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -83,12 +83,12 @@ class FasterRcnnInceptionV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_dies_on_very_small_images(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
+    preprocessed_inputs = tf.compat.v1.placeholder(tf.float32, (4, None, None, 3))
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       with self.assertRaises(tf.errors.InvalidArgumentError):
@@ -99,7 +99,7 @@ class FasterRcnnInceptionV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [224, 224, 3], maxval=255, dtype=tf.float32)
     with self.assertRaises(ValueError):
       feature_extractor.extract_proposal_features(
@@ -108,14 +108,14 @@ class FasterRcnnInceptionV2FeatureExtractorTest(tf.test.TestCase):
   def test_extract_box_classifier_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    proposal_feature_maps = tf.random_uniform(
+    proposal_feature_maps = tf.random.uniform(
         [3, 14, 14, 576], maxval=255, dtype=tf.float32)
     proposal_classifier_features = (
         feature_extractor.extract_box_classifier_features(
             proposal_feature_maps, scope='TestScope'))
-    features_shape = tf.shape(proposal_classifier_features)
+    features_shape = tf.shape(input=proposal_classifier_features)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
diff --git a/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py b/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py
index 52c744b8..4bc8e715 100644
--- a/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py
@@ -135,7 +135,7 @@ class FasterRCNNMobilenetV1FeatureExtractor(
         mobilenet_v1.mobilenet_v1_arg_scope(
             is_training=self._train_batch_norm,
             weight_decay=self._weight_decay)):
-      with tf.variable_scope('MobilenetV1',
+      with tf.compat.v1.variable_scope('MobilenetV1',
                              reuse=self._reuse_weights) as scope:
         params = {}
         if self._skip_last_stride:
@@ -173,7 +173,7 @@ class FasterRCNNMobilenetV1FeatureExtractor(
       conv_depth = int(float(conv_depth) * conv_depth_ratio)
 
     depth = lambda d: max(int(d * 1.0), 16)
-    with tf.variable_scope('MobilenetV1', reuse=self._reuse_weights):
+    with tf.compat.v1.variable_scope('MobilenetV1', reuse=self._reuse_weights):
       with slim.arg_scope(
           mobilenet_v1.mobilenet_v1_arg_scope(
               is_training=self._train_batch_norm,
diff --git a/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py
index fcefe616..64c36139 100644
--- a/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py
@@ -34,13 +34,13 @@ class FasterRcnnMobilenetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [4, 224, 224, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -49,13 +49,13 @@ class FasterRcnnMobilenetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_stride_eight(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=8)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [4, 224, 224, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -64,13 +64,13 @@ class FasterRcnnMobilenetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_half_size_input(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 112, 112, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -83,12 +83,12 @@ class FasterRcnnMobilenetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_dies_on_very_small_images(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
+    preprocessed_inputs = tf.compat.v1.placeholder(tf.float32, (4, None, None, 3))
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       with self.assertRaises(tf.errors.InvalidArgumentError):
@@ -99,7 +99,7 @@ class FasterRcnnMobilenetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [224, 224, 3], maxval=255, dtype=tf.float32)
     with self.assertRaises(ValueError):
       feature_extractor.extract_proposal_features(
@@ -108,14 +108,14 @@ class FasterRcnnMobilenetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_box_classifier_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    proposal_feature_maps = tf.random_uniform(
+    proposal_feature_maps = tf.random.uniform(
         [3, 14, 14, 576], maxval=255, dtype=tf.float32)
     proposal_classifier_features = (
         feature_extractor.extract_box_classifier_features(
             proposal_feature_maps, scope='TestScope'))
-    features_shape = tf.shape(proposal_classifier_features)
+    features_shape = tf.shape(input=proposal_classifier_features)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
diff --git a/research/object_detection/models/faster_rcnn_nas_feature_extractor.py b/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
index 5fa6bf75..7d86f6c8 100644
--- a/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
@@ -103,7 +103,7 @@ def _build_nasnet_base(hidden_previous,
   # Final nonlinearity.
   # Note that we have dropped the final pooling, dropout and softmax layers
   # from the default nasnet version.
-  with tf.variable_scope('final_layer'):
+  with tf.compat.v1.variable_scope('final_layer'):
     net = tf.nn.relu(net)
   return net
 
@@ -307,7 +307,7 @@ class FasterRCNNNASFeatureExtractor(
     # Note that the NAS checkpoint only contains the moving average version of
     # the Variables so we need to generate an appropriate dictionary mapping.
     variables_to_restore = {}
-    for variable in tf.global_variables():
+    for variable in tf.compat.v1.global_variables():
       if variable.op.name.startswith(
           first_stage_feature_extractor_scope):
         var_name = variable.op.name.replace(
diff --git a/research/object_detection/models/faster_rcnn_nas_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_nas_feature_extractor_test.py
index cecfc4f8..caa7de85 100644
--- a/research/object_detection/models/faster_rcnn_nas_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_nas_feature_extractor_test.py
@@ -33,13 +33,13 @@ class FasterRcnnNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 299, 299, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -48,13 +48,13 @@ class FasterRcnnNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_input_size_224(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 224, 224, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -63,13 +63,13 @@ class FasterRcnnNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_input_size_112(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 112, 112, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -82,7 +82,7 @@ class FasterRcnnNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [224, 224, 3], maxval=255, dtype=tf.float32)
     with self.assertRaises(ValueError):
       feature_extractor.extract_proposal_features(
@@ -91,14 +91,14 @@ class FasterRcnnNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_box_classifier_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    proposal_feature_maps = tf.random_uniform(
+    proposal_feature_maps = tf.random.uniform(
         [2, 17, 17, 1088], maxval=255, dtype=tf.float32)
     proposal_classifier_features = (
         feature_extractor.extract_box_classifier_features(
             proposal_feature_maps, scope='TestScope'))
-    features_shape = tf.shape(proposal_classifier_features)
+    features_shape = tf.shape(input=proposal_classifier_features)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
diff --git a/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py b/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py
index b5d0f43a..b5b37325 100644
--- a/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py
@@ -104,7 +104,7 @@ def _build_pnasnet_base(
   # Final nonlinearity.
   # Note that we have dropped the final pooling, dropout and softmax layers
   # from the default pnasnet version.
-  with tf.variable_scope('final_layer'):
+  with tf.compat.v1.variable_scope('final_layer'):
     net = tf.nn.relu(net)
   return net
 
@@ -302,7 +302,7 @@ class FasterRCNNPNASFeatureExtractor(
       the model graph.
     """
     variables_to_restore = {}
-    for variable in tf.global_variables():
+    for variable in tf.compat.v1.global_variables():
       if variable.op.name.startswith(
           first_stage_feature_extractor_scope):
         var_name = variable.op.name.replace(
diff --git a/research/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py
index 6bb36804..6fdedc8c 100644
--- a/research/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py
@@ -33,13 +33,13 @@ class FasterRcnnPNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 299, 299, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -48,13 +48,13 @@ class FasterRcnnPNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_input_size_224(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 224, 224, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -63,13 +63,13 @@ class FasterRcnnPNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_input_size_112(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 112, 112, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -82,7 +82,7 @@ class FasterRcnnPNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [224, 224, 3], maxval=255, dtype=tf.float32)
     with self.assertRaises(ValueError):
       feature_extractor.extract_proposal_features(
@@ -91,14 +91,14 @@ class FasterRcnnPNASFeatureExtractorTest(tf.test.TestCase):
   def test_extract_box_classifier_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    proposal_feature_maps = tf.random_uniform(
+    proposal_feature_maps = tf.random.uniform(
         [2, 17, 17, 1088], maxval=255, dtype=tf.float32)
     proposal_classifier_features = (
         feature_extractor.extract_box_classifier_features(
             proposal_feature_maps, scope='TestScope'))
-    features_shape = tf.shape(proposal_classifier_features)
+    features_shape = tf.shape(input=proposal_classifier_features)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
diff --git a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
index 286deae3..5743ddfd 100644
--- a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
@@ -108,8 +108,8 @@ class FasterRCNNResnetV1FeatureExtractor(
                        'tensor of shape %s' % preprocessed_inputs.get_shape())
     shape_assert = tf.Assert(
         tf.logical_and(
-            tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),
-            tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),
+            tf.greater_equal(tf.shape(input=preprocessed_inputs)[1], 33),
+            tf.greater_equal(tf.shape(input=preprocessed_inputs)[2], 33)),
         ['image size must at least be 33 in both height and width.'])
 
     with tf.control_dependencies([shape_assert]):
@@ -121,7 +121,7 @@ class FasterRCNNResnetV1FeatureExtractor(
               batch_norm_epsilon=1e-5,
               batch_norm_scale=True,
               weight_decay=self._weight_decay)):
-        with tf.variable_scope(
+        with tf.compat.v1.variable_scope(
             self._architecture, reuse=self._reuse_weights) as var_scope:
           _, activations = self._resnet_model(
               preprocessed_inputs,
@@ -149,7 +149,7 @@ class FasterRCNNResnetV1FeatureExtractor(
         [batch_size * self.max_num_proposals, height, width, depth]
         representing box classifier features for each proposal.
     """
-    with tf.variable_scope(self._architecture, reuse=self._reuse_weights):
+    with tf.compat.v1.variable_scope(self._architecture, reuse=self._reuse_weights):
       with slim.arg_scope(
           resnet_utils.resnet_arg_scope(
               batch_norm_epsilon=1e-5,
diff --git a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py
index e2a336f0..017b745f 100644
--- a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py
@@ -45,13 +45,13 @@ class FasterRcnnResnetV1FeatureExtractorTest(tf.test.TestCase):
     for architecture in ['resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152']:
       feature_extractor = self._build_feature_extractor(
           first_stage_features_stride=16, architecture=architecture)
-      preprocessed_inputs = tf.random_uniform(
+      preprocessed_inputs = tf.random.uniform(
           [4, 224, 224, 3], maxval=255, dtype=tf.float32)
       rpn_feature_map, _ = feature_extractor.extract_proposal_features(
           preprocessed_inputs, scope='TestScope')
-      features_shape = tf.shape(rpn_feature_map)
+      features_shape = tf.shape(input=rpn_feature_map)
 
-      init_op = tf.global_variables_initializer()
+      init_op = tf.compat.v1.global_variables_initializer()
       with self.test_session() as sess:
         sess.run(init_op)
         features_shape_out = sess.run(features_shape)
@@ -60,13 +60,13 @@ class FasterRcnnResnetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_stride_eight(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=8)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [4, 224, 224, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -75,13 +75,13 @@ class FasterRcnnResnetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_half_size_input(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [1, 112, 112, 3], maxval=255, dtype=tf.float32)
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
@@ -94,12 +94,12 @@ class FasterRcnnResnetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_dies_on_very_small_images(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
+    preprocessed_inputs = tf.compat.v1.placeholder(tf.float32, (4, None, None, 3))
     rpn_feature_map, _ = feature_extractor.extract_proposal_features(
         preprocessed_inputs, scope='TestScope')
-    features_shape = tf.shape(rpn_feature_map)
+    features_shape = tf.shape(input=rpn_feature_map)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       with self.assertRaises(tf.errors.InvalidArgumentError):
@@ -110,7 +110,7 @@ class FasterRcnnResnetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_proposal_features_dies_with_incorrect_rank_inputs(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    preprocessed_inputs = tf.random_uniform(
+    preprocessed_inputs = tf.random.uniform(
         [224, 224, 3], maxval=255, dtype=tf.float32)
     with self.assertRaises(ValueError):
       feature_extractor.extract_proposal_features(
@@ -119,14 +119,14 @@ class FasterRcnnResnetV1FeatureExtractorTest(tf.test.TestCase):
   def test_extract_box_classifier_features_returns_expected_size(self):
     feature_extractor = self._build_feature_extractor(
         first_stage_features_stride=16)
-    proposal_feature_maps = tf.random_uniform(
+    proposal_feature_maps = tf.random.uniform(
         [3, 7, 7, 1024], maxval=255, dtype=tf.float32)
     proposal_classifier_features = (
         feature_extractor.extract_box_classifier_features(
             proposal_feature_maps, scope='TestScope'))
-    features_shape = tf.shape(proposal_classifier_features)
+    features_shape = tf.shape(input=proposal_classifier_features)
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       features_shape_out = sess.run(features_shape)
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
index 42ae19e1..caa1ebf7 100644
--- a/research/object_detection/models/feature_map_generators.py
+++ b/research/object_detection/models/feature_map_generators.py
@@ -195,7 +195,7 @@ def fpn_top_down_feature_maps(image_features, depth, scope=None):
     feature_maps: an OrderedDict mapping keys (feature map names) to
       tensors where each tensor has shape [batch, height_i, width_i, depth_i].
   """
-  with tf.name_scope(scope, 'top_down'):
+  with tf.compat.v1.name_scope(scope, 'top_down'):
     num_levels = len(image_features)
     output_feature_maps_list = []
     output_feature_map_keys = []
diff --git a/research/object_detection/models/feature_map_generators_test.py b/research/object_detection/models/feature_map_generators_test.py
index 540bc4ef..6afbdae3 100644
--- a/research/object_detection/models/feature_map_generators_test.py
+++ b/research/object_detection/models/feature_map_generators_test.py
@@ -45,9 +45,9 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
 
   def test_get_expected_feature_map_shapes_with_inception_v2(self):
     image_features = {
-        'Mixed_3c': tf.random_uniform([4, 28, 28, 256], dtype=tf.float32),
-        'Mixed_4c': tf.random_uniform([4, 14, 14, 576], dtype=tf.float32),
-        'Mixed_5c': tf.random_uniform([4, 7, 7, 1024], dtype=tf.float32)
+        'Mixed_3c': tf.random.uniform([4, 28, 28, 256], dtype=tf.float32),
+        'Mixed_4c': tf.random.uniform([4, 14, 14, 576], dtype=tf.float32),
+        'Mixed_5c': tf.random.uniform([4, 7, 7, 1024], dtype=tf.float32)
     }
     feature_maps = feature_map_generators.multi_resolution_feature_maps(
         feature_map_layout=INCEPTION_V2_LAYOUT,
@@ -64,7 +64,7 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
         'Mixed_5c_2_Conv2d_4_3x3_s2_256': (4, 2, 2, 256),
         'Mixed_5c_2_Conv2d_5_3x3_s2_256': (4, 1, 1, 256)}
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       out_feature_maps = sess.run(feature_maps)
@@ -74,9 +74,9 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
 
   def test_get_expected_feature_map_shapes_with_inception_v3(self):
     image_features = {
-        'Mixed_5d': tf.random_uniform([4, 35, 35, 256], dtype=tf.float32),
-        'Mixed_6e': tf.random_uniform([4, 17, 17, 576], dtype=tf.float32),
-        'Mixed_7c': tf.random_uniform([4, 8, 8, 1024], dtype=tf.float32)
+        'Mixed_5d': tf.random.uniform([4, 35, 35, 256], dtype=tf.float32),
+        'Mixed_6e': tf.random.uniform([4, 17, 17, 576], dtype=tf.float32),
+        'Mixed_7c': tf.random.uniform([4, 8, 8, 1024], dtype=tf.float32)
     }
 
     feature_maps = feature_map_generators.multi_resolution_feature_maps(
@@ -94,7 +94,7 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
         'Mixed_7c_2_Conv2d_4_3x3_s2_256': (4, 2, 2, 256),
         'Mixed_7c_2_Conv2d_5_3x3_s2_128': (4, 1, 1, 128)}
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       out_feature_maps = sess.run(feature_maps)
@@ -105,9 +105,9 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
   def test_get_expected_feature_map_shapes_with_embedded_ssd_mobilenet_v1(
       self):
     image_features = {
-        'Conv2d_11_pointwise': tf.random_uniform([4, 16, 16, 512],
+        'Conv2d_11_pointwise': tf.random.uniform([4, 16, 16, 512],
                                                  dtype=tf.float32),
-        'Conv2d_13_pointwise': tf.random_uniform([4, 8, 8, 1024],
+        'Conv2d_13_pointwise': tf.random.uniform([4, 8, 8, 1024],
                                                  dtype=tf.float32),
     }
 
@@ -125,7 +125,7 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
         'Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256': (4, 2, 2, 256),
         'Conv2d_13_pointwise_2_Conv2d_4_2x2_s2_256': (4, 1, 1, 256)}
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       out_feature_maps = sess.run(feature_maps)
@@ -138,10 +138,10 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase):
 
   def test_get_expected_feature_map_shapes(self):
     image_features = [
-        ('block2', tf.random_uniform([4, 8, 8, 256], dtype=tf.float32)),
-        ('block3', tf.random_uniform([4, 4, 4, 256], dtype=tf.float32)),
-        ('block4', tf.random_uniform([4, 2, 2, 256], dtype=tf.float32)),
-        ('block5', tf.random_uniform([4, 1, 1, 256], dtype=tf.float32))
+        ('block2', tf.random.uniform([4, 8, 8, 256], dtype=tf.float32)),
+        ('block3', tf.random.uniform([4, 4, 4, 256], dtype=tf.float32)),
+        ('block4', tf.random.uniform([4, 2, 2, 256], dtype=tf.float32)),
+        ('block5', tf.random.uniform([4, 1, 1, 256], dtype=tf.float32))
     ]
     feature_maps = feature_map_generators.fpn_top_down_feature_maps(
         image_features=image_features, depth=128)
@@ -153,7 +153,7 @@ class FPNFeatureMapGeneratorTest(tf.test.TestCase):
         'top_down_block5': (4, 1, 1, 128)
     }
 
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       out_feature_maps = sess.run(feature_maps)
diff --git a/research/object_detection/models/ssd_feature_extractor_test.py b/research/object_detection/models/ssd_feature_extractor_test.py
index 899214b2..646c7773 100644
--- a/research/object_detection/models/ssd_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_feature_extractor_test.py
@@ -71,7 +71,7 @@ class SsdFeatureExtractorTestBase(test_case.TestCase):
       feature_extractor = self._create_feature_extractor(depth_multiplier,
                                                          pad_to_multiple,
                                                          use_explicit_padding)
-      image_tensor = tf.random_uniform([batch_size, image_height, image_width,
+      image_tensor = tf.random.uniform([batch_size, image_height, image_width,
                                         3], dtype=tf.float32)
       feature_maps = feature_extractor.extract_features(image_tensor)
       return feature_maps
@@ -88,11 +88,11 @@ class SsdFeatureExtractorTestBase(test_case.TestCase):
       self, image_height, image_width, depth_multiplier, pad_to_multiple):
     feature_extractor = self._create_feature_extractor(depth_multiplier,
                                                        pad_to_multiple)
-    preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
+    preprocessed_inputs = tf.compat.v1.placeholder(tf.float32, (4, None, None, 3))
     feature_maps = feature_extractor.extract_features(preprocessed_inputs)
     test_preprocessed_image = np.random.rand(4, image_height, image_width, 3)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       with self.assertRaises(tf.errors.InvalidArgumentError):
         sess.run(feature_maps,
                  feed_dict={preprocessed_inputs: test_preprocessed_image})
@@ -103,8 +103,8 @@ class SsdFeatureExtractorTestBase(test_case.TestCase):
     with g.as_default():
       feature_extractor = self._create_feature_extractor(
           depth_multiplier, pad_to_multiple)
-      preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
+      preprocessed_inputs = tf.compat.v1.placeholder(tf.float32, (4, None, None, 3))
       feature_extractor.extract_features(preprocessed_inputs)
-      variables = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+      variables = g.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)
       for variable in variables:
         self.assertTrue(variable.name.startswith(scope_name))
diff --git a/research/object_detection/models/ssd_inception_v2_feature_extractor.py b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
index f7e97527..2ca725fa 100644
--- a/research/object_detection/models/ssd_inception_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
@@ -115,7 +115,7 @@ class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     }
 
     with slim.arg_scope(self._conv_hyperparams_fn()):
-      with tf.variable_scope('InceptionV2',
+      with tf.compat.v1.variable_scope('InceptionV2',
                              reuse=self._reuse_weights) as scope:
         _, image_features = inception_v2.inception_v2_base(
             ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
diff --git a/research/object_detection/models/ssd_inception_v3_feature_extractor.py b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
index b0ff9ab6..70b365e0 100644
--- a/research/object_detection/models/ssd_inception_v3_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
@@ -116,7 +116,7 @@ class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     }
 
     with slim.arg_scope(self._conv_hyperparams_fn()):
-      with tf.variable_scope('InceptionV3', reuse=self._reuse_weights) as scope:
+      with tf.compat.v1.variable_scope('InceptionV3', reuse=self._reuse_weights) as scope:
         _, image_features = inception_v3.inception_v3_base(
             ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
             final_endpoint='Mixed_7c',
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
index 3b859ab0..e18a9f5d 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
@@ -109,7 +109,7 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         'use_depthwise': self._use_depthwise,
     }
 
-    with tf.variable_scope('MobilenetV1',
+    with tf.compat.v1.variable_scope('MobilenetV1',
                            reuse=self._reuse_weights) as scope:
       with slim.arg_scope(
           mobilenet_v1.mobilenet_v1_arg_scope(
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
index d3a9542b..54f3ac74 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
@@ -153,14 +153,14 @@ class SsdMobilenetV1FeatureExtractorTest(
     image_width = 40
     depth_multiplier = 1
     pad_to_multiple = 1
-    image_placeholder = tf.placeholder(tf.float32,
+    image_placeholder = tf.compat.v1.placeholder(tf.float32,
                                        [1, image_height, image_width, 3])
     feature_extractor = self._create_feature_extractor(depth_multiplier,
                                                        pad_to_multiple)
     preprocessed_image = feature_extractor.preprocess(image_placeholder)
     _ = feature_extractor.extract_features(preprocessed_image)
     self.assertTrue(any(op.type == 'FusedBatchNorm'
-                        for op in tf.get_default_graph().get_operations()))
+                        for op in tf.compat.v1.get_default_graph().get_operations()))
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
index bb5bf73c..b8dfa0b0 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
@@ -114,7 +114,7 @@ class SSDMobileNetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     preprocessed_inputs = shape_utils.check_min_image_dim(
         33, preprocessed_inputs)
 
-    with tf.variable_scope('MobilenetV1',
+    with tf.compat.v1.variable_scope('MobilenetV1',
                            reuse=self._reuse_weights) as scope:
       with slim.arg_scope(
           mobilenet_v1.mobilenet_v1_arg_scope(
@@ -132,7 +132,7 @@ class SSDMobileNetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
 
       depth_fn = lambda d: max(int(d * self._depth_multiplier), self._min_depth)
       with slim.arg_scope(self._conv_hyperparams_fn()):
-        with tf.variable_scope('fpn', reuse=self._reuse_weights):
+        with tf.compat.v1.variable_scope('fpn', reuse=self._reuse_weights):
           feature_blocks = [
               'Conv2d_3_pointwise', 'Conv2d_5_pointwise', 'Conv2d_11_pointwise',
               'Conv2d_13_pointwise'
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
index 72771ceb..ae7b2d2f 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
@@ -157,7 +157,7 @@ class SsdMobilenetV1FpnFeatureExtractorTest(
     image_width = 256
     depth_multiplier = 1
     pad_to_multiple = 1
-    image_placeholder = tf.placeholder(tf.float32,
+    image_placeholder = tf.compat.v1.placeholder(tf.float32,
                                        [1, image_height, image_width, 3])
     feature_extractor = self._create_feature_extractor(depth_multiplier,
                                                        pad_to_multiple)
@@ -165,7 +165,7 @@ class SsdMobilenetV1FpnFeatureExtractorTest(
     _ = feature_extractor.extract_features(preprocessed_image)
     self.assertTrue(
         any(op.type == 'FusedBatchNorm'
-            for op in tf.get_default_graph().get_operations()))
+            for op in tf.compat.v1.get_default_graph().get_operations()))
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
index 34e9cb4e..f10260b0 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
@@ -59,7 +59,7 @@ class SSDMobileNetV1PpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     preprocessed_inputs = shape_utils.check_min_image_dim(
         33, preprocessed_inputs)
 
-    with tf.variable_scope('MobilenetV1',
+    with tf.compat.v1.variable_scope('MobilenetV1',
                            reuse=self._reuse_weights) as scope:
       with slim.arg_scope(
           mobilenet_v1.mobilenet_v1_arg_scope(
diff --git a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py
index 1fb17df2..4e730457 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py
@@ -172,14 +172,14 @@ class SsdMobilenetV1PpnFeatureExtractorTest(
     image_width = 320
     depth_multiplier = 1
     pad_to_multiple = 1
-    image_placeholder = tf.placeholder(tf.float32,
+    image_placeholder = tf.compat.v1.placeholder(tf.float32,
                                        [1, image_height, image_width, 3])
     feature_extractor = self._create_feature_extractor(depth_multiplier,
                                                        pad_to_multiple)
     preprocessed_image = feature_extractor.preprocess(image_placeholder)
     _ = feature_extractor.extract_features(preprocessed_image)
     self.assertTrue(any(op.type == 'FusedBatchNorm'
-                        for op in tf.get_default_graph().get_operations()))
+                        for op in tf.compat.v1.get_default_graph().get_operations()))
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
index d8e5ccc9..f46c3c47 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
@@ -111,7 +111,7 @@ class SSDMobileNetV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         'use_explicit_padding': self._use_explicit_padding,
     }
 
-    with tf.variable_scope('MobilenetV2', reuse=self._reuse_weights) as scope:
+    with tf.compat.v1.variable_scope('MobilenetV2', reuse=self._reuse_weights) as scope:
       with slim.arg_scope(
           mobilenet_v2.training_scope(is_training=None, bn_decay=0.9997)), \
           slim.arg_scope(
diff --git a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py
index 0b374749..e424a0c9 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py
@@ -140,14 +140,14 @@ class SsdMobilenetV2FeatureExtractorTest(
     image_width = 40
     depth_multiplier = 1
     pad_to_multiple = 1
-    image_placeholder = tf.placeholder(tf.float32,
+    image_placeholder = tf.compat.v1.placeholder(tf.float32,
                                        [1, image_height, image_width, 3])
     feature_extractor = self._create_feature_extractor(depth_multiplier,
                                                        pad_to_multiple)
     preprocessed_image = feature_extractor.preprocess(image_placeholder)
     _ = feature_extractor.extract_features(preprocessed_image)
     self.assertTrue(any(op.type == 'FusedBatchNorm'
-                        for op in tf.get_default_graph().get_operations()))
+                        for op in tf.compat.v1.get_default_graph().get_operations()))
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
index 5d0c78c5..bd5a33f4 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -152,7 +152,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     preprocessed_inputs = shape_utils.check_min_image_dim(
         129, preprocessed_inputs)
 
-    with tf.variable_scope(
+    with tf.compat.v1.variable_scope(
         self._resnet_scope_name, reuse=self._reuse_weights) as scope:
       with slim.arg_scope(resnet_v1.resnet_arg_scope()):
         with (slim.arg_scope(self._conv_hyperparams_fn())
@@ -169,7 +169,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
               scope=scope)
           image_features = self._filter_features(image_features)
       with slim.arg_scope(self._conv_hyperparams_fn()):
-        with tf.variable_scope(self._fpn_scope_name,
+        with tf.compat.v1.variable_scope(self._fpn_scope_name,
                                reuse=self._reuse_weights):
           base_fpn_max_level = min(self._fpn_max_level, 5)
           feature_block_list = []
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
index 186f2b17..3fb3fc34 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
@@ -96,9 +96,9 @@ class SSDResnetFPNFeatureExtractorTestBase(
     with g.as_default():
       feature_extractor = self._create_feature_extractor(
           depth_multiplier, pad_to_multiple)
-      preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
+      preprocessed_inputs = tf.compat.v1.placeholder(tf.float32, (4, None, None, 3))
       feature_extractor.extract_features(preprocessed_inputs)
-      variables = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+      variables = g.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)
       for variable in variables:
         self.assertTrue(
             variable.name.startswith(self._resnet_scope_name())
diff --git a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
index 13422503..666b54f1 100644
--- a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
@@ -130,7 +130,7 @@ class _SSDResnetPpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     preprocessed_inputs = shape_utils.check_min_image_dim(
         129, preprocessed_inputs)
 
-    with tf.variable_scope(
+    with tf.compat.v1.variable_scope(
         self._resnet_scope_name, reuse=self._reuse_weights) as scope:
       with slim.arg_scope(resnet_v1.resnet_arg_scope()):
         with (slim.arg_scope(self._conv_hyperparams_fn())
diff --git a/research/object_detection/predictors/convolutional_box_predictor.py b/research/object_detection/predictors/convolutional_box_predictor.py
index 4feb010a..4110b34d 100644
--- a/research/object_detection/predictors/convolutional_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_box_predictor.py
@@ -129,7 +129,7 @@ class ConvolutionalBoxPredictor(box_predictor.BoxPredictor):
     box_predictor_scopes = [_NoopVariableScope()]
     if len(image_features) > 1:
       box_predictor_scopes = [
-          tf.variable_scope('BoxPredictor_{}'.format(i))
+          tf.compat.v1.variable_scope('BoxPredictor_{}'.format(i))
           for i in range(len(image_features))
       ]
     for (image_feature,
@@ -143,14 +143,14 @@ class ConvolutionalBoxPredictor(box_predictor.BoxPredictor):
             # Add additional conv layers before the class predictor.
             features_depth = static_shape.get_depth(image_feature.get_shape())
             depth = max(min(features_depth, self._max_depth), self._min_depth)
-            tf.logging.info('depth of additional conv before box predictor: {}'.
+            tf.compat.v1.logging.info('depth of additional conv before box predictor: {}'.
                             format(depth))
             if depth > 0 and self._num_layers_before_predictor > 0:
               for i in range(self._num_layers_before_predictor):
                 net = slim.conv2d(
                     net,
                     depth, [1, 1],
-                    reuse=tf.AUTO_REUSE,
+                    reuse=tf.compat.v1.AUTO_REUSE,
                     scope='Conv2d_%d_1x1_%d' % (i, depth))
             sorted_keys = sorted(self._other_heads.keys())
             sorted_keys.append(BOX_ENCODINGS)
@@ -354,7 +354,7 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
     if has_different_feature_channels:
       inserted_layer_counter = 0
       target_channel = max(set(feature_channels), key=feature_channels.count)
-      tf.logging.info('Not all feature maps have the same number of '
+      tf.compat.v1.logging.info('Not all feature maps have the same number of '
                       'channels, found: {}, addition project layers '
                       'to bring all feature maps to uniform channels '
                       'of {}'.format(feature_channels, target_channel))
@@ -372,8 +372,8 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
                         num_predictions_per_location) in enumerate(
                             zip(image_features,
                                 num_predictions_per_location_list)):
-      with tf.variable_scope('WeightSharedConvolutionalBoxPredictor',
-                             reuse=tf.AUTO_REUSE):
+      with tf.compat.v1.variable_scope('WeightSharedConvolutionalBoxPredictor',
+                             reuse=tf.compat.v1.AUTO_REUSE):
         with slim.arg_scope(self._conv_hyperparams_fn()):
           (image_feature,
            inserted_layer_counter) = self._insert_additional_projection_layer(
diff --git a/research/object_detection/predictors/convolutional_box_predictor_test.py b/research/object_detection/predictors/convolutional_box_predictor_test.py
index 6c8126c7..90531623 100644
--- a/research/object_detection/predictors/convolutional_box_predictor_test.py
+++ b/research/object_detection/predictors/convolutional_box_predictor_test.py
@@ -136,7 +136,7 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
 
   def test_get_predictions_with_feature_maps_of_dynamic_shape(
       self):
-    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
+    image_features = tf.compat.v1.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
     conv_box_predictor = (
         box_predictor_builder.build_convolutional_box_predictor(
             is_training=False,
@@ -157,7 +157,7 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
     objectness_predictions = tf.concat(
         box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
         axis=1)
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
 
     resolution = 32
     expected_num_anchors = resolution*resolution*5
@@ -165,11 +165,11 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
       sess.run(init_op)
       (box_encodings_shape,
        objectness_predictions_shape) = sess.run(
-           [tf.shape(box_encodings), tf.shape(objectness_predictions)],
+           [tf.shape(input=box_encodings), tf.shape(input=objectness_predictions)],
            feed_dict={image_features:
                       np.random.rand(4, resolution, resolution, 64)})
       actual_variable_set = set(
-          [var.op.name for var in tf.trainable_variables()])
+          [var.op.name for var in tf.compat.v1.trainable_variables()])
       self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])
       self.assertAllEqual(objectness_predictions_shape,
                           [4, expected_num_anchors, 1])
@@ -183,7 +183,7 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
     self.assertEqual(expected_variable_set, actual_variable_set)
 
   def test_use_depthwise_convolution(self):
-    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
+    image_features = tf.compat.v1.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
     conv_box_predictor = (
         box_predictor_builder.build_convolutional_box_predictor(
             is_training=False,
@@ -205,7 +205,7 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
     objectness_predictions = tf.concat(
         box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
         axis=1)
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
 
     resolution = 32
     expected_num_anchors = resolution*resolution*5
@@ -213,11 +213,11 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
       sess.run(init_op)
       (box_encodings_shape,
        objectness_predictions_shape) = sess.run(
-           [tf.shape(box_encodings), tf.shape(objectness_predictions)],
+           [tf.shape(input=box_encodings), tf.shape(input=objectness_predictions)],
            feed_dict={image_features:
                       np.random.rand(4, resolution, resolution, 64)})
       actual_variable_set = set(
-          [var.op.name for var in tf.trainable_variables()])
+          [var.op.name for var in tf.compat.v1.trainable_variables()])
     self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])
     self.assertAllEqual(objectness_predictions_shape,
                         [4, expected_num_anchors, 1])
@@ -442,10 +442,10 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       return (box_encodings, class_predictions_with_background)
 
     with self.test_session(graph=tf.Graph()):
-      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
-               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      graph_fn(tf.random.uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random.uniform([4, 16, 16, 3], dtype=tf.float32))
       actual_variable_set = set(
-          [var.op.name for var in tf.trainable_variables()])
+          [var.op.name for var in tf.compat.v1.trainable_variables()])
     expected_variable_set = set([
         # Box prediction tower
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
@@ -510,10 +510,10 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       return (box_encodings, class_predictions_with_background)
 
     with self.test_session(graph=tf.Graph()):
-      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
-               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      graph_fn(tf.random.uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random.uniform([4, 16, 16, 3], dtype=tf.float32))
       actual_variable_set = set(
-          [var.op.name for var in tf.trainable_variables()])
+          [var.op.name for var in tf.compat.v1.trainable_variables()])
     expected_variable_set = set([
         # Box prediction tower
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
@@ -569,10 +569,10 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       return (box_encodings, class_predictions_with_background)
 
     with self.test_session(graph=tf.Graph()):
-      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
-               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      graph_fn(tf.random.uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random.uniform([4, 16, 16, 3], dtype=tf.float32))
       actual_variable_set = set(
-          [var.op.name for var in tf.trainable_variables()])
+          [var.op.name for var in tf.compat.v1.trainable_variables()])
     expected_variable_set = set([
         # Box prediction tower
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
@@ -629,10 +629,10 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       return (box_encodings, class_predictions_with_background)
 
     with self.test_session(graph=tf.Graph()):
-      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
-               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      graph_fn(tf.random.uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random.uniform([4, 16, 16, 3], dtype=tf.float32))
       actual_variable_set = set(
-          [var.op.name for var in tf.trainable_variables()])
+          [var.op.name for var in tf.compat.v1.trainable_variables()])
     expected_variable_set = set([
         # Shared prediction tower
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
@@ -685,10 +685,10 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       return (box_encodings, class_predictions_with_background)
 
     with self.test_session(graph=tf.Graph()):
-      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
-               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      graph_fn(tf.random.uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random.uniform([4, 16, 16, 3], dtype=tf.float32))
       actual_variable_set = set(
-          [var.op.name for var in tf.trainable_variables()])
+          [var.op.name for var in tf.compat.v1.trainable_variables()])
     expected_variable_set = set([
         # Shared prediction tower
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
@@ -714,7 +714,7 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
 
   def test_get_predictions_with_feature_maps_of_dynamic_shape(
       self):
-    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
+    image_features = tf.compat.v1.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
     conv_box_predictor = (
         box_predictor_builder.build_weight_shared_convolutional_box_predictor(
             is_training=False,
@@ -730,7 +730,7 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
                               axis=1)
     objectness_predictions = tf.concat(box_predictions[
         box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
 
     resolution = 32
     expected_num_anchors = resolution*resolution*5
@@ -738,7 +738,7 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       sess.run(init_op)
       (box_encodings_shape,
        objectness_predictions_shape) = sess.run(
-           [tf.shape(box_encodings), tf.shape(objectness_predictions)],
+           [tf.shape(input=box_encodings), tf.shape(input=objectness_predictions)],
            feed_dict={image_features:
                       np.random.rand(4, resolution, resolution, 64)})
       self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 4])
diff --git a/research/object_detection/predictors/heads/box_head.py b/research/object_detection/predictors/heads/box_head.py
index 9d55190f..9f73e5bf 100644
--- a/research/object_detection/predictors/heads/box_head.py
+++ b/research/object_detection/predictors/heads/box_head.py
@@ -89,7 +89,7 @@ class MaskRCNNBoxHead(head.Head):
     if num_predictions_per_location != 1:
       raise ValueError('Only num_predictions_per_location=1 is supported')
     spatial_averaged_roi_pooled_features = tf.reduce_mean(
-        features, [1, 2], keep_dims=True, name='AvgPool')
+        input_tensor=features, axis=[1, 2], keepdims=True, name='AvgPool')
     flattened_roi_pooled_features = slim.flatten(
         spatial_averaged_roi_pooled_features)
     if self._use_dropout:
@@ -178,7 +178,7 @@ class ConvolutionalBoxHead(head.Head):
           scope='BoxEncodingPredictor')
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
-      batch_size = tf.shape(features)[0]
+      batch_size = tf.shape(input=features)[0]
     box_encodings = tf.reshape(box_encodings,
                                [batch_size, -1, 1, self._box_code_size])
     return box_encodings
@@ -233,7 +233,7 @@ class WeightSharedConvolutionalBoxHead(head.Head):
         scope='BoxPredictor')
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
-      batch_size = tf.shape(features)[0]
+      batch_size = tf.shape(input=features)[0]
     box_encodings = tf.reshape(box_encodings,
                                [batch_size, -1, self._box_code_size])
     return box_encodings
diff --git a/research/object_detection/predictors/heads/box_head_test.py b/research/object_detection/predictors/heads/box_head_test.py
index 34df8f43..aa8d64c4 100644
--- a/research/object_detection/predictors/heads/box_head_test.py
+++ b/research/object_detection/predictors/heads/box_head_test.py
@@ -52,7 +52,7 @@ class MaskRCNNBoxHeadTest(test_case.TestCase):
         dropout_keep_prob=0.5,
         box_code_size=4,
         share_box_across_classes=False)
-    roi_pooled_features = tf.random_uniform(
+    roi_pooled_features = tf.random.uniform(
         [64, 7, 7, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     prediction = box_prediction_head.predict(
         features=roi_pooled_features, num_predictions_per_location=1)
@@ -84,7 +84,7 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
         is_training=True,
         box_code_size=4,
         kernel_size=3)
-    image_feature = tf.random_uniform(
+    image_feature = tf.random.uniform(
         [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     box_encodings = box_prediction_head.predict(
         features=image_feature,
@@ -115,7 +115,7 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
   def test_prediction_size(self):
     box_prediction_head = box_head.WeightSharedConvolutionalBoxHead(
         box_code_size=4)
-    image_feature = tf.random_uniform(
+    image_feature = tf.random.uniform(
         [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     box_encodings = box_prediction_head.predict(
         features=image_feature,
diff --git a/research/object_detection/predictors/heads/class_head.py b/research/object_detection/predictors/heads/class_head.py
index 6bb057a0..85f491d5 100644
--- a/research/object_detection/predictors/heads/class_head.py
+++ b/research/object_detection/predictors/heads/class_head.py
@@ -78,7 +78,7 @@ class MaskRCNNClassHead(head.Head):
     if num_predictions_per_location != 1:
       raise ValueError('Only num_predictions_per_location=1 is supported')
     spatial_averaged_roi_pooled_features = tf.reduce_mean(
-        features, [1, 2], keep_dims=True, name='AvgPool')
+        input_tensor=features, axis=[1, 2], keepdims=True, name='AvgPool')
     flattened_roi_pooled_features = slim.flatten(
         spatial_averaged_roi_pooled_features)
     if self._use_dropout:
@@ -184,14 +184,14 @@ class ConvolutionalClassHead(head.Head):
           normalizer_fn=None,
           normalizer_params=None,
           scope='ClassPredictor',
-          biases_initializer=tf.constant_initializer(
+          biases_initializer=tf.compat.v1.constant_initializer(
               self._class_prediction_bias_init))
     if self._apply_sigmoid_to_scores:
       class_predictions_with_background = tf.sigmoid(
           class_predictions_with_background)
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
-      batch_size = tf.shape(features)[0]
+      batch_size = tf.shape(input=features)[0]
     class_predictions_with_background = tf.reshape(
         class_predictions_with_background, [batch_size, -1, num_class_slots])
     return class_predictions_with_background
@@ -258,12 +258,12 @@ class WeightSharedConvolutionalClassHead(head.Head):
         [self._kernel_size, self._kernel_size],
         activation_fn=None, stride=1, padding='SAME',
         normalizer_fn=None,
-        biases_initializer=tf.constant_initializer(
+        biases_initializer=tf.compat.v1.constant_initializer(
             self._class_prediction_bias_init),
         scope='ClassPredictor')
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
-      batch_size = tf.shape(features)[0]
+      batch_size = tf.shape(input=features)[0]
     class_predictions_with_background = tf.reshape(
         class_predictions_with_background, [batch_size, -1, num_class_slots])
     return class_predictions_with_background
diff --git a/research/object_detection/predictors/heads/class_head_test.py b/research/object_detection/predictors/heads/class_head_test.py
index 737e0984..595460fe 100644
--- a/research/object_detection/predictors/heads/class_head_test.py
+++ b/research/object_detection/predictors/heads/class_head_test.py
@@ -50,7 +50,7 @@ class MaskRCNNClassHeadTest(test_case.TestCase):
         fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
         use_dropout=True,
         dropout_keep_prob=0.5)
-    roi_pooled_features = tf.random_uniform(
+    roi_pooled_features = tf.random.uniform(
         [64, 7, 7, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     prediction = class_prediction_head.predict(
         features=roi_pooled_features, num_predictions_per_location=1)
@@ -84,7 +84,7 @@ class ConvolutionalClassPredictorTest(test_case.TestCase):
         use_dropout=True,
         dropout_keep_prob=0.5,
         kernel_size=3)
-    image_feature = tf.random_uniform(
+    image_feature = tf.random.uniform(
         [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     class_predictions = class_prediction_head.predict(
         features=image_feature,
@@ -116,7 +116,7 @@ class WeightSharedConvolutionalClassPredictorTest(test_case.TestCase):
   def test_prediction_size(self):
     class_prediction_head = (
         class_head.WeightSharedConvolutionalClassHead(num_classes=20))
-    image_feature = tf.random_uniform(
+    image_feature = tf.random.uniform(
         [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     class_predictions = class_prediction_head.predict(
         features=image_feature,
diff --git a/research/object_detection/predictors/heads/keypoint_head.py b/research/object_detection/predictors/heads/keypoint_head.py
index f02d92c7..376a4bde 100644
--- a/research/object_detection/predictors/heads/keypoint_head.py
+++ b/research/object_detection/predictors/heads/keypoint_head.py
@@ -99,11 +99,11 @@ class MaskRCNNKeypointHead(head.Head):
             scope='conv_%d' % (i + 1))
       net = slim.conv2d_transpose(
           net, self._num_keypoints, [2, 2], scope='deconv1')
-      heatmaps_mask = tf.image.resize_bilinear(
+      heatmaps_mask = tf.image.resize(
           net, [self._keypoint_heatmap_height, self._keypoint_heatmap_width],
-          align_corners=True,
+          method=tf.image.ResizeMethod.BILINEAR,
           name='upsample')
       return tf.expand_dims(
-          tf.transpose(heatmaps_mask, perm=[0, 3, 1, 2]),
+          tf.transpose(a=heatmaps_mask, perm=[0, 3, 1, 2]),
           axis=1,
           name='KeypointPredictor')
diff --git a/research/object_detection/predictors/heads/keypoint_head_test.py b/research/object_detection/predictors/heads/keypoint_head_test.py
index 626d59c4..71c99ed5 100644
--- a/research/object_detection/predictors/heads/keypoint_head_test.py
+++ b/research/object_detection/predictors/heads/keypoint_head_test.py
@@ -46,7 +46,7 @@ class MaskRCNNKeypointHeadTest(test_case.TestCase):
   def test_prediction_size(self):
     keypoint_prediction_head = keypoint_head.MaskRCNNKeypointHead(
         conv_hyperparams_fn=self._build_arg_scope_with_hyperparams())
-    roi_pooled_features = tf.random_uniform(
+    roi_pooled_features = tf.random.uniform(
         [64, 14, 14, 1024], minval=-2.0, maxval=2.0, dtype=tf.float32)
     prediction = keypoint_prediction_head.predict(
         features=roi_pooled_features, num_predictions_per_location=1)
diff --git a/research/object_detection/predictors/heads/mask_head.py b/research/object_detection/predictors/heads/mask_head.py
index c53ba63d..0740e001 100644
--- a/research/object_detection/predictors/heads/mask_head.py
+++ b/research/object_detection/predictors/heads/mask_head.py
@@ -135,9 +135,9 @@ class MaskRCNNMaskHead(head.Head):
       num_conv_channels = self._get_mask_predictor_conv_depth(
           num_feature_channels, self._num_classes)
     with slim.arg_scope(self._conv_hyperparams_fn()):
-      upsampled_features = tf.image.resize_bilinear(
+      upsampled_features = tf.image.resize(
           features, [self._mask_height, self._mask_width],
-          align_corners=True)
+          method=tf.image.ResizeMethod.BILINEAR)
       for _ in range(self._mask_prediction_num_conv_layers - 1):
         upsampled_features = slim.conv2d(
             upsampled_features,
@@ -150,7 +150,7 @@ class MaskRCNNMaskHead(head.Head):
           activation_fn=None,
           kernel_size=[3, 3])
       return tf.expand_dims(
-          tf.transpose(mask_predictions, perm=[0, 3, 1, 2]),
+          tf.transpose(a=mask_predictions, perm=[0, 3, 1, 2]),
           axis=1,
           name='MaskPredictor')
 
@@ -251,7 +251,7 @@ class ConvolutionalMaskHead(head.Head):
           scope='MaskPredictor')
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
-      batch_size = tf.shape(features)[0]
+      batch_size = tf.shape(input=features)[0]
     mask_predictions = tf.reshape(
         mask_predictions,
         [batch_size, -1, num_masks, self._mask_height, self._mask_width])
@@ -327,7 +327,7 @@ class WeightSharedConvolutionalMaskHead(head.Head):
         scope='MaskPredictor')
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
-      batch_size = tf.shape(features)[0]
+      batch_size = tf.shape(input=features)[0]
     mask_predictions = tf.reshape(
         mask_predictions,
         [batch_size, -1, num_masks, self._mask_height, self._mask_width])
diff --git a/research/object_detection/predictors/heads/mask_head_test.py b/research/object_detection/predictors/heads/mask_head_test.py
index c9e4c70e..17d51f36 100644
--- a/research/object_detection/predictors/heads/mask_head_test.py
+++ b/research/object_detection/predictors/heads/mask_head_test.py
@@ -52,7 +52,7 @@ class MaskRCNNMaskHeadTest(test_case.TestCase):
         mask_prediction_num_conv_layers=2,
         mask_prediction_conv_depth=256,
         masks_are_class_agnostic=False)
-    roi_pooled_features = tf.random_uniform(
+    roi_pooled_features = tf.random.uniform(
         [64, 7, 7, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     prediction = mask_prediction_head.predict(
         features=roi_pooled_features, num_predictions_per_location=1)
@@ -88,7 +88,7 @@ class ConvolutionalMaskPredictorTest(test_case.TestCase):
         kernel_size=3,
         mask_height=7,
         mask_width=7)
-    image_feature = tf.random_uniform(
+    image_feature = tf.random.uniform(
         [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     mask_predictions = mask_prediction_head.predict(
         features=image_feature,
@@ -106,7 +106,7 @@ class ConvolutionalMaskPredictorTest(test_case.TestCase):
         mask_height=7,
         mask_width=7,
         masks_are_class_agnostic=True)
-    image_feature = tf.random_uniform(
+    image_feature = tf.random.uniform(
         [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     mask_predictions = mask_prediction_head.predict(
         features=image_feature,
@@ -141,7 +141,7 @@ class WeightSharedConvolutionalMaskPredictorTest(test_case.TestCase):
             num_classes=20,
             mask_height=7,
             mask_width=7))
-    image_feature = tf.random_uniform(
+    image_feature = tf.random.uniform(
         [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     mask_predictions = mask_prediction_head.predict(
         features=image_feature,
@@ -156,7 +156,7 @@ class WeightSharedConvolutionalMaskPredictorTest(test_case.TestCase):
             mask_height=7,
             mask_width=7,
             masks_are_class_agnostic=True))
-    image_feature = tf.random_uniform(
+    image_feature = tf.random.uniform(
         [64, 17, 19, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
     mask_predictions = mask_prediction_head.predict(
         features=image_feature,
diff --git a/research/object_detection/predictors/mask_rcnn_box_predictor_test.py b/research/object_detection/predictors/mask_rcnn_box_predictor_test.py
index 77042b5b..e0676566 100644
--- a/research/object_detection/predictors/mask_rcnn_box_predictor_test.py
+++ b/research/object_detection/predictors/mask_rcnn_box_predictor_test.py
@@ -128,7 +128,7 @@ class MaskRCNNBoxPredictorTest(test_case.TestCase):
     self.assertAllEqual(mask_predictions.shape, [2, 1, 5, 14, 14])
 
   def test_do_not_return_instance_masks_without_request(self):
-    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)
+    image_features = tf.random.uniform([2, 7, 7, 3], dtype=tf.float32)
     mask_box_predictor = box_predictor_builder.build_mask_rcnn_box_predictor(
         is_training=False,
         num_classes=5,
diff --git a/research/object_detection/predictors/rfcn_box_predictor.py b/research/object_detection/predictors/rfcn_box_predictor.py
index d16de044..ed4e9a62 100644
--- a/research/object_detection/predictors/rfcn_box_predictor.py
+++ b/research/object_detection/predictors/rfcn_box_predictor.py
@@ -109,8 +109,8 @@ class RfcnBoxPredictor(box_predictor.BoxPredictor):
                        format(len(image_features)))
     image_feature = image_features[0]
     num_predictions_per_location = num_predictions_per_location[0]
-    batch_size = tf.shape(proposal_boxes)[0]
-    num_boxes = tf.shape(proposal_boxes)[1]
+    batch_size = tf.shape(input=proposal_boxes)[0]
+    num_boxes = tf.shape(input=proposal_boxes)[1]
     net = image_feature
     with slim.arg_scope(self._conv_hyperparams_fn()):
       net = slim.conv2d(net, self._depth, [1, 1], scope='reduce_depth')
@@ -128,7 +128,7 @@ class RfcnBoxPredictor(box_predictor.BoxPredictor):
           crop_size=self._crop_size,
           num_spatial_bins=self._num_spatial_bins,
           global_pool=True)
-      box_encodings = tf.squeeze(box_encodings, squeeze_dims=[2, 3])
+      box_encodings = tf.squeeze(box_encodings, axis=[2, 3])
       box_encodings = tf.reshape(box_encodings,
                                  [batch_size * num_boxes, 1, self.num_classes,
                                   self._box_code_size])
@@ -149,7 +149,7 @@ class RfcnBoxPredictor(box_predictor.BoxPredictor):
               num_spatial_bins=self._num_spatial_bins,
               global_pool=True))
       class_predictions_with_background = tf.squeeze(
-          class_predictions_with_background, squeeze_dims=[2, 3])
+          class_predictions_with_background, axis=[2, 3])
       class_predictions_with_background = tf.reshape(
           class_predictions_with_background,
           [batch_size * num_boxes, 1, total_classes])
diff --git a/research/object_detection/utils/category_util.py b/research/object_detection/utils/category_util.py
index fdd9c1c1..e5f292ce 100644
--- a/research/object_detection/utils/category_util.py
+++ b/research/object_detection/utils/category_util.py
@@ -41,7 +41,7 @@ def load_categories_from_csv_file(csv_path):
   """
   categories = []
 
-  with tf.gfile.Open(csv_path, 'r') as csvfile:
+  with tf.io.gfile.GFile(csv_path, 'r') as csvfile:
     reader = csv.reader(csvfile, delimiter=',', quotechar='"')
     for row in reader:
       if not row:
@@ -66,7 +66,7 @@ def save_categories_to_csv_file(categories, csv_path):
     csv_path: Path to the csv file to be parsed into categories.
   """
   categories.sort(key=lambda x: x['id'])
-  with tf.gfile.Open(csv_path, 'w') as csvfile:
+  with tf.io.gfile.GFile(csv_path, 'w') as csvfile:
     writer = csv.writer(csvfile, delimiter=',', quotechar='"')
     for category in categories:
       writer.writerow([category['id'], category['name']])
diff --git a/research/object_detection/utils/category_util_test.py b/research/object_detection/utils/category_util_test.py
index 9c99079e..f95cb49c 100644
--- a/research/object_detection/utils/category_util_test.py
+++ b/research/object_detection/utils/category_util_test.py
@@ -30,7 +30,7 @@ class EvalUtilTest(tf.test.TestCase):
         2,"bird"
     """.strip(' ')
     csv_path = os.path.join(self.get_temp_dir(), 'test.csv')
-    with tf.gfile.Open(csv_path, 'wb') as f:
+    with tf.io.gfile.GFile(csv_path, 'wb') as f:
       f.write(csv_data)
 
     categories = category_util.load_categories_from_csv_file(csv_path)
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index e7835223..4e8125ab 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -89,7 +89,7 @@ def get_configs_from_pipeline_file(pipeline_config_path):
       corresponding config objects.
   """
   pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
-  with tf.gfile.GFile(pipeline_config_path, "r") as f:
+  with tf.io.gfile.GFile(pipeline_config_path, "r") as f:
     proto_str = f.read()
     text_format.Merge(proto_str, pipeline_config)
   return create_configs_from_pipeline_proto(pipeline_config)
@@ -128,7 +128,7 @@ def get_graph_rewriter_config_from_file(graph_rewriter_config_file):
     graph_rewriter_pb2.GraphRewriter proto
   """
   graph_rewriter_config = graph_rewriter_pb2.GraphRewriter()
-  with tf.gfile.GFile(graph_rewriter_config_file, "r") as f:
+  with tf.io.gfile.GFile(graph_rewriter_config_file, "r") as f:
     text_format.Merge(f.read(), graph_rewriter_config)
   return graph_rewriter_config
 
@@ -168,8 +168,8 @@ def save_pipeline_config(pipeline_config, directory):
     file_io.recursive_create_dir(directory)
   pipeline_config_path = os.path.join(directory, "pipeline.config")
   config_text = text_format.MessageToString(pipeline_config)
-  with tf.gfile.Open(pipeline_config_path, "wb") as f:
-    tf.logging.info("Writing pipeline config file to %s",
+  with tf.io.gfile.GFile(pipeline_config_path, "wb") as f:
+    tf.compat.v1.logging.info("Writing pipeline config file to %s",
                     pipeline_config_path)
     f.write(config_text)
 
@@ -198,31 +198,31 @@ def get_configs_from_multiple_files(model_config_path="",
   configs = {}
   if model_config_path:
     model_config = model_pb2.DetectionModel()
-    with tf.gfile.GFile(model_config_path, "r") as f:
+    with tf.io.gfile.GFile(model_config_path, "r") as f:
       text_format.Merge(f.read(), model_config)
       configs["model"] = model_config
 
   if train_config_path:
     train_config = train_pb2.TrainConfig()
-    with tf.gfile.GFile(train_config_path, "r") as f:
+    with tf.io.gfile.GFile(train_config_path, "r") as f:
       text_format.Merge(f.read(), train_config)
       configs["train_config"] = train_config
 
   if train_input_config_path:
     train_input_config = input_reader_pb2.InputReader()
-    with tf.gfile.GFile(train_input_config_path, "r") as f:
+    with tf.io.gfile.GFile(train_input_config_path, "r") as f:
       text_format.Merge(f.read(), train_input_config)
       configs["train_input_config"] = train_input_config
 
   if eval_config_path:
     eval_config = eval_pb2.EvalConfig()
-    with tf.gfile.GFile(eval_config_path, "r") as f:
+    with tf.io.gfile.GFile(eval_config_path, "r") as f:
       text_format.Merge(f.read(), eval_config)
       configs["eval_config"] = eval_config
 
   if eval_input_config_path:
     eval_input_config = input_reader_pb2.InputReader()
-    with tf.gfile.GFile(eval_input_config_path, "r") as f:
+    with tf.io.gfile.GFile(eval_input_config_path, "r") as f:
       text_format.Merge(f.read(), eval_input_config)
       configs["eval_input_config"] = eval_input_config
 
@@ -325,7 +325,7 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
   if hparams:
     kwargs.update(hparams.values())
   for key, value in kwargs.items():
-    tf.logging.info("Maybe overwriting %s: %s", key, value)
+    tf.compat.v1.logging.info("Maybe overwriting %s: %s", key, value)
     # pylint: disable=g-explicit-bool-comparison
     if value == "" or value is None:
       continue
@@ -366,7 +366,7 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
     elif _is_generic_key(key):
       _update_generic(configs, key, value)
     else:
-      tf.logging.info("Ignoring config override key: %s", key)
+      tf.compat.v1.logging.info("Ignoring config override key: %s", key)
   return configs
 
 
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index cbb42157..59973387 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -32,7 +32,7 @@ from object_detection.utils import config_util
 def _write_config(config, config_path):
   """Writes a config object to disk."""
   config_text = text_format.MessageToString(config)
-  with tf.gfile.Open(config_path, "wb") as f:
+  with tf.io.gfile.GFile(config_path, "wb") as f:
     f.write(config_text)
 
 
diff --git a/research/object_detection/utils/dataset_util.py b/research/object_detection/utils/dataset_util.py
index 77b46a2d..4de4fb85 100644
--- a/research/object_detection/utils/dataset_util.py
+++ b/research/object_detection/utils/dataset_util.py
@@ -55,7 +55,7 @@ def read_examples_list(path):
   Returns:
     list of example identifiers (strings).
   """
-  with tf.gfile.GFile(path) as fid:
+  with tf.io.gfile.GFile(path) as fid:
     lines = fid.readlines()
   return [line.strip().split(' ')[0] for line in lines]
 
diff --git a/research/object_detection/utils/dataset_util_test.py b/research/object_detection/utils/dataset_util_test.py
index 99cfb2cd..ea9b93c8 100644
--- a/research/object_detection/utils/dataset_util_test.py
+++ b/research/object_detection/utils/dataset_util_test.py
@@ -26,7 +26,7 @@ class DatasetUtilTest(tf.test.TestCase):
   def test_read_examples_list(self):
     example_list_data = """example1 1\nexample2 2"""
     example_list_path = os.path.join(self.get_temp_dir(), 'examples.txt')
-    with tf.gfile.Open(example_list_path, 'wb') as f:
+    with tf.io.gfile.GFile(example_list_path, 'wb') as f:
       f.write(example_list_data)
 
     examples = dataset_util.read_examples_list(example_list_path)
diff --git a/research/object_detection/utils/json_utils_test.py b/research/object_detection/utils/json_utils_test.py
index 5e379043..4a238a0f 100644
--- a/research/object_detection/utils/json_utils_test.py
+++ b/research/object_detection/utils/json_utils_test.py
@@ -23,31 +23,31 @@ from object_detection.utils import json_utils
 class JsonUtilsTest(tf.test.TestCase):
 
   def testDumpReasonablePrecision(self):
-    output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
-    with tf.gfile.GFile(output_path, 'w') as f:
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'test.json')
+    with tf.io.gfile.GFile(output_path, 'w') as f:
       json_utils.Dump(1.0, f, float_digits=2)
-    with tf.gfile.GFile(output_path, 'r') as f:
+    with tf.io.gfile.GFile(output_path, 'r') as f:
       self.assertEqual(f.read(), '1.00')
 
   def testDumpPassExtraParams(self):
-    output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
-    with tf.gfile.GFile(output_path, 'w') as f:
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'test.json')
+    with tf.io.gfile.GFile(output_path, 'w') as f:
       json_utils.Dump([1.0], f, float_digits=2, indent=3)
-    with tf.gfile.GFile(output_path, 'r') as f:
+    with tf.io.gfile.GFile(output_path, 'r') as f:
       self.assertEqual(f.read(), '[\n   1.00\n]')
 
   def testDumpZeroPrecision(self):
-    output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
-    with tf.gfile.GFile(output_path, 'w') as f:
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'test.json')
+    with tf.io.gfile.GFile(output_path, 'w') as f:
       json_utils.Dump(1.0, f, float_digits=0, indent=3)
-    with tf.gfile.GFile(output_path, 'r') as f:
+    with tf.io.gfile.GFile(output_path, 'r') as f:
       self.assertEqual(f.read(), '1')
 
   def testDumpUnspecifiedPrecision(self):
-    output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
-    with tf.gfile.GFile(output_path, 'w') as f:
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'test.json')
+    with tf.io.gfile.GFile(output_path, 'w') as f:
       json_utils.Dump(1.012345, f)
-    with tf.gfile.GFile(output_path, 'r') as f:
+    with tf.io.gfile.GFile(output_path, 'r') as f:
       self.assertEqual(f.read(), '1.012345')
 
   def testDumpsReasonablePrecision(self):
diff --git a/research/object_detection/utils/label_map_util.py b/research/object_detection/utils/label_map_util.py
index 06a1b345..8ab7dcd1 100644
--- a/research/object_detection/utils/label_map_util.py
+++ b/research/object_detection/utils/label_map_util.py
@@ -128,7 +128,7 @@ def load_labelmap(path):
   Returns:
     a StringIntLabelMapProto
   """
-  with tf.gfile.GFile(path, 'r') as fid:
+  with tf.io.gfile.GFile(path, 'r') as fid:
     label_map_string = fid.read()
     label_map = string_int_label_map_pb2.StringIntLabelMap()
     try:
diff --git a/research/object_detection/utils/label_map_util_test.py b/research/object_detection/utils/label_map_util_test.py
index d528b804..908b7dbc 100644
--- a/research/object_detection/utils/label_map_util_test.py
+++ b/research/object_detection/utils/label_map_util_test.py
@@ -46,7 +46,7 @@ class LabelMapUtilTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
 
     label_map_dict = label_map_util.get_label_map_dict(label_map_path)
@@ -65,7 +65,7 @@ class LabelMapUtilTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
 
     label_map_dict = label_map_util.get_label_map_dict(
@@ -89,7 +89,7 @@ class LabelMapUtilTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
 
     with self.assertRaises(ValueError):
@@ -111,7 +111,7 @@ class LabelMapUtilTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
 
     label_map_dict = label_map_util.get_label_map_dict(label_map_path)
@@ -131,7 +131,7 @@ class LabelMapUtilTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
 
     label_map_dict = label_map_util.get_label_map_dict(
@@ -250,7 +250,7 @@ class LabelMapUtilTest(tf.test.TestCase):
       }
     """
     label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
+    with tf.io.gfile.GFile(label_map_path, 'wb') as f:
       f.write(label_map_string)
 
     category_index = label_map_util.create_category_index_from_labelmap(
diff --git a/research/object_detection/utils/learning_schedules.py b/research/object_detection/utils/learning_schedules.py
index 8b62f709..73d931ef 100644
--- a/research/object_detection/utils/learning_schedules.py
+++ b/research/object_detection/utils/learning_schedules.py
@@ -51,13 +51,13 @@ def exponential_decay_with_burnin(global_step,
   """
   if burnin_learning_rate == 0:
     burnin_learning_rate = learning_rate_base
-  post_burnin_learning_rate = tf.train.exponential_decay(
+  post_burnin_learning_rate = tf.compat.v1.train.exponential_decay(
       learning_rate_base,
       global_step - burnin_steps,
       learning_rate_decay_steps,
       learning_rate_decay_factor,
       staircase=staircase)
-  return tf.maximum(tf.where(
+  return tf.maximum(tf.compat.v1.where(
       tf.less(tf.cast(global_step, tf.int32), tf.constant(burnin_steps)),
       tf.constant(burnin_learning_rate),
       post_burnin_learning_rate), min_learning_rate, name='learning_rate')
@@ -102,7 +102,7 @@ def cosine_decay_with_warmup(global_step,
       (tf.cast(global_step, tf.float32) - warmup_steps - hold_base_rate_steps
       ) / float(total_steps - warmup_steps - hold_base_rate_steps)))
   if hold_base_rate_steps > 0:
-    learning_rate = tf.where(global_step > warmup_steps + hold_base_rate_steps,
+    learning_rate = tf.compat.v1.where(global_step > warmup_steps + hold_base_rate_steps,
                              learning_rate, learning_rate_base)
   if warmup_steps > 0:
     if learning_rate_base < warmup_learning_rate:
@@ -111,9 +111,9 @@ def cosine_decay_with_warmup(global_step,
     slope = (learning_rate_base - warmup_learning_rate) / warmup_steps
     warmup_rate = slope * tf.cast(global_step,
                                   tf.float32) + warmup_learning_rate
-    learning_rate = tf.where(global_step < warmup_steps, warmup_rate,
+    learning_rate = tf.compat.v1.where(global_step < warmup_steps, warmup_rate,
                              learning_rate)
-  return tf.where(global_step > total_steps, 0.0, learning_rate,
+  return tf.compat.v1.where(global_step > total_steps, 0.0, learning_rate,
                   name='learning_rate')
 
 
@@ -168,8 +168,8 @@ def manual_stepping(global_step, boundaries, rates, warmup=False):
   else:
     boundaries = [0] + boundaries
   num_boundaries = len(boundaries)
-  rate_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),
+  rate_index = tf.reduce_max(input_tensor=tf.compat.v1.where(tf.greater_equal(global_step, boundaries),
                                       list(range(num_boundaries)),
                                       [0] * num_boundaries))
-  return tf.reduce_sum(rates * tf.one_hot(rate_index, depth=num_boundaries),
+  return tf.reduce_sum(input_tensor=rates * tf.one_hot(rate_index, depth=num_boundaries),
                        name='learning_rate')
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index a6a44ffb..13064e55 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -38,7 +38,7 @@ def expanded_shape(orig_shape, start_dim, num_dims):
   Returns:
     An int32 vector of length tf.size(orig_shape) + num_dims.
   """
-  with tf.name_scope('ExpandedShape'):
+  with tf.compat.v1.name_scope('ExpandedShape'):
     start_dim = tf.expand_dims(start_dim, 0)  # scalar to rank-1
     before = tf.slice(orig_shape, [0], start_dim)
     add_shape = tf.ones(tf.reshape(num_dims, [1]), dtype=tf.int32)
@@ -106,11 +106,11 @@ def meshgrid(x, y):
   Returns:
     A tuple of tensors (xgrid, ygrid).
   """
-  with tf.name_scope('Meshgrid'):
-    x = tf.convert_to_tensor(x)
-    y = tf.convert_to_tensor(y)
-    x_exp_shape = expanded_shape(tf.shape(x), 0, tf.rank(y))
-    y_exp_shape = expanded_shape(tf.shape(y), tf.rank(y), tf.rank(x))
+  with tf.compat.v1.name_scope('Meshgrid'):
+    x = tf.convert_to_tensor(value=x)
+    y = tf.convert_to_tensor(value=y)
+    x_exp_shape = expanded_shape(tf.shape(input=x), 0, tf.rank(y))
+    y_exp_shape = expanded_shape(tf.shape(input=y), tf.rank(y), tf.rank(x))
 
     xgrid = tf.tile(tf.reshape(x, x_exp_shape), y_exp_shape)
     ygrid = tf.tile(tf.reshape(y, y_exp_shape), x_exp_shape)
@@ -138,7 +138,7 @@ def fixed_padding(inputs, kernel_size, rate=1):
   pad_total = kernel_size_effective - 1
   pad_beg = pad_total // 2
   pad_end = pad_total - pad_beg
-  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],
+  padded_inputs = tf.pad(tensor=inputs, paddings=[[0, 0], [pad_beg, pad_end],
                                   [pad_beg, pad_end], [0, 0]])
   return padded_inputs
 
@@ -167,26 +167,26 @@ def pad_to_multiple(tensor, multiple):
   tensor_depth = static_shape.get_depth(tensor_shape)
 
   if batch_size is None:
-    batch_size = tf.shape(tensor)[0]
+    batch_size = tf.shape(input=tensor)[0]
 
   if tensor_height is None:
-    tensor_height = tf.shape(tensor)[1]
-    padded_tensor_height = tf.to_int32(
-        tf.ceil(tf.to_float(tensor_height) / tf.to_float(multiple))) * multiple
+    tensor_height = tf.shape(input=tensor)[1]
+    padded_tensor_height = tf.cast(
+        tf.math.ceil(tf.cast(tensor_height, dtype=tf.float32) / tf.cast(multiple, dtype=tf.float32)), dtype=tf.int32) * multiple
   else:
     padded_tensor_height = int(
         math.ceil(float(tensor_height) / multiple) * multiple)
 
   if tensor_width is None:
-    tensor_width = tf.shape(tensor)[2]
-    padded_tensor_width = tf.to_int32(
-        tf.ceil(tf.to_float(tensor_width) / tf.to_float(multiple))) * multiple
+    tensor_width = tf.shape(input=tensor)[2]
+    padded_tensor_width = tf.cast(
+        tf.math.ceil(tf.cast(tensor_width, dtype=tf.float32) / tf.cast(multiple, dtype=tf.float32)), dtype=tf.int32) * multiple
   else:
     padded_tensor_width = int(
         math.ceil(float(tensor_width) / multiple) * multiple)
 
   if tensor_depth is None:
-    tensor_depth = tf.shape(tensor)[3]
+    tensor_depth = tf.shape(input=tensor)[3]
 
   # Use tf.concat instead of tf.pad to preserve static shape
   if padded_tensor_height != tensor_height:
@@ -243,9 +243,9 @@ def padded_one_hot_encoding(indices, depth, left_pad):
   def one_hot_and_pad():
     one_hot = tf.cast(tf.one_hot(tf.cast(indices, tf.int64), depth,
                                  on_value=1, off_value=0), tf.float32)
-    return tf.pad(one_hot, [[0, 0], [left_pad, 0]], mode='CONSTANT')
-  result = tf.cond(tf.greater(tf.size(indices), 0), one_hot_and_pad,
-                   lambda: tf.zeros((depth + left_pad, 0)))
+    return tf.pad(tensor=one_hot, paddings=[[0, 0], [left_pad, 0]], mode='CONSTANT')
+  result = tf.cond(pred=tf.greater(tf.size(input=indices), 0), true_fn=one_hot_and_pad,
+                   false_fn=lambda: tf.zeros((depth + left_pad, 0)))
   return tf.reshape(result, [-1, depth + left_pad])
 
 
@@ -270,7 +270,7 @@ def dense_to_sparse_boxes(dense_locations, dense_num_boxes, num_classes):
        box (e.g. dense_num_boxes = [1, 0, 0, 2] => box_classes = [0, 3, 3]
   """
 
-  num_valid_boxes = tf.reduce_sum(dense_num_boxes)
+  num_valid_boxes = tf.reduce_sum(input_tensor=dense_num_boxes)
   box_locations = tf.slice(dense_locations,
                            tf.constant([0, 0]), tf.stack([num_valid_boxes, 4]))
   tiled_classes = [tf.tile([i], tf.expand_dims(dense_num_boxes[i], 0))
@@ -304,17 +304,17 @@ def indices_to_dense_vector(indices,
     dense 1D Tensor of shape [size] with indices set to indices_values and the
         rest set to default_value.
   """
-  size = tf.to_int32(size)
+  size = tf.cast(size, dtype=tf.int32)
   zeros = tf.ones([size], dtype=dtype) * default_value
   values = tf.ones_like(indices, dtype=dtype) * indices_value
 
-  return tf.dynamic_stitch([tf.range(size), tf.to_int32(indices)],
+  return tf.dynamic_stitch([tf.range(size), tf.cast(indices, dtype=tf.int32)],
                            [zeros, values])
 
 
 def reduce_sum_trailing_dimensions(tensor, ndims):
   """Computes sum across all dimensions following first `ndims` dimensions."""
-  return tf.reduce_sum(tensor, axis=tuple(range(ndims, tensor.shape.ndims)))
+  return tf.reduce_sum(input_tensor=tensor, axis=tuple(range(ndims, tensor.shape.ndims)))
 
 
 def retain_groundtruth(tensor_dict, valid_indices):
@@ -349,7 +349,7 @@ def retain_groundtruth(tensor_dict, valid_indices):
   if fields.InputDataFields.groundtruth_boxes in tensor_dict:
     # Prevents reshape failure when num_boxes is 0.
     num_boxes = tf.maximum(tf.shape(
-        tensor_dict[fields.InputDataFields.groundtruth_boxes])[0], 1)
+        input=tensor_dict[fields.InputDataFields.groundtruth_boxes])[0], 1)
     for key in tensor_dict:
       if key in [fields.InputDataFields.groundtruth_boxes,
                  fields.InputDataFields.groundtruth_classes,
@@ -397,7 +397,7 @@ def retain_groundtruth_with_positive_classes(tensor_dict):
   """
   if fields.InputDataFields.groundtruth_classes not in tensor_dict:
     raise ValueError('`groundtruth classes` not in tensor_dict.')
-  keep_indices = tf.where(tf.greater(
+  keep_indices = tf.compat.v1.where(tf.greater(
       tensor_dict[fields.InputDataFields.groundtruth_classes], 0))
   return retain_groundtruth(tensor_dict, keep_indices)
 
@@ -411,8 +411,8 @@ def replace_nan_groundtruth_label_scores_with_ones(label_scores):
   Returns:
     a tensor where NaN label scores have been replaced by ones.
   """
-  return tf.where(
-      tf.is_nan(label_scores), tf.ones(tf.shape(label_scores)), label_scores)
+  return tf.compat.v1.where(
+      tf.math.is_nan(label_scores), tf.ones(tf.shape(input=label_scores)), label_scores)
 
 
 def filter_groundtruth_with_crowd_boxes(tensor_dict):
@@ -435,7 +435,7 @@ def filter_groundtruth_with_crowd_boxes(tensor_dict):
   if fields.InputDataFields.groundtruth_is_crowd in tensor_dict:
     is_crowd = tensor_dict[fields.InputDataFields.groundtruth_is_crowd]
     is_not_crowd = tf.logical_not(is_crowd)
-    is_not_crowd_indices = tf.where(is_not_crowd)
+    is_not_crowd_indices = tf.compat.v1.where(is_not_crowd)
     tensor_dict = retain_groundtruth(tensor_dict, is_not_crowd_indices)
   return tensor_dict
 
@@ -458,10 +458,10 @@ def filter_groundtruth_with_nan_box_coordinates(tensor_dict):
     boxes.
   """
   groundtruth_boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
-  nan_indicator_vector = tf.greater(tf.reduce_sum(tf.to_int32(
-      tf.is_nan(groundtruth_boxes)), reduction_indices=[1]), 0)
+  nan_indicator_vector = tf.greater(tf.reduce_sum(input_tensor=tf.cast(
+      tf.math.is_nan(groundtruth_boxes), dtype=tf.int32), axis=[1]), 0)
   valid_indicator_vector = tf.logical_not(nan_indicator_vector)
-  valid_indices = tf.where(valid_indicator_vector)
+  valid_indices = tf.compat.v1.where(valid_indicator_vector)
 
   return retain_groundtruth(tensor_dict, valid_indices)
 
@@ -506,7 +506,7 @@ def normalize_to_target(inputs,
     ValueError: If target_norm_value is not a float or a list of floats with
       length equal to the depth along the dimension to be normalized.
   """
-  with tf.variable_scope(scope, 'NormalizeToTarget', [inputs]):
+  with tf.compat.v1.variable_scope(scope, 'NormalizeToTarget', [inputs]):
     if not inputs.get_shape():
       raise ValueError('The input rank must be known.')
     input_shape = inputs.get_shape().as_list()
@@ -534,10 +534,10 @@ def normalize_to_target(inputs,
         initializer=tf.constant(initial_norm, dtype=tf.float32),
         trainable=trainable)
     if summarize:
-      mean = tf.reduce_mean(target_norm)
-      mean = tf.Print(mean, ['NormalizeToTarget:', mean])
-      tf.summary.scalar(tf.get_variable_scope().name, mean)
-    lengths = epsilon + tf.sqrt(tf.reduce_sum(tf.square(inputs), dim, True))
+      mean = tf.reduce_mean(input_tensor=target_norm)
+      mean = tf.compat.v1.Print(mean, ['NormalizeToTarget:', mean])
+      tf.compat.v1.summary.scalar(tf.compat.v1.get_variable_scope().name, mean)
+    lengths = epsilon + tf.sqrt(tf.reduce_sum(input_tensor=tf.square(inputs), axis=dim, keepdims=True))
     mult_shape = input_rank*[1]
     mult_shape[dim] = depth
     return tf.reshape(target_norm, mult_shape) * tf.truediv(inputs, lengths)
@@ -702,7 +702,7 @@ def position_sensitive_crop_regions(image,
     else:
       crop = tf.image.crop_and_resize(
           tf.expand_dims(split, 0), box,
-          tf.zeros(tf.shape(boxes)[0], dtype=tf.int32), bin_crop_size)
+          tf.zeros(tf.shape(input=boxes)[0], dtype=tf.int32), bin_crop_size)
     image_crops.append(crop)
 
   if global_pool:
@@ -710,28 +710,28 @@ def position_sensitive_crop_regions(image,
     position_sensitive_features = tf.add_n(image_crops) / len(image_crops)
     # Then average over spatial positions within the bins.
     position_sensitive_features = tf.reduce_mean(
-        position_sensitive_features, [1, 2], keep_dims=True)
+        input_tensor=position_sensitive_features, axis=[1, 2], keepdims=True)
   else:
     # Reorder height/width to depth channel.
     block_size = bin_crop_size[0]
     if block_size >= 2:
-      image_crops = [tf.space_to_depth(
-          crop, block_size=block_size) for crop in image_crops]
+      image_crops = [tf.compat.v1.space_to_depth(
+          input=crop, block_size=block_size) for crop in image_crops]
 
     # Pack image_crops so that first dimension is for position-senstive boxes.
     position_sensitive_features = tf.stack(image_crops, axis=0)
 
     # Unroll the position-sensitive boxes to spatial positions.
     position_sensitive_features = tf.squeeze(
-        tf.batch_to_space_nd(position_sensitive_features,
+        tf.batch_to_space(position_sensitive_features,
                              block_shape=[1] + num_spatial_bins,
                              crops=tf.zeros((3, 2), dtype=tf.int32)),
-        squeeze_dims=[0])
+        axis=[0])
 
     # Reorder back the depth channel.
     if block_size >= 2:
-      position_sensitive_features = tf.depth_to_space(
-          position_sensitive_features, block_size=block_size)
+      position_sensitive_features = tf.compat.v1.depth_to_space(
+          input=position_sensitive_features, block_size=block_size)
 
   return position_sensitive_features
 
@@ -768,20 +768,20 @@ def reframe_box_masks_to_image_masks(box_masks, boxes, image_height,
       return tf.reshape(transformed_boxes, [-1, 4])
 
     box_masks_expanded = tf.expand_dims(box_masks, axis=3)
-    num_boxes = tf.shape(box_masks_expanded)[0]
+    num_boxes = tf.shape(input=box_masks_expanded)[0]
     unit_boxes = tf.concat(
         [tf.zeros([num_boxes, 2]), tf.ones([num_boxes, 2])], axis=1)
     reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes, boxes)
     return tf.image.crop_and_resize(
         image=box_masks_expanded,
         boxes=reverse_boxes,
-        box_ind=tf.range(num_boxes),
+        box_indices=tf.range(num_boxes),
         crop_size=[image_height, image_width],
         extrapolation_value=0.0)
   image_masks = tf.cond(
-      tf.shape(box_masks)[0] > 0,
-      reframe_box_masks_to_image_masks_default,
-      lambda: tf.zeros([0, image_height, image_width, 1], dtype=tf.float32))
+      pred=tf.shape(input=box_masks)[0] > 0,
+      true_fn=reframe_box_masks_to_image_masks_default,
+      false_fn=lambda: tf.zeros([0, image_height, image_width, 1], dtype=tf.float32))
   return tf.squeeze(image_masks, axis=3)
 
 
@@ -822,7 +822,7 @@ def merge_boxes_with_multiple_labels(boxes, classes, num_classes):
     merged_box_indices = np.array(merged_box_indices).astype(np.int32)
     return merged_boxes, class_encodings, merged_box_indices
 
-  merged_boxes, class_encodings, merged_box_indices = tf.py_func(
+  merged_boxes, class_encodings, merged_box_indices = tf.compat.v1.py_func(
       merge_numpy_boxes, [boxes, classes, num_classes],
       [tf.float32, tf.int32, tf.int32])
   merged_boxes = tf.reshape(merged_boxes, [-1, 4])
@@ -847,7 +847,7 @@ def nearest_neighbor_upsampling(input_tensor, scale):
     data_up: A float32 tensor of size
       [batch, height_in*scale, width_in*scale, channels].
   """
-  with tf.name_scope('nearest_neighbor_upsampling'):
+  with tf.compat.v1.name_scope('nearest_neighbor_upsampling'):
     (batch_size, height, width,
      channels) = shape_utils.combined_static_and_dynamic_shape(input_tensor)
     output_tensor = tf.reshape(
@@ -873,7 +873,7 @@ def matmul_gather_on_zeroth_axis(params, indices, scope=None):
     A Tensor. Has the same type as params. Values from params gathered
     from indices given by indices, with shape indices.shape + params.shape[1:].
   """
-  with tf.name_scope(scope, 'MatMulGather'):
+  with tf.compat.v1.name_scope(scope, 'MatMulGather'):
     params_shape = shape_utils.combined_static_and_dynamic_shape(params)
     indices_shape = shape_utils.combined_static_and_dynamic_shape(indices)
     params2d = tf.reshape(params, [params_shape[0], -1])
@@ -978,7 +978,7 @@ def matmul_crop_and_resize(image, boxes, crop_size, scope=None):
     return (tf.constant(start_weights, dtype=tf.float32),
             tf.constant(stop_weights, dtype=tf.float32))
 
-  with tf.name_scope(scope, 'MatMulCropAndResize'):
+  with tf.compat.v1.name_scope(scope, 'MatMulCropAndResize'):
     y1_weights, y2_weights = _lin_space_weights(crop_size[0], img_height)
     x1_weights, x2_weights = _lin_space_weights(crop_size[1], img_width)
     [y1, x1, y2, x2] = tf.split(value=boxes, num_or_size_splits=4, axis=1)
@@ -1035,24 +1035,24 @@ def expected_classification_loss_under_sampling(batch_cls_targets, cls_losses,
   Returns:
     The classification loss.
   """
-  num_anchors = tf.cast(tf.shape(batch_cls_targets)[1], tf.float32)
+  num_anchors = tf.cast(tf.shape(input=batch_cls_targets)[1], tf.float32)
 
   # find the p_i
   foreground_probabilities = (
       foreground_probabilities_from_targets(batch_cls_targets))
-  foreground_sum = tf.reduce_sum(foreground_probabilities, axis=-1)
+  foreground_sum = tf.reduce_sum(input_tensor=foreground_probabilities, axis=-1)
 
   k = desired_negative_sampling_ratio
 
   # compute beta
   denominators = (num_anchors - foreground_sum)
-  beta = tf.where(
+  beta = tf.compat.v1.where(
       tf.equal(denominators, 0), tf.zeros_like(foreground_sum),
       k * foreground_sum / denominators)
 
   # where the foreground sum is zero, use a minimum negative weight.
   min_negative_weight = 1.0 * minimum_negative_sampling / num_anchors
-  beta = tf.where(
+  beta = tf.compat.v1.where(
       tf.equal(foreground_sum, 0), min_negative_weight * tf.ones_like(beta),
       beta)
   beta = tf.reshape(beta, [-1, 1])
@@ -1062,7 +1062,7 @@ def expected_classification_loss_under_sampling(batch_cls_targets, cls_losses,
 
   weighted_losses = cls_loss_weights * cls_losses
 
-  cls_losses = tf.reduce_sum(weighted_losses, axis=-1)
+  cls_losses = tf.reduce_sum(input_tensor=weighted_losses, axis=-1)
 
   return cls_losses
 
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index 9fbdb944..07b11767 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -25,10 +25,10 @@ from object_detection.utils import test_case
 class NormalizedToImageCoordinatesTest(tf.test.TestCase):
 
   def test_normalized_to_image_coordinates(self):
-    normalized_boxes = tf.placeholder(tf.float32, shape=(None, 1, 4))
+    normalized_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 1, 4))
     normalized_boxes_np = np.array([[[0.0, 0.0, 1.0, 1.0]],
                                     [[0.5, 0.5, 1.0, 1.0]]])
-    image_shape = tf.convert_to_tensor([1, 4, 4, 3], dtype=tf.int32)
+    image_shape = tf.convert_to_tensor(value=[1, 4, 4, 3], dtype=tf.int32)
     absolute_boxes = ops.normalized_to_image_coordinates(normalized_boxes,
                                                          image_shape,
                                                          parallel_iterations=2)
@@ -46,7 +46,7 @@ class NormalizedToImageCoordinatesTest(tf.test.TestCase):
 class ReduceSumTrailingDimensions(tf.test.TestCase):
 
   def test_reduce_sum_trailing_dimensions(self):
-    input_tensor = tf.placeholder(tf.float32, shape=[None, None, None])
+    input_tensor = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None])
     reduced_tensor = ops.reduce_sum_trailing_dimensions(input_tensor, ndims=2)
     with self.test_session() as sess:
       reduced_np = sess.run(reduced_tensor,
@@ -233,10 +233,10 @@ class OpsDenseToSparseBoxesTest(tf.test.TestCase):
     num_classes = 4
     num_valid_boxes = 3
     code_size = 4
-    dense_location_placeholder = tf.placeholder(tf.float32,
+    dense_location_placeholder = tf.compat.v1.placeholder(tf.float32,
                                                 shape=(num_valid_boxes,
                                                        code_size))
-    dense_num_boxes_placeholder = tf.placeholder(tf.int32, shape=(num_classes))
+    dense_num_boxes_placeholder = tf.compat.v1.placeholder(tf.int32, shape=(num_classes))
     box_locations, box_classes = ops.dense_to_sparse_boxes(
         dense_location_placeholder, dense_num_boxes_placeholder, num_classes)
     feed_dict = {dense_location_placeholder: np.random.uniform(
@@ -260,9 +260,9 @@ class OpsDenseToSparseBoxesTest(tf.test.TestCase):
     num_boxes = 10
     code_size = 4
 
-    dense_location_placeholder = tf.placeholder(tf.float32, shape=(num_boxes,
+    dense_location_placeholder = tf.compat.v1.placeholder(tf.float32, shape=(num_boxes,
                                                                    code_size))
-    dense_num_boxes_placeholder = tf.placeholder(tf.int32, shape=(num_classes))
+    dense_num_boxes_placeholder = tf.compat.v1.placeholder(tf.int32, shape=(num_classes))
     box_locations, box_classes = ops.dense_to_sparse_boxes(
         dense_location_placeholder, dense_num_boxes_placeholder, num_classes)
     feed_dict = {dense_location_placeholder: np.random.uniform(
@@ -309,10 +309,10 @@ class OpsTestIndicesToDenseVector(tf.test.TestCase):
     expected_output = np.zeros(size, dtype=np.float32)
     expected_output[rand_indices] = 1.
 
-    tf_all_indices = tf.placeholder(tf.int32)
+    tf_all_indices = tf.compat.v1.placeholder(tf.int32)
     tf_rand_indices = tf.constant(rand_indices)
     indicator = ops.indices_to_dense_vector(tf_rand_indices,
-                                            tf.shape(tf_all_indices)[0])
+                                            tf.shape(input=tf_all_indices)[0])
     feed_dict = {tf_all_indices: all_indices}
 
     with self.test_session() as sess:
@@ -392,14 +392,14 @@ class OpsTestIndicesToDenseVector(tf.test.TestCase):
 class GroundtruthFilterTest(tf.test.TestCase):
 
   def test_filter_groundtruth(self):
-    input_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
-    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
-    input_area = tf.placeholder(tf.float32, shape=(None,))
-    input_difficult = tf.placeholder(tf.float32, shape=(None,))
-    input_label_types = tf.placeholder(tf.string, shape=(None,))
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
+    input_image = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3))
+    input_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    input_classes = tf.compat.v1.placeholder(tf.int32, shape=(None,))
+    input_is_crowd = tf.compat.v1.placeholder(tf.bool, shape=(None,))
+    input_area = tf.compat.v1.placeholder(tf.float32, shape=(None,))
+    input_difficult = tf.compat.v1.placeholder(tf.float32, shape=(None,))
+    input_label_types = tf.compat.v1.placeholder(tf.string, shape=(None,))
+    valid_indices = tf.compat.v1.placeholder(tf.int32, shape=(None,))
     input_tensors = {
         fields.InputDataFields.image: input_image,
         fields.InputDataFields.groundtruth_boxes: input_boxes,
@@ -457,13 +457,13 @@ class GroundtruthFilterTest(tf.test.TestCase):
         self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
   def test_filter_with_missing_fields(self):
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
+    input_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    input_classes = tf.compat.v1.placeholder(tf.int32, shape=(None,))
     input_tensors = {
         fields.InputDataFields.groundtruth_boxes: input_boxes,
         fields.InputDataFields.groundtruth_classes: input_classes
     }
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
+    valid_indices = tf.compat.v1.placeholder(tf.int32, shape=(None,))
 
     feed_dict = {
         input_boxes:
@@ -489,12 +489,12 @@ class GroundtruthFilterTest(tf.test.TestCase):
         self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
   def test_filter_with_empty_fields(self):
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
-    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
-    input_area = tf.placeholder(tf.float32, shape=(None,))
-    input_difficult = tf.placeholder(tf.float32, shape=(None,))
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
+    input_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    input_classes = tf.compat.v1.placeholder(tf.int32, shape=(None,))
+    input_is_crowd = tf.compat.v1.placeholder(tf.bool, shape=(None,))
+    input_area = tf.compat.v1.placeholder(tf.float32, shape=(None,))
+    input_difficult = tf.compat.v1.placeholder(tf.float32, shape=(None,))
+    valid_indices = tf.compat.v1.placeholder(tf.int32, shape=(None,))
     input_tensors = {
         fields.InputDataFields.groundtruth_boxes: input_boxes,
         fields.InputDataFields.groundtruth_classes: input_classes,
@@ -540,12 +540,12 @@ class GroundtruthFilterTest(tf.test.TestCase):
         self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
   def test_filter_with_empty_groundtruth_boxes(self):
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
-    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
-    input_area = tf.placeholder(tf.float32, shape=(None,))
-    input_difficult = tf.placeholder(tf.float32, shape=(None,))
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
+    input_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    input_classes = tf.compat.v1.placeholder(tf.int32, shape=(None,))
+    input_is_crowd = tf.compat.v1.placeholder(tf.bool, shape=(None,))
+    input_area = tf.compat.v1.placeholder(tf.float32, shape=(None,))
+    input_difficult = tf.compat.v1.placeholder(tf.float32, shape=(None,))
+    valid_indices = tf.compat.v1.placeholder(tf.int32, shape=(None,))
     input_tensors = {
         fields.InputDataFields.groundtruth_boxes: input_boxes,
         fields.InputDataFields.groundtruth_classes: input_classes,
@@ -581,14 +581,14 @@ class GroundtruthFilterTest(tf.test.TestCase):
 class RetainGroundTruthWithPositiveClasses(tf.test.TestCase):
 
   def test_filter_groundtruth_with_positive_classes(self):
-    input_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
-    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
-    input_area = tf.placeholder(tf.float32, shape=(None,))
-    input_difficult = tf.placeholder(tf.float32, shape=(None,))
-    input_label_types = tf.placeholder(tf.string, shape=(None,))
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
+    input_image = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3))
+    input_boxes = tf.compat.v1.placeholder(tf.float32, shape=(None, 4))
+    input_classes = tf.compat.v1.placeholder(tf.int32, shape=(None,))
+    input_is_crowd = tf.compat.v1.placeholder(tf.bool, shape=(None,))
+    input_area = tf.compat.v1.placeholder(tf.float32, shape=(None,))
+    input_difficult = tf.compat.v1.placeholder(tf.float32, shape=(None,))
+    input_label_types = tf.compat.v1.placeholder(tf.string, shape=(None,))
+    valid_indices = tf.compat.v1.placeholder(tf.int32, shape=(None,))
     input_tensors = {
         fields.InputDataFields.image: input_image,
         fields.InputDataFields.groundtruth_boxes: input_boxes,
@@ -744,7 +744,7 @@ class GroundtruthFilterWithNanBoxTest(tf.test.TestCase):
 class OpsTestNormalizeToTarget(tf.test.TestCase):
 
   def test_create_normalize_to_target(self):
-    inputs = tf.random_uniform([5, 10, 12, 3])
+    inputs = tf.random.uniform([5, 10, 12, 3])
     target_norm_value = 4.0
     dim = 3
     with self.test_session():
@@ -754,7 +754,7 @@ class OpsTestNormalizeToTarget(tf.test.TestCase):
       self.assertEqual(var_name, 'NormalizeToTarget/weights:0')
 
   def test_invalid_dim(self):
-    inputs = tf.random_uniform([5, 10, 12, 3])
+    inputs = tf.random.uniform([5, 10, 12, 3])
     target_norm_value = 4.0
     dim = 10
     with self.assertRaisesRegexp(
@@ -763,7 +763,7 @@ class OpsTestNormalizeToTarget(tf.test.TestCase):
       ops.normalize_to_target(inputs, target_norm_value, dim)
 
   def test_invalid_target_norm_values(self):
-    inputs = tf.random_uniform([5, 10, 12, 3])
+    inputs = tf.random.uniform([5, 10, 12, 3])
     target_norm_value = [4.0, 4.0]
     dim = 3
     with self.assertRaisesRegexp(
@@ -771,7 +771,7 @@ class OpsTestNormalizeToTarget(tf.test.TestCase):
       ops.normalize_to_target(inputs, target_norm_value, dim)
 
   def test_correct_output_shape(self):
-    inputs = tf.random_uniform([5, 10, 12, 3])
+    inputs = tf.random.uniform([5, 10, 12, 3])
     target_norm_value = 4.0
     dim = 3
     with self.test_session():
@@ -789,7 +789,7 @@ class OpsTestNormalizeToTarget(tf.test.TestCase):
     with self.test_session() as sess:
       normalized_inputs = ops.normalize_to_target(inputs, target_norm_value,
                                                   dim)
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = normalized_inputs.eval()
       self.assertAllClose(output, expected_output)
 
@@ -803,7 +803,7 @@ class OpsTestNormalizeToTarget(tf.test.TestCase):
     with self.test_session() as sess:
       normalized_inputs = ops.normalize_to_target(inputs, target_norm_value,
                                                   dim)
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = normalized_inputs.eval()
       self.assertAllClose(output, expected_output)
 
@@ -817,7 +817,7 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     # First channel is 1's, second channel is 2's, etc.
     image = tf.constant(range(1, 3 * 2 + 1) * 6, dtype=tf.float32,
                         shape=image_shape)
-    boxes = tf.random_uniform((2, 4))
+    boxes = tf.random.uniform((2, 4))
 
     # The result for both boxes should be [[1, 2], [3, 4], [5, 6]]
     # before averaging.
@@ -840,14 +840,14 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     image = tf.constant(range(1, 3 * 3 + 1), dtype=tf.float32,
                         shape=[3, 3, 1])
     tiled_image = tf.tile(image, [1, 1, image_shape[2]])
-    boxes = tf.random_uniform((3, 4))
+    boxes = tf.random.uniform((3, 4))
     box_ind = tf.constant([0, 0, 0], dtype=tf.int32)
 
     # All channels are equal so position-sensitive crop and resize should
     # work as the usual crop and resize for just one channel.
     crop = tf.image.crop_and_resize(tf.expand_dims(image, axis=0), boxes,
                                     box_ind, crop_size)
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)
+    crop_and_pool = tf.reduce_mean(input_tensor=crop, axis=[1, 2], keepdims=True)
 
     ps_crop_and_pool = ops.position_sensitive_crop_regions(
         tiled_image,
@@ -906,7 +906,7 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     # First channel is 1's, second channel is 2's, etc.
     image = tf.constant(range(1, 3 * 2 + 1) * 6, dtype=tf.float32,
                         shape=image_shape)
-    boxes = tf.random_uniform((num_boxes, 4))
+    boxes = tf.random.uniform((num_boxes, 4))
 
     expected_output = []
 
@@ -944,7 +944,7 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     # First channel is 1's, second channel is 2's, etc.
     image = tf.constant(range(1, 3 * 2 + 1) * 6, dtype=tf.float32,
                         shape=image_shape)
-    boxes = tf.random_uniform((num_boxes, 4))
+    boxes = tf.random.uniform((num_boxes, 4))
 
     expected_output = []
 
@@ -977,7 +977,7 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
       ps_crop = ops.position_sensitive_crop_regions(
           image, boxes, crop_size, num_spatial_bins, global_pool=False)
       ps_crop_and_pool = tf.reduce_mean(
-          ps_crop, reduction_indices=(1, 2), keep_dims=True)
+          input_tensor=ps_crop, axis=(1, 2), keepdims=True)
 
       with self.test_session() as sess:
         output = sess.run(ps_crop_and_pool)
@@ -1005,15 +1005,15 @@ class OpsTestBatchPositionSensitiveCropRegions(tf.test.TestCase):
     image_shape = [2, 3, 3, 4]
     crop_size = [2, 2]
 
-    image = tf.random_uniform(image_shape)
-    boxes = tf.random_uniform((2, 3, 4))
+    image = tf.random.uniform(image_shape)
+    boxes = tf.random.uniform((2, 3, 4))
     box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
 
     # When a single bin is used, position-sensitive crop and pool should be
     # the same as non-position sensitive crop and pool.
     crop = tf.image.crop_and_resize(image, tf.reshape(boxes, [-1, 4]), box_ind,
                                     crop_size)
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
+    crop_and_pool = tf.reduce_mean(input_tensor=crop, axis=[1, 2], keepdims=True)
     crop_and_pool = tf.reshape(crop_and_pool, [2, 3, 1, 1, 4])
 
     ps_crop_and_pool = ops.batch_position_sensitive_crop_regions(
@@ -1065,8 +1065,8 @@ class OpsTestBatchPositionSensitiveCropRegions(tf.test.TestCase):
     image_shape = [2, 3, 3, 4]
     crop_size = [1, 1]
 
-    images = tf.random_uniform(image_shape)
-    boxes = tf.random_uniform((2, 3, 4))
+    images = tf.random.uniform(image_shape)
+    boxes = tf.random.uniform((2, 3, 4))
     # box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
 
     # Since single_bin is used and crop_size = [1, 1] (i.e., no crop resize),
@@ -1244,8 +1244,8 @@ class MatmulGatherOnZerothAxis(test_case.TestCase):
     self.assertAllClose(gather_output, expected_output)
 
   def test_gather_with_dynamic_shape_input(self):
-    params_placeholder = tf.placeholder(tf.float32, shape=[None, 4])
-    indices_placeholder = tf.placeholder(tf.int32, shape=[None])
+    params_placeholder = tf.compat.v1.placeholder(tf.float32, shape=[None, 4])
+    indices_placeholder = tf.compat.v1.placeholder(tf.int32, shape=[None])
     gather_result = ops.matmul_gather_on_zeroth_axis(
         params_placeholder, indices_placeholder)
     params = np.array([[1, 2, 3, 4],
diff --git a/research/object_detection/utils/per_image_evaluation_test.py b/research/object_detection/utils/per_image_evaluation_test.py
index a870843f..a1de1514 100644
--- a/research/object_detection/utils/per_image_evaluation_test.py
+++ b/research/object_detection/utils/per_image_evaluation_test.py
@@ -296,7 +296,7 @@ class SingleClassTpFpWithGroupOfBoxesTestWeighted(tf.test.TestCase):
         detected_masks=self.detected_masks,
         groundtruth_masks=self.groundtruth_masks)
 
-    tf.logging.info(
+    tf.compat.v1.logging.info(
         "test_mask_match_to_non_group_of_and_group_of_box {} {}".format(
             tp_fp_labels, expected_tp_fp_labels))
 
@@ -315,7 +315,7 @@ class SingleClassTpFpWithGroupOfBoxesTestWeighted(tf.test.TestCase):
         groundtruth_groundtruth_is_difficult_list,
         groundtruth_groundtruth_is_group_of_list)
 
-    tf.logging.info("test_match_two_to_group_of_box {} {}".format(
+    tf.compat.v1.logging.info("test_match_two_to_group_of_box {} {}".format(
         tp_fp_labels, expected_tp_fp_labels))
 
     self.assertTrue(np.allclose(expected_scores, scores))
@@ -338,7 +338,7 @@ class SingleClassTpFpWithGroupOfBoxesTestWeighted(tf.test.TestCase):
         detected_masks=self.detected_masks,
         groundtruth_masks=self.groundtruth_masks)
 
-    tf.logging.info("test_mask_match_two_to_group_of_box {} {}".format(
+    tf.compat.v1.logging.info("test_mask_match_two_to_group_of_box {} {}".format(
         tp_fp_labels, expected_tp_fp_labels))
 
     self.assertTrue(np.allclose(expected_scores, scores))
diff --git a/research/object_detection/utils/shape_utils.py b/research/object_detection/utils/shape_utils.py
index dfa96e79..fb0ca5c8 100644
--- a/research/object_detection/utils/shape_utils.py
+++ b/research/object_detection/utils/shape_utils.py
@@ -62,12 +62,12 @@ def pad_tensor(t, length):
       statically.
   """
   t_rank = tf.rank(t)
-  t_shape = tf.shape(t)
+  t_shape = tf.shape(input=t)
   t_d0 = t_shape[0]
   pad_d0 = tf.expand_dims(length - t_d0, 0)
   pad_shape = tf.cond(
-      tf.greater(t_rank, 1), lambda: tf.concat([pad_d0, t_shape[1:]], 0),
-      lambda: tf.expand_dims(length - t_d0, 0))
+      pred=tf.greater(t_rank, 1), true_fn=lambda: tf.concat([pad_d0, t_shape[1:]], 0),
+      false_fn=lambda: tf.expand_dims(length - t_d0, 0))
   padded_t = tf.concat([t, tf.zeros(pad_shape, dtype=t.dtype)], 0)
   if not _is_tensor(length):
     padded_t = _set_dim_0(padded_t, length)
@@ -120,9 +120,9 @@ def pad_or_clip_nd(tensor, output_shape):
   Returns:
     Input tensor padded and clipped to the output shape.
   """
-  tensor_shape = tf.shape(tensor)
+  tensor_shape = tf.shape(input=tensor)
   clip_size = [
-      tf.where(tensor_shape[i] - shape > 0, shape, -1)
+      tf.compat.v1.where(tensor_shape[i] - shape > 0, shape, -1)
       if shape is not None else -1 for i, shape in enumerate(output_shape)
   ]
   clipped_tensor = tf.slice(
@@ -132,7 +132,7 @@ def pad_or_clip_nd(tensor, output_shape):
 
   # Pad tensor if the shape of clipped tensor is smaller than the expected
   # shape.
-  clipped_tensor_shape = tf.shape(clipped_tensor)
+  clipped_tensor_shape = tf.shape(input=clipped_tensor)
   trailing_paddings = [
       shape - clipped_tensor_shape[i] if shape is not None else 0
       for i, shape in enumerate(output_shape)
@@ -143,7 +143,7 @@ def pad_or_clip_nd(tensor, output_shape):
           trailing_paddings
       ],
       axis=1)
-  padded_tensor = tf.pad(clipped_tensor, paddings=paddings)
+  padded_tensor = tf.pad(tensor=clipped_tensor, paddings=paddings)
   output_static_shape = [
       dim if not isinstance(dim, tf.Tensor) else None for dim in output_shape
   ]
@@ -164,7 +164,7 @@ def combined_static_and_dynamic_shape(tensor):
     A list of size tensor.shape.ndims containing integers or a scalar tensor.
   """
   static_tensor_shape = tensor.shape.as_list()
-  dynamic_tensor_shape = tf.shape(tensor)
+  dynamic_tensor_shape = tf.shape(input=tensor)
   combined_shape = []
   for index, dim in enumerate(static_tensor_shape):
     if dim is not None:
@@ -272,8 +272,8 @@ def check_min_image_dim(min_dim, image_tensor):
   image_width = static_shape.get_width(image_shape)
   if image_height is None or image_width is None:
     shape_assert = tf.Assert(
-        tf.logical_and(tf.greater_equal(tf.shape(image_tensor)[1], min_dim),
-                       tf.greater_equal(tf.shape(image_tensor)[2], min_dim)),
+        tf.logical_and(tf.greater_equal(tf.shape(input=image_tensor)[1], min_dim),
+                       tf.greater_equal(tf.shape(input=image_tensor)[2], min_dim)),
         ['image size must be >= {} in both height and width.'.format(min_dim)])
     with tf.control_dependencies([shape_assert]):
       return tf.identity(image_tensor)
@@ -312,7 +312,7 @@ def assert_shape_equal(shape_a, shape_b):
       raise ValueError('Unequal shapes {}, {}'.format(shape_a, shape_b))
     else: return tf.no_op()
   else:
-    return tf.assert_equal(shape_a, shape_b)
+    return tf.compat.v1.assert_equal(shape_a, shape_b)
 
 
 def assert_shape_equal_along_first_dimension(shape_a, shape_b):
@@ -341,4 +341,4 @@ def assert_shape_equal_along_first_dimension(shape_a, shape_b):
           shape_a[0], shape_b[0]))
     else: return tf.no_op()
   else:
-    return tf.assert_equal(shape_a[0], shape_b[0])
+    return tf.compat.v1.assert_equal(shape_a[0], shape_b[0])
diff --git a/research/object_detection/utils/shape_utils_test.py b/research/object_detection/utils/shape_utils_test.py
index b2b33456..3fb624b6 100644
--- a/research/object_detection/utils/shape_utils_test.py
+++ b/research/object_detection/utils/shape_utils_test.py
@@ -117,14 +117,14 @@ class UtilTest(tf.test.TestCase):
       self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], tt4_result)
 
   def test_combines_static_dynamic_shape(self):
-    tensor = tf.placeholder(tf.float32, shape=(None, 2, 3))
+    tensor = tf.compat.v1.placeholder(tf.float32, shape=(None, 2, 3))
     combined_shape = shape_utils.combined_static_and_dynamic_shape(
         tensor)
-    self.assertTrue(tf.contrib.framework.is_tensor(combined_shape[0]))
+    self.assertTrue(tf.is_tensor(combined_shape[0]))
     self.assertListEqual(combined_shape[1:], [2, 3])
 
   def test_pad_or_clip_nd_tensor(self):
-    tensor_placeholder = tf.placeholder(tf.float32, [None, 5, 4, 7])
+    tensor_placeholder = tf.compat.v1.placeholder(tf.float32, [None, 5, 4, 7])
     output_tensor = shape_utils.pad_or_clip_nd(
         tensor_placeholder, [None, 3, 5, tf.constant(6)])
 
@@ -144,11 +144,11 @@ class StaticOrDynamicMapFnTest(tf.test.TestCase):
 
   def test_with_dynamic_shape(self):
     def fn(input_tensor):
-      return tf.reduce_sum(input_tensor)
-    input_tensor = tf.placeholder(tf.float32, shape=(None, 2))
+      return tf.reduce_sum(input_tensor=input_tensor)
+    input_tensor = tf.compat.v1.placeholder(tf.float32, shape=(None, 2))
     map_fn_output = shape_utils.static_or_dynamic_map_fn(fn, input_tensor)
 
-    op_names = [op.name for op in tf.get_default_graph().get_operations()]
+    op_names = [op.name for op in tf.compat.v1.get_default_graph().get_operations()]
     self.assertTrue(any(['map' == op_name[:3] for op_name in op_names]))
 
     with self.test_session() as sess:
@@ -163,11 +163,11 @@ class StaticOrDynamicMapFnTest(tf.test.TestCase):
 
   def test_with_static_shape(self):
     def fn(input_tensor):
-      return tf.reduce_sum(input_tensor)
+      return tf.reduce_sum(input_tensor=input_tensor)
     input_tensor = tf.constant([[1, 2], [3, 1], [0, 4]], dtype=tf.float32)
     map_fn_output = shape_utils.static_or_dynamic_map_fn(fn, input_tensor)
 
-    op_names = [op.name for op in tf.get_default_graph().get_operations()]
+    op_names = [op.name for op in tf.compat.v1.get_default_graph().get_operations()]
     self.assertTrue(all(['map' != op_name[:3] for op_name in op_names]))
 
     with self.test_session() as sess:
@@ -179,12 +179,12 @@ class StaticOrDynamicMapFnTest(tf.test.TestCase):
       input_tensor, scalar_index_tensor = elems
       return tf.reshape(tf.slice(input_tensor, scalar_index_tensor, [1]), [])
 
-    input_tensor = tf.placeholder(tf.float32, shape=(None, 3))
-    scalar_index_tensor = tf.placeholder(tf.int32, shape=(None, 1))
+    input_tensor = tf.compat.v1.placeholder(tf.float32, shape=(None, 3))
+    scalar_index_tensor = tf.compat.v1.placeholder(tf.int32, shape=(None, 1))
     map_fn_output = shape_utils.static_or_dynamic_map_fn(
         fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)
 
-    op_names = [op.name for op in tf.get_default_graph().get_operations()]
+    op_names = [op.name for op in tf.compat.v1.get_default_graph().get_operations()]
     self.assertTrue(any(['map' == op_name[:3] for op_name in op_names]))
 
     with self.test_session() as sess:
@@ -212,7 +212,7 @@ class StaticOrDynamicMapFnTest(tf.test.TestCase):
     map_fn_output = shape_utils.static_or_dynamic_map_fn(
         fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)
 
-    op_names = [op.name for op in tf.get_default_graph().get_operations()]
+    op_names = [op.name for op in tf.compat.v1.get_default_graph().get_operations()]
     self.assertTrue(all(['map' != op_name[:3] for op_name in op_names]))
 
     with self.test_session() as sess:
@@ -241,7 +241,7 @@ class CheckMinImageShapeTest(tf.test.TestCase):
       _ = shape_utils.check_min_image_dim(64, input_tensor)
 
   def test_check_min_image_dim_dynamic_shape(self):
-    input_placeholder = tf.placeholder(tf.float32, shape=[1, None, None, 3])
+    input_placeholder = tf.compat.v1.placeholder(tf.float32, shape=[1, None, None, 3])
     image_tensor = shape_utils.check_min_image_dim(33, input_placeholder)
 
     with self.test_session() as sess:
@@ -273,8 +273,8 @@ class AssertShapeEqualTest(tf.test.TestCase):
       sess.run(op)
 
   def test_unequal_dynamic_shape_raises_tf_assert(self):
-    tensor_a = tf.placeholder(tf.float32, shape=[1, None, None, 3])
-    tensor_b = tf.placeholder(tf.float32, shape=[1, None, None, 3])
+    tensor_a = tf.compat.v1.placeholder(tf.float32, shape=[1, None, None, 3])
+    tensor_b = tf.compat.v1.placeholder(tf.float32, shape=[1, None, None, 3])
     op = shape_utils.assert_shape_equal(
         shape_utils.combined_static_and_dynamic_shape(tensor_a),
         shape_utils.combined_static_and_dynamic_shape(tensor_b))
@@ -284,8 +284,8 @@ class AssertShapeEqualTest(tf.test.TestCase):
                                 tensor_b: np.zeros([1, 4, 4, 3])})
 
   def test_equal_dynamic_shape_succeeds(self):
-    tensor_a = tf.placeholder(tf.float32, shape=[1, None, None, 3])
-    tensor_b = tf.placeholder(tf.float32, shape=[1, None, None, 3])
+    tensor_a = tf.compat.v1.placeholder(tf.float32, shape=[1, None, None, 3])
+    tensor_b = tf.compat.v1.placeholder(tf.float32, shape=[1, None, None, 3])
     op = shape_utils.assert_shape_equal(
         shape_utils.combined_static_and_dynamic_shape(tensor_a),
         shape_utils.combined_static_and_dynamic_shape(tensor_b))
@@ -312,8 +312,8 @@ class AssertShapeEqualTest(tf.test.TestCase):
       sess.run(op)
 
   def test_unequal_dynamic_shape_along_first_dim_raises_tf_assert(self):
-    tensor_a = tf.placeholder(tf.float32, shape=[None, None, None, 3])
-    tensor_b = tf.placeholder(tf.float32, shape=[None, None, 3])
+    tensor_a = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, 3])
+    tensor_b = tf.compat.v1.placeholder(tf.float32, shape=[None, None, 3])
     op = shape_utils.assert_shape_equal_along_first_dimension(
         shape_utils.combined_static_and_dynamic_shape(tensor_a),
         shape_utils.combined_static_and_dynamic_shape(tensor_b))
@@ -323,8 +323,8 @@ class AssertShapeEqualTest(tf.test.TestCase):
                                 tensor_b: np.zeros([2, 4, 3])})
 
   def test_equal_dynamic_shape_along_first_dim_succeeds(self):
-    tensor_a = tf.placeholder(tf.float32, shape=[None, None, None, 3])
-    tensor_b = tf.placeholder(tf.float32, shape=[None])
+    tensor_a = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, 3])
+    tensor_b = tf.compat.v1.placeholder(tf.float32, shape=[None])
     op = shape_utils.assert_shape_equal_along_first_dimension(
         shape_utils.combined_static_and_dynamic_shape(tensor_a),
         shape_utils.combined_static_and_dynamic_shape(tensor_b))
diff --git a/research/object_detection/utils/test_case.py b/research/object_detection/utils/test_case.py
index 5d05a845..9a50bd90 100644
--- a/research/object_detection/utils/test_case.py
+++ b/research/object_detection/utils/test_case.py
@@ -39,11 +39,11 @@ class TestCase(tf.test.TestCase):
       graph.
     """
     with self.test_session(graph=tf.Graph()) as sess:
-      placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]
+      placeholders = [tf.compat.v1.placeholder_with_default(v, v.shape) for v in inputs]
       tpu_computation = tpu.rewrite(graph_fn, placeholders)
       sess.run(tpu.initialize_system())
-      sess.run([tf.global_variables_initializer(), tf.tables_initializer(),
-                tf.local_variables_initializer()])
+      sess.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer(),
+                tf.compat.v1.local_variables_initializer()])
       materialized_results = sess.run(tpu_computation,
                                       feed_dict=dict(zip(placeholders, inputs)))
       sess.run(tpu.shutdown_system())
@@ -67,10 +67,10 @@ class TestCase(tf.test.TestCase):
       graph.
     """
     with self.test_session(graph=tf.Graph()) as sess:
-      placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]
+      placeholders = [tf.compat.v1.placeholder_with_default(v, v.shape) for v in inputs]
       results = graph_fn(*placeholders)
-      sess.run([tf.global_variables_initializer(), tf.tables_initializer(),
-                tf.local_variables_initializer()])
+      sess.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer(),
+                tf.compat.v1.local_variables_initializer()])
       materialized_results = sess.run(results, feed_dict=dict(zip(placeholders,
                                                                   inputs)))
 
diff --git a/research/object_detection/utils/test_utils.py b/research/object_detection/utils/test_utils.py
index fa74c970..3617792c 100644
--- a/research/object_detection/utils/test_utils.py
+++ b/research/object_detection/utils/test_utils.py
@@ -52,7 +52,7 @@ class MockBoxPredictor(box_predictor.BoxPredictor):
     batch_size = combined_feature_shape[0]
     num_anchors = (combined_feature_shape[1] * combined_feature_shape[2])
     code_size = 4
-    zero = tf.reduce_sum(0 * image_feature)
+    zero = tf.reduce_sum(input_tensor=0 * image_feature)
     box_encodings = zero + tf.zeros(
         (batch_size, num_anchors, 1, code_size), dtype=tf.float32)
     class_predictions_with_background = zero + tf.zeros(
@@ -76,7 +76,7 @@ class MockKerasBoxPredictor(box_predictor.KerasBoxPredictor):
     batch_size = combined_feature_shape[0]
     num_anchors = (combined_feature_shape[1] * combined_feature_shape[2])
     code_size = 4
-    zero = tf.reduce_sum(0 * image_feature)
+    zero = tf.reduce_sum(input_tensor=0 * image_feature)
     box_encodings = zero + tf.zeros(
         (batch_size, num_anchors, 1, code_size), dtype=tf.float32)
     class_predictions_with_background = zero + tf.zeros(
diff --git a/research/object_detection/utils/variables_helper.py b/research/object_detection/utils/variables_helper.py
index 141228cd..1fe02303 100644
--- a/research/object_detection/utils/variables_helper.py
+++ b/research/object_detection/utils/variables_helper.py
@@ -123,10 +123,10 @@ def get_variables_available_in_checkpoint(variables,
     variable_names_map = variables
   else:
     raise ValueError('`variables` is expected to be a list or dict.')
-  ckpt_reader = tf.train.NewCheckpointReader(checkpoint_path)
+  ckpt_reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path)
   ckpt_vars_to_shape_map = ckpt_reader.get_variable_to_shape_map()
   if not include_global_step:
-    ckpt_vars_to_shape_map.pop(tf.GraphKeys.GLOBAL_STEP, None)
+    ckpt_vars_to_shape_map.pop(tf.compat.v1.GraphKeys.GLOBAL_STEP, None)
   vars_in_ckpt = {}
   for variable_name, variable in sorted(variable_names_map.items()):
     if variable_name in ckpt_vars_to_shape_map:
diff --git a/research/object_detection/utils/variables_helper_test.py b/research/object_detection/utils/variables_helper_test.py
index 94585d71..861d1589 100644
--- a/research/object_detection/utils/variables_helper_test.py
+++ b/research/object_detection/utils/variables_helper_test.py
@@ -81,7 +81,7 @@ class MultiplyGradientsMatchingRegexTest(tf.test.TestCase):
     grads_and_vars = variables_helper.multiply_gradients_matching_regex(
         grads_and_vars, regex_list, multiplier)
     exp_output = [(0.0, 1.0), (0.0, 2.0), (3.0, 3.0), (4.0, 4.0)]
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       output = sess.run(grads_and_vars)
@@ -94,7 +94,7 @@ class MultiplyGradientsMatchingRegexTest(tf.test.TestCase):
     grads_and_vars = variables_helper.multiply_gradients_matching_regex(
         grads_and_vars, regex_list, multiplier)
     exp_output = [(1.0, 1.0), (0.0, 2.0), (3.0, 3.0), (0.0, 4.0)]
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       output = sess.run(grads_and_vars)
@@ -119,7 +119,7 @@ class FreezeGradientsMatchingRegexTest(tf.test.TestCase):
     grads_and_vars = variables_helper.freeze_gradients_matching_regex(
         grads_and_vars, regex_list)
     exp_output = [(3.0, 3.0), (4.0, 4.0)]
-    init_op = tf.global_variables_initializer()
+    init_op = tf.compat.v1.global_variables_initializer()
     with self.test_session() as sess:
       sess.run(init_op)
       output = sess.run(grads_and_vars)
@@ -135,8 +135,8 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
           tf.Variable(1.0, name='biases')
       ]
       checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
-      init_op = tf.global_variables_initializer()
-      saver = tf.train.Saver(variables)
+      init_op = tf.compat.v1.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver(variables)
       with self.test_session() as sess:
         sess.run(init_op)
         saver.save(sess, checkpoint_path)
@@ -148,13 +148,13 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
     checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
     with tf.Graph().as_default():
       weight_variable = tf.Variable(1.0, name='weights')
-      global_step = tf.train.get_or_create_global_step()
+      global_step = tf.compat.v1.train.get_or_create_global_step()
       graph1_variables = [
           weight_variable,
           global_step
       ]
-      init_op = tf.global_variables_initializer()
-      saver = tf.train.Saver(graph1_variables)
+      init_op = tf.compat.v1.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver(graph1_variables)
       with self.test_session() as sess:
         sess.run(init_op)
         saver.save(sess, checkpoint_path)
@@ -171,8 +171,8 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
       graph1_variables = [
           tf.Variable(1.0, name='ckpt_weights'),
       ]
-      init_op = tf.global_variables_initializer()
-      saver = tf.train.Saver(graph1_variables)
+      init_op = tf.compat.v1.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver(graph1_variables)
       with self.test_session() as sess:
         sess.run(init_op)
         saver.save(sess, checkpoint_path)
@@ -193,14 +193,14 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
     checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
     with tf.Graph().as_default():
       bias_variable = tf.Variable(3.0, name='biases')
-      global_step = tf.train.get_or_create_global_step()
+      global_step = tf.compat.v1.train.get_or_create_global_step()
       graph1_variables = [
           tf.Variable([[1.0, 2.0], [3.0, 4.0]], name='weights'),
           bias_variable,
           global_step
       ]
-      init_op = tf.global_variables_initializer()
-      saver = tf.train.Saver(graph1_variables)
+      init_op = tf.compat.v1.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver(graph1_variables)
       with self.test_session() as sess:
         sess.run(init_op)
         saver.save(sess, checkpoint_path)
diff --git a/research/object_detection/utils/visualization_utils.py b/research/object_detection/utils/visualization_utils.py
index 79e18250..9a52b2b3 100644
--- a/research/object_detection/utils/visualization_utils.py
+++ b/research/object_detection/utils/visualization_utils.py
@@ -72,7 +72,7 @@ def save_image_array_as_png(image, output_path):
     output_path: path to which image should be written.
   """
   image_pil = Image.fromarray(np.uint8(image)).convert('RGB')
-  with tf.gfile.Open(output_path, 'w') as fid:
+  with tf.io.gfile.GFile(output_path, 'w') as fid:
     image_pil.save(fid, 'PNG')
 
 
@@ -378,7 +378,7 @@ def draw_bounding_boxes_on_image_tensors(images,
 
   def draw_boxes(image_and_detections):
     """Draws boxes on image."""
-    image_with_boxes = tf.py_func(visualize_boxes_fn, image_and_detections,
+    image_with_boxes = tf.compat.v1.py_func(visualize_boxes_fn, image_and_detections,
                                   tf.uint8)
     return image_with_boxes
 
@@ -700,8 +700,8 @@ def add_cdf_image_summary(values, name):
     image = np.fromstring(fig.canvas.tostring_rgb(), dtype='uint8').reshape(
         1, int(height), int(width), 3)
     return image
-  cdf_plot = tf.py_func(cdf_plot, [values], tf.uint8)
-  tf.summary.image(name, cdf_plot)
+  cdf_plot = tf.compat.v1.py_func(cdf_plot, [values], tf.uint8)
+  tf.compat.v1.summary.image(name, cdf_plot)
 
 
 def add_hist_image_summary(values, bins, name):
@@ -729,5 +729,5 @@ def add_hist_image_summary(values, bins, name):
         fig.canvas.tostring_rgb(), dtype='uint8').reshape(
             1, int(height), int(width), 3)
     return image
-  hist_plot = tf.py_func(hist_plot, [values, bins], tf.uint8)
-  tf.summary.image(name, hist_plot)
+  hist_plot = tf.compat.v1.py_func(hist_plot, [values, bins], tf.uint8)
+  tf.compat.v1.summary.image(name, hist_plot)
diff --git a/research/object_detection/utils/visualization_utils_test.py b/research/object_detection/utils/visualization_utils_test.py
index 87bbacad..39e4ac6a 100644
--- a/research/object_detection/utils/visualization_utils_test.py
+++ b/research/object_detection/utils/visualization_utils_test.py
@@ -135,7 +135,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
               min_score_thresh=0.2))
 
       with self.test_session() as sess:
-        sess.run(tf.global_variables_initializer())
+        sess.run(tf.compat.v1.global_variables_initializer())
 
         # Write output images for visualization.
         images_with_boxes_np = sess.run(images_with_boxes)
@@ -168,7 +168,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
               min_score_thresh=0.2))
 
       with self.test_session() as sess:
-        sess.run(tf.global_variables_initializer())
+        sess.run(tf.compat.v1.global_variables_initializer())
 
         final_images_np = sess.run(images_with_boxes)
         self.assertEqual((2, 100, 200, 3), final_images_np.shape)
@@ -212,7 +212,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
   def test_add_cdf_image_summary(self):
     values = [0.1, 0.2, 0.3, 0.4, 0.42, 0.44, 0.46, 0.48, 0.50]
     visualization_utils.add_cdf_image_summary(values, 'PositiveAnchorLoss')
-    cdf_image_summary = tf.get_collection(key=tf.GraphKeys.SUMMARIES)[0]
+    cdf_image_summary = tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.SUMMARIES)[0]
     with self.test_session():
       cdf_image_summary.eval()
 
@@ -221,7 +221,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
     bins = [0.01 * i for i in range(101)]
     visualization_utils.add_hist_image_summary(values, bins,
                                                'ScoresDistribution')
-    hist_image_summary = tf.get_collection(key=tf.GraphKeys.SUMMARIES)[0]
+    hist_image_summary = tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.SUMMARIES)[0]
     with self.test_session():
       hist_image_summary.eval()
 
