[
  {
    "_comment": "bert_fp32_inference",
    "input": "run_tf_benchmark.py --framework=tensorflow --use-case=language_modeling --model-name=bert_large --precision=fp32 --mode=inference --benchmark-dir=/workspace/benchmarks --checkpoint=/checkpoints --intelai-models=/workspace/intelai_models --num-cores=28 --batch-size=1 --socket-id=0 --output-dir=/workspace/benchmarks/common/tensorflow/logs --benchmark-only --verbose --model-source-dir=/workspace/models --data-location=/dataset --num-inter-threads=1 --num-intra-threads=28 --disable-tcmalloc=True --max-seq-length=128 --batch-size=8",
    "output": "numactl --cpunodebind=0 --membind=0 python /workspace/intelai_models/inference/run_squad.py --init_checkpoint=/checkpoints/model.ckpt-3649 --vocab_file=/dataset/vocab.txt --bert_config_file=/dataset/bert_config.json --predict_file=/dataset/dev-v1.1.json --precision=fp32 --output_dir=/workspace/benchmarks/common/tensorflow/logs --predict_batch_size=8 --do_predict=True  --mode=benchmark --max_seq_length=128 --inter_op_parallelism_threads=1 --intra_op_parallelism_threads=28"
  }
]
